{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:51.518625Z",
     "start_time": "2024-08-26T07:12:49.113780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import pickle\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import optuna.visualization as vis\n",
    "import plotly\n",
    "import optuna\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import DataInit\n",
    "from DataInit import DataManager, RewardDataManager\n",
    "from pathlib import Path"
   ],
   "id": "dd091a7d72915d61",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:51.562370Z",
     "start_time": "2024-08-26T07:12:51.533713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 配置管理\n",
    "config = DataInit.config_manager()\n",
    "\n",
    "# 路径管理\n",
    "global_path, data_path, load_latency_original_csv_path, rewards_npy_path, models_pkl_path = DataInit.path_manager(config)"
   ],
   "id": "11564e0709d033d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Config Info ----------\n",
      "path:\n",
      "  global_path: E:/Study in the UK/Project/MScProject\n",
      "base:\n",
      "  'N': 10\n",
      "  T: 11000\n",
      "  T_train_val: 10000\n",
      "  train_ratio: 0.8\n",
      "  T_train: 8000\n",
      "  T_val: 2000\n",
      "  T_test: 1000\n",
      "  lambda_load: 0.5\n",
      "  top_k:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "data_generation:\n",
      "  load_data:\n",
      "    node_load_mean_mean: 50.0\n",
      "    node_load_mean_std: 10.0\n",
      "    node_load_iid_std: 5.0\n",
      "    node_load_ar1_theta: 0.9\n",
      "  latency_data:\n",
      "    node_latency_mean_mean: 30.0\n",
      "    node_latency_mean_std: 10.0\n",
      "    node_latency_ar1_theta: 0.9\n",
      "  reward_parameters:\n",
      "    iid:\n",
      "      alpha_load_0: 36.0\n",
      "      alpha_latency_1: 0.031\n",
      "    ar1:\n",
      "      alpha_load_0: 36.0\n",
      "      alpha_latency_1: 0.02\n",
      "  reward_parameters_slider:\n",
      "    alpha_load_0:\n",
      "      value: 1.0\n",
      "      min: 0.001\n",
      "      max: 40.0\n",
      "      step: 0.01\n",
      "      description: alpha_load_0\n",
      "    alpha_latency_0:\n",
      "      value: 1.0\n",
      "      min: 0.001\n",
      "      max: 6.0\n",
      "      step: 0.01\n",
      "      description: alpha_latency_0\n",
      "    alpha_latency_1:\n",
      "      value: 0.5\n",
      "      min: 0.0001\n",
      "      max: 0.5\n",
      "      step: 0.001\n",
      "      description: alpha_latency_1\n",
      "epsilon_greedy:\n",
      "  dynamic_plot:\n",
      "    epsilon_min: 0.001\n",
      "    epsilon_max: 0.5\n",
      "    epsilon_step: 0.01\n",
      "    epsilon_default_value: 0.1\n",
      "  evaluation:\n",
      "    epsilon_min: 0.0001\n",
      "    epsilon_max: 0.2001\n",
      "    epsilon_step: 0.0005\n",
      "adaptive_epsilon_greedy:\n",
      "  dynamic_plot:\n",
      "    init_epsilon_min: 0.001\n",
      "    init_epsilon_max: 0.5\n",
      "    init_epsilon_step: 0.01\n",
      "    init_epsilon_default_value: 0.1\n",
      "    min_epsilon_min: 0.001\n",
      "    min_epsilon_max: 0.1\n",
      "    min_epsilon_step: 0.001\n",
      "    min_epsilon_default_value: 0.05\n",
      "  evaluation:\n",
      "    init_epsilon_min: 0.0001\n",
      "    init_epsilon_max: 0.1001\n",
      "    init_epsilon_step: 0.0005\n",
      "    min_epsilon: 0.05\n",
      "dynamic_adaptive_epsilon_greedy:\n",
      "  min_epsilon: 0.05\n",
      "  max_epsilon: 0.8\n",
      "  dynamic_plot:\n",
      "    init_epsilon_min: 0.001\n",
      "    init_epsilon_max: 0.5\n",
      "    init_epsilon_step: 0.01\n",
      "    init_epsilon_default_value: 0.1\n",
      "    percentiles_min: 50\n",
      "    percentiles_max: 100\n",
      "    percentiles_step: 1\n",
      "    percentiles_default_value: 80\n",
      "  evaluation:\n",
      "    init_epsilon_min: 0.0001\n",
      "    init_epsilon_max: 0.1001\n",
      "    init_epsilon_step: 0.0005\n",
      "    default_percentiles: 80\n",
      "    percentiles_min: 50\n",
      "    percentiles_max: 100\n",
      "    percentiles_step: 1\n",
      "    default_init_epsilon: 0.04\n",
      "boltzmann:\n",
      "  dynamic_plot:\n",
      "    temperature_min: 0.01\n",
      "    temperature_max: 1.01\n",
      "    temperature_step: 0.001\n",
      "    temperature_default_value: 0.5\n",
      "  evaluation:\n",
      "    temperature_min: 0.01\n",
      "    temperature_max: 0.25\n",
      "    temperature_step: 0.001\n",
      "thompson_sampling: None\n",
      "ucb:\n",
      "  dynamic_plot:\n",
      "    c_min: 0.001\n",
      "    c_max: 1\n",
      "    c_step: 0.001\n",
      "    c_default_value: 0.5\n",
      "  evaluation:\n",
      "    c_min: 0.001\n",
      "    c_max: 1.0\n",
      "    c_step: 0.001\n",
      "exp3:\n",
      "  dynamic_plot:\n",
      "    gamma_min: 0.001\n",
      "    gamma_max: 1.0\n",
      "    gamma_step: 0.001\n",
      "    gamma_default_value: 0.01\n",
      "  evaluation:\n",
      "    gamma_min: 0.001\n",
      "    gamma_max: 1.0\n",
      "    gamma_step: 0.001\n",
      "exp_ix:\n",
      "  dynamic_plot:\n",
      "    eta_min: 0.01\n",
      "    eta_max: 1.01\n",
      "    eta_step: 0.01\n",
      "    eta_default_value: 0.5\n",
      "  evaluation:\n",
      "    eta_min: 0.01\n",
      "    eta_max: 1.01\n",
      "    eta_step: 0.01\n",
      "exp4:\n",
      "  batch_size: 64\n",
      "  seq_length: 20\n",
      "  input_size: 10\n",
      "  output_size: 10\n",
      "  learning_rate: 0.001\n",
      "  num_workers: 16\n",
      "  num_epochs: 100\n",
      "  device: cuda\n",
      "  mix_precision: true\n",
      "  patience_epochs: 6\n",
      "  min_delta: 0.001\n",
      "  mode: min\n",
      "  factor: 0.1\n",
      "  patience_lr: 2\n",
      "  min_lr: 1.0e-06\n",
      "  threshold: 0.01\n",
      "  ARconfig:\n",
      "    order: 5\n",
      "  LSTMconfig:\n",
      "    hidden_size: 128\n",
      "    num_layers: 4\n",
      "    dropout_prob: 0.2\n",
      "    weight_decay: 0.0001\n",
      "  GNNconfig:\n",
      "    hidden_size: 128\n",
      "    num_layers: 4\n",
      "\n",
      "----------- config End -----------\n",
      "\n",
      "---------- Path Info ----------\n",
      "Global Path: E:\\Study in the UK\\Project\\MScProject\n",
      "Data Path: E:\\Study in the UK\\Project\\MScProject\\Data\n",
      "Load Latency Original CSV Path: E:\\Study in the UK\\Project\\MScProject\\Data\\load_latency_original_csv\n",
      "Rewards NPY Path: E:\\Study in the UK\\Project\\MScProject\\Data\\rewards_npy\n",
      "Models PKL Path: E:\\Study in the UK\\Project\\MScProject\\Data\\models_pkl\n",
      "----------- Path End -----------\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:51.917832Z",
     "start_time": "2024-08-26T07:12:51.648059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据\n",
    "iid_load_data_manager = DataInit.import_data_manager(models_pkl_path, 'iid_load', if_print=False)\n",
    "ar1_load_data_manager = DataInit.import_data_manager(models_pkl_path, 'ar1_load', if_print=False)\n",
    "iid_latency_data_manager = DataInit.import_data_manager(models_pkl_path, 'iid_latency', if_print=False)\n",
    "ar1_latency_data_manager = DataInit.import_data_manager(models_pkl_path, 'ar1_latency', if_print=False)\n",
    "reward_data_manager = DataInit.import_data_manager(models_pkl_path, 'reward', if_print=False)"
   ],
   "id": "43d2a0e171be8b85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:52.912983Z",
     "start_time": "2024-08-26T07:12:51.927420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ],
   "id": "4e1587fe8ab58233",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:52.925899Z",
     "start_time": "2024-08-26T07:12:52.920824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.layer_norm(out[:, -1, :])\n",
    "        out = F.relu(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "61b8ab09960c7b01",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:52.936101Z",
     "start_time": "2024-08-26T07:12:52.932806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_prob=0.5):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)  # LSTM层\n",
    "#         self.layer_norm = nn.LayerNorm(hidden_size)  # 层归一化\n",
    "#         self.dropout = nn.Dropout(dropout_prob)  # Dropout层        \n",
    "#         self.fc = nn.Linear(hidden_size, output_size)  # 全连接层\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h_0, c_0))  # LSTM层的输出\n",
    "#         out = self.layer_norm(out[:, -1, :])  # 对最后一个时间步进行层归一化\n",
    "#         out = self.dropout(out)  # 应用Dropout\n",
    "#         out = F.relu(out)  # 应用ReLU激活函数\n",
    "#         out = self.fc(out)  # 全连接层输出\n",
    "# \n",
    "#         return out\n"
   ],
   "id": "b251bb731308d489",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:52.947035Z",
     "start_time": "2024-08-26T07:12:52.943176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience_epochs=5, min_delta=1e-2):\n",
    "        self.patience_epochs = patience_epochs\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = train_loss\n",
    "        elif train_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = train_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience_epochs:\n",
    "                self.early_stop = True"
   ],
   "id": "89ea14983ec70822",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:52.998223Z",
     "start_time": "2024-08-26T07:12:52.957994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMTraining:\n",
    "    def __init__(self, config: dict, data_manage: object, model_type:str='lstm') -> None:\n",
    "        # 判断是LSTM还是GNN\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        # 从 config 提取基础参数\n",
    "        self.T_train: int = config['base']['T_train']\n",
    "        self.T_val: int = config['base']['T_val']\n",
    "        self.T_test: int = config['base']['T_test']\n",
    "\n",
    "        # 从 config 提取 LSTM 模型参数\n",
    "        \n",
    "        self.seq_length: int = config['exp4']['seq_length']\n",
    "        self.input_size: int = config['exp4']['input_size']\n",
    "        self.output_size: int = config['exp4']['output_size']\n",
    "\n",
    "        self.num_epochs: int = config['exp4']['num_epochs']\n",
    "        self.device: str = config['exp4']['device']\n",
    "        self.mix_precision: bool = config['exp4']['mix_precision']\n",
    "        \n",
    "        self.model: nn.Module = LSTMModel(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=config['exp4']['LSTMconfig']['hidden_size'],\n",
    "            output_size=self.output_size,\n",
    "            num_layers=config['exp4']['LSTMconfig']['num_layers'],\n",
    "            # dropout_prob=config['exp4']['LSTMconfig']['dropout_prob']\n",
    "        ).to(self.device)\n",
    "\n",
    "        # 优化器、损失函数、学习率调度器、早停\n",
    "        self.criterion: nn.Module = nn.MSELoss()\n",
    "        self.optimizer: optim.Optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=config['exp4']['learning_rate'],\n",
    "            weight_decay=config['exp4']['LSTMconfig']['weight_decay']  # L2正则化\n",
    "        )\n",
    "        self.scheduler: lr_scheduler.ReduceLROnPlateau = lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode=config['exp4']['mode'],\n",
    "            factor=config['exp4']['factor'],\n",
    "            patience=config['exp4']['patience_lr'],\n",
    "            min_lr=config['exp4']['min_lr'],\n",
    "            threshold=config['exp4']['threshold']\n",
    "        )\n",
    "        self.early_stopping: EarlyStopping = EarlyStopping(patience_epochs=config['exp4']['patience_epochs'], min_delta=config['exp4']['min_delta'])\n",
    "        self.scaler: Optional[torch.amp.GradScaler] = torch.amp.GradScaler() if config['exp4']['mix_precision'] else None\n",
    "\n",
    "        # 数据管理\n",
    "        self.train_dataloader: DataLoader = data_manage.train_dataloader\n",
    "        self.val_dataloader: DataLoader = data_manage.val_dataloader\n",
    "        self.test_data_tensor: torch.Tensor = data_manage.test_data_tensor\n",
    "        \n",
    "        # 获取完整tarin_val的tensor数据，用于截取最后一组，对test的第一个进行预测\n",
    "        self.train_val_data_tensor: torch.Tensor = data_manage.train_val_data_tensor\n",
    "        \n",
    "        self.train_data_np: np.ndarray = data_manage.train_data_np\n",
    "        self.val_data_np: np.ndarray = data_manage.val_data_np\n",
    "        self.test_data_np: np.ndarray = data_manage.test_data_np\n",
    "\n",
    "        # # 从 data_manage 模块中获取边索引张量\n",
    "        # edge_index = data_manage.edge_index_tensor\n",
    "\n",
    "        # 用于记录预测值\n",
    "        self.train_predictions: Optional[torch.Tensor] = None\n",
    "        self.val_predictions: Optional[torch.Tensor] = None\n",
    "        self.test_predictions: Optional[torch.Tensor] = None\n",
    "        \n",
    "        # 记录损失\n",
    "        self.train_losses: List[float] = []\n",
    "        self.val_losses: List[float] = []\n",
    "        self.fine_tune_losses: List[float] = []\n",
    "\n",
    "        # 记录学习率变化\n",
    "        self.train_with_val_learning_rates: List[float] = []\n",
    "        self.fine_tune_learning_rates: List[float] = []\n",
    "\n",
    "        # 记录扩展的学习率\n",
    "        self.train_extended_lr: Optional[np.ndarray] = None\n",
    "        self.val_extended_lr: Optional[np.ndarray] = None\n",
    "\n",
    "    def train_epoch(self, epoch: int) -> Tuple[float, torch.Tensor]:\n",
    "        self.model.train()  # Ensure model is in training mode\n",
    "        train_cumulative_loss: float = 0.0  # 训练集累积损失\n",
    "        epoch_train_pred: torch.Tensor = torch.zeros((self.T_train, self.output_size), device=self.device)\n",
    "        train_start_idx: int = 20  # 预测初始index值\n",
    "\n",
    "        for train, val in self.train_dataloader:\n",
    "            batch_size: int = train.size(0)\n",
    "            end_idx: int = train_start_idx + batch_size\n",
    "            train, val = train.to(self.device, non_blocking=True).float(), val.to(self.device, non_blocking=True).float()\n",
    "\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            if self.mix_precision:  # Mixed precision training\n",
    "                with torch.amp.autocast(device_type=self.device):\n",
    "                    train_pred: torch.Tensor = self.model(train)  # 前向传播, shape: [batch_size, output_size], [64, 10]\n",
    "                    train_loss: torch.Tensor = self.criterion(train_pred, val)\n",
    "                self.scaler.scale(train_loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                train_pred: torch.Tensor = self.model(train)\n",
    "                train_loss: torch.Tensor = self.criterion(train_pred, val)\n",
    "                train_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # 将当前批次的预测结果填入对应位置\n",
    "            epoch_train_pred[train_start_idx:end_idx] = train_pred.detach()\n",
    "            # print(f'Train start index: {train_start_idx}, End index: {end_idx}')\n",
    "            train_start_idx = end_idx\n",
    "            self.train_losses.append(train_loss.item())    # Record single training loss\n",
    "            train_cumulative_loss += train_loss.item()\n",
    "\n",
    "        return train_cumulative_loss, epoch_train_pred\n",
    "\n",
    "    def validate_epoch(self) -> Tuple[float, torch.Tensor]:\n",
    "        self.model.eval()  # Ensure model is in evaluation mode\n",
    "        val_cumulative_loss: float = 0.0\n",
    "        epoch_val_pred: torch.Tensor = torch.zeros((self.T_val, self.output_size), device=self.device)\n",
    "        val_start_idx: int = 0  # 预测值起始位置\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_input, val_target in self.val_dataloader:\n",
    "                batch_size: int = val_input.size(0)\n",
    "                \n",
    "                end_idx: int = val_start_idx + batch_size\n",
    "\n",
    "                val_input, val_target = val_input.to(self.device, non_blocking=True).float(), val_target.to(self.device, non_blocking=True).float()\n",
    "\n",
    "                if self.mix_precision: # 混合精度推断\n",
    "                    with torch.amp.autocast(device_type=self.device):\n",
    "                        val_pred: torch.Tensor = self.model(val_input)\n",
    "                        val_loss: torch.Tensor = self.criterion(val_pred, val_target)\n",
    "                else:\n",
    "                    val_pred: torch.Tensor = self.model(val_input)\n",
    "                    val_loss: torch.Tensor = self.criterion(val_pred, val_target)\n",
    "\n",
    "                # 将当前批次的预测结果填入对应位置\n",
    "                epoch_val_pred[val_start_idx:end_idx] = val_pred.detach()\n",
    "                val_start_idx = end_idx  # 更新起始索引\n",
    "                self.val_losses.append(val_loss.item())  # Record single validation loss\n",
    "                val_cumulative_loss += val_loss.item()\n",
    "\n",
    "        return val_cumulative_loss, epoch_val_pred\n",
    "\n",
    "    def train(self) -> None:\n",
    "        for epoch in tqdm(range(self.num_epochs), desc=\"Training\"):\n",
    "            train_cumulative_loss, epoch_train_pred = self.train_epoch(epoch)\n",
    "            val_cumulative_loss, epoch_val_pred = self.validate_epoch()\n",
    "\n",
    "            # 计算训练和验证的平均损失\n",
    "            train_avg_loss: float = train_cumulative_loss / len(self.train_dataloader)\n",
    "            val_avg_loss: float = val_cumulative_loss / len(self.val_dataloader)\n",
    "\n",
    "            # 早停机制基于验证集损失\n",
    "            self.early_stopping(val_avg_loss)\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered. Stopping training. Epoch: {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "            # 调度器更新基于验证集损失\n",
    "            self.scheduler.step(val_avg_loss)\n",
    "            \n",
    "            # 动态学习率\n",
    "            current_lr: float = self.scheduler.optimizer.param_groups[0]['lr']\n",
    "            self.train_with_val_learning_rates.append(current_lr)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:  # 每10轮输出一次损失, 以及释放GPU缓存\n",
    "                print(f'Epoch [{epoch + 1}/{self.num_epochs}], Train_Loss: {train_avg_loss:.4f}, Val_Loss: {val_avg_loss:.4f}')\n",
    "                print(f'Current Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "                # if torch.cuda.is_available():  # 释放GPU缓存\n",
    "                #     torch.cuda.empty_cache()\n",
    "\n",
    "        self.train_predictions = epoch_train_pred\n",
    "        self.val_predictions = epoch_val_pred\n",
    "\n",
    "        # 训练结束后计算扩展的学习率\n",
    "        self.compute_extended_learning_rates()\n",
    "\n",
    "        print(f\"Training finished after {epoch + 1} epochs.\")\n",
    "        print(f\"Average loss per epoch: {train_avg_loss:.4f}, Validation loss: {val_avg_loss:.4f}\")\n",
    "\n",
    "    def fine_tune(self) -> None:\n",
    "        tune_cumulative_loss: float = 0.0\n",
    "        predictions: torch.Tensor = torch.zeros((self.T_test, self.output_size), device=self.device)\n",
    "        input_data: torch.Tensor = self.train_val_data_tensor[:, -self.seq_length:].T.unsqueeze(0).to(self.device)\n",
    "\n",
    "        for i in tqdm(range(self.T_test), desc=\"Predicting and Fine-tuning\"):\n",
    "            self.model.train()  # Keep model in training mode for fine-tuning\n",
    "            pred: torch.Tensor = self.model(input_data)\n",
    "            predictions[i] = pred.squeeze(0)  # 将预测值保存到 predictions, shape: [T_test, output_size], [1000, 10]\n",
    "\n",
    "\n",
    "            loss: torch.Tensor = self.criterion(pred.squeeze(0), self.test_data_tensor[:, i])    # 计算损失\n",
    "            self.fine_tune_losses.append(loss.item())    # 记录单次损失\n",
    "            tune_cumulative_loss += loss.item()    # 累加损失\n",
    "\n",
    "            # 使用真实值进行微调\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # 在每个epoch结束时，计算验证集的平均损失\n",
    "            tune_avg_loss: float = tune_cumulative_loss / (i + 1)\n",
    "\n",
    "            # 调度器更新（可选，基于微调期间的损失）\n",
    "            self.scheduler.step(tune_avg_loss)\n",
    "            current_lr: float = self.scheduler.get_last_lr()[0]\n",
    "            self.fine_tune_learning_rates.append(current_lr)\n",
    "\n",
    "            if (i + 1) % 100 == 0:  # 每100步输出一次损失\n",
    "                print(f'Step {i + 1}/{self.T_test}, Loss: {tune_avg_loss:.4f}')\n",
    "                print(f'Current Learning Rate: {current_lr:.6f}')\n",
    "            \n",
    "            # 更新输入序列 \n",
    "            input_data = torch.cat((input_data[:, 1:, :], self.test_data_tensor[:, i].unsqueeze(0).unsqueeze(1)), dim=1)\n",
    "\n",
    "        print(f'Average Fine-tuning Loss over {self.T_test} steps: {tune_avg_loss:.4f}')\n",
    "        self.test_predictions = predictions\n",
    "        \n",
    "    def compute_extended_learning_rates(self) -> None:\n",
    "        \"\"\"计算用于绘图的扩展学习率数组\"\"\"\n",
    "        train_steps = len(self.train_losses)\n",
    "        val_steps = len(self.val_losses)\n",
    "        num_epochs = len(self.train_with_val_learning_rates)\n",
    "\n",
    "        train_repeats_per_epoch = train_steps // num_epochs\n",
    "        val_repeats_per_epoch = val_steps // num_epochs\n",
    "\n",
    "        self.train_extended_lr = np.repeat(self.train_with_val_learning_rates, train_repeats_per_epoch)\n",
    "        train_remaining_steps = train_steps - len(self.train_extended_lr)\n",
    "        if train_remaining_steps > 0:\n",
    "            self.train_extended_lr = np.append(self.train_extended_lr, [self.train_with_val_learning_rates[-1]] * train_remaining_steps)\n",
    "\n",
    "        self.val_extended_lr = np.repeat(self.train_with_val_learning_rates, val_repeats_per_epoch)\n",
    "        val_remaining_steps = val_steps - len(self.val_extended_lr)\n",
    "        if val_remaining_steps > 0:\n",
    "            self.val_extended_lr = np.append(self.val_extended_lr, [self.train_with_val_learning_rates[-1]] * val_remaining_steps)\n",
    "\n",
    "    def plot_loss(self) -> None:\n",
    "        \"\"\"绘制训练和验证的损失曲线\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label='Train Losses')\n",
    "        plt.plot(self.val_losses, label='Validation Losses')\n",
    "        plt.title('Training and Validation Loss over Time')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_loss_and_lr(self) -> None:\n",
    "        \"\"\"同时绘制训练损失、验证损失和学习率变化曲线\"\"\"\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss', color=color)\n",
    "        ax1.plot(range(len(self.train_losses)), self.train_losses, label='Train Loss', color='tab:red')\n",
    "        ax1.plot(range(len(self.val_losses)), self.val_losses, label='Val Loss', color='tab:orange')\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax1.legend(loc='upper left')\n",
    "    \n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Learning Rate', color=color)\n",
    "        ax2.plot(range(len(self.train_with_val_learning_rates)), self.train_with_val_learning_rates, label='Training LR', linestyle='--', color='tab:blue')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        ax2.legend(loc='upper right')\n",
    "    \n",
    "        fig.tight_layout()\n",
    "        plt.title('Loss and Learning Rate over Time')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_full(self, i: int) -> None:\n",
    "        \"\"\"绘制完整的训练、验证和测试集的预测与实际值对比图\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # 绘制训练集预测与实际值\n",
    "        plt.plot(range(self.T_train), self.train_predictions[:, i].detach().cpu().numpy(), label='Train Predictions', linestyle='--')\n",
    "        plt.plot(range(self.T_train), self.train_data_np[i, :], label='Train Actual')\n",
    "\n",
    "        # 绘制验证集预测与实际值\n",
    "        plt.plot(range(self.T_train, self.T_train + self.T_val), self.val_predictions[:, i].detach().cpu().numpy(), label='Val Predictions', linestyle='--')\n",
    "        plt.plot(range(self.T_train, self.T_train + self.T_val), self.val_data_np[i, :], label='Val Actual')\n",
    "\n",
    "        # 绘制测试集预测与实际值\n",
    "        plt.plot(range(self.T_train + self.T_val, self.T_train + self.T_val + self.T_test), self.test_predictions[:, i].detach().cpu().numpy(), label='Test Predictions', linestyle='--')\n",
    "        plt.plot(range(self.T_train + self.T_val, self.T_train + self.T_val + self.T_test), self.test_data_np[i, :], label='Test Actual')\n",
    "\n",
    "        # 标记训练集、验证集和测试集的边界\n",
    "        plt.axvline(x=self.T_train, color='black', linestyle='--', label='Train/Val Boundary')\n",
    "        plt.axvline(x=self.T_train + self.T_val, color='black', linestyle='--', label='Val/Test Boundary')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Full Prediction vs Actual')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Values')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_train(self, i: int) -> None:\n",
    "        \"\"\"绘制训练集的预测与实际值对比图\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # 绘制训练集预测与实际值\n",
    "        plt.plot(range(self.T_train), self.train_predictions[:, i].detach().cpu().numpy(), label='Train Predictions', linestyle='--')\n",
    "        plt.plot(range(self.T_train), self.train_data_np[i, :], label='Train Actual')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Train Prediction vs Actual')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Values')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_val(self, i: int) -> None:\n",
    "        \"\"\"绘制验证集的预测与实际值对比图\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # 绘制验证集预测与实际值\n",
    "        plt.plot(range(self.T_val), self.val_predictions[:, i].detach().cpu().numpy(), label='Val Predictions', linestyle='--')\n",
    "        plt.plot(range(self.T_val), self.val_data_np[i, :], label='Val Actual')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Validation Prediction vs Actual')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Values')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_test(self, i: int) -> None:\n",
    "        \"\"\"绘制测试集的预测与实际值对比图\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # 绘制测试集预测与实际值\n",
    "        plt.plot(range(self.T_test), self.test_predictions[:, i].detach().cpu().numpy(), label='Test Predictions', linestyle='--')\n",
    "        plt.plot(range(self.T_test), self.test_data_np[i, :], label='Test Actual')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Test Prediction vs Actual')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Values')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_range(self, start: int, end: int, i: int) -> None:\n",
    "        \"\"\"在指定范围内绘制预测值与实际值的对比图\"\"\"\n",
    "        total_length = self.T_train + self.T_val + self.T_test\n",
    "        if start < 0 or end > total_length or start >= end:\n",
    "            raise ValueError(f\"Invalid range: start={start}, end={end}. Check the range values.\")\n",
    "\n",
    "        # 拼接训练、验证和测试集的预测结果\n",
    "        full_data = torch.cat([self.train_predictions.detach(), self.val_predictions.detach(), self.test_predictions.detach()], dim=0).cpu().numpy()\n",
    "        full_actual = np.concatenate([self.train_data_np, self.val_data_np, self.test_data_np], axis=1)\n",
    "\n",
    "        if i < 0 or i >= full_actual.shape[0]:\n",
    "            raise ValueError(f\"Invalid node index: i={i}. Check the node index.\")\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # 在指定范围内绘制第 i 个节点的预测值与实际值\n",
    "        plt.plot(range(start, end), full_data[start:end, i], label='Predictions', linestyle='--')\n",
    "        plt.plot(range(start, end), full_actual[i, start:end], label='Actual')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(f'Prediction vs Actual for Node {i} from {start} to {end}')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Values')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_all_losses_and_lr_0(self) -> None:\n",
    "        \"\"\"绘制训练、验证、微调的损失和学习率曲线\"\"\"\n",
    "        if self.train_extended_lr is None or self.val_extended_lr is None:\n",
    "            raise ValueError(\"Learning rates have not been computed. Call compute_extended_learning_rates() first.\")\n",
    "\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "        # 训练集的单次损失、累积损失和学习率\n",
    "        axs[0, 0].plot(self.train_losses, label='Train Losses')\n",
    "        axs[0, 0].set_title('Train Losses over Time')\n",
    "        axs[0, 0].set_xlabel('Steps')\n",
    "        axs[0, 0].set_ylabel('Loss')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        cumulative_train_losses = np.cumsum(self.train_losses)\n",
    "        axs[0, 1].plot(cumulative_train_losses, label='Cumulative Train Losses')\n",
    "        axs[0, 1].set_title('Cumulative Train Losses over Time')\n",
    "        axs[0, 1].set_xlabel('Steps')\n",
    "        axs[0, 1].set_ylabel('Cumulative Loss')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[0, 2].plot(self.train_extended_lr, label='Train Learning Rate')\n",
    "        axs[0, 2].set_title('Train Learning Rate over Time')\n",
    "        axs[0, 2].set_xlabel('Steps')\n",
    "        axs[0, 2].set_ylabel('Learning Rate')\n",
    "        axs[0, 2].legend()\n",
    "\n",
    "        # 验证集的单次损失、累积损失和学习率\n",
    "        axs[1, 0].plot(self.val_losses, label='Val Losses')\n",
    "        axs[1, 0].set_title('Val Losses over Time')\n",
    "        axs[1, 0].set_xlabel('Steps')\n",
    "        axs[1, 0].set_ylabel('Loss')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        cumulative_val_losses = np.cumsum(self.val_losses)\n",
    "        axs[1, 1].plot(cumulative_val_losses, label='Cumulative Val Losses')\n",
    "        axs[1, 1].set_title('Cumulative Val Losses over Time')\n",
    "        axs[1, 1].set_xlabel('Steps')\n",
    "        axs[1, 1].set_ylabel('Cumulative Loss')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        axs[1, 2].plot(self.val_extended_lr, label='Val Learning Rate')\n",
    "        axs[1, 2].set_title('Val Learning Rate over Time')\n",
    "        axs[1, 2].set_xlabel('Steps')\n",
    "        axs[1, 2].set_ylabel('Learning Rate')\n",
    "        axs[1, 2].legend()\n",
    "\n",
    "        # 微调测试集的单次损失、累积损失和学习率\n",
    "        axs[2, 0].plot(self.fine_tune_losses, label='Fine-tune Losses')\n",
    "        axs[2, 0].set_title('Fine-tune Losses over Time')\n",
    "        axs[2, 0].set_xlabel('Steps')\n",
    "        axs[2, 0].set_ylabel('Loss')\n",
    "        axs[2, 0].legend()\n",
    "\n",
    "        cumulative_fine_tune_losses = np.cumsum(self.fine_tune_losses)\n",
    "        axs[2, 1].plot(cumulative_fine_tune_losses, label='Cumulative Fine-tune Losses')\n",
    "        axs[2, 1].set_title('Cumulative Fine-tune Losses over Time')\n",
    "        axs[2, 1].set_xlabel('Steps')\n",
    "        axs[2, 1].set_ylabel('Cumulative Loss')\n",
    "        axs[2, 1].legend()\n",
    "\n",
    "        axs[2, 2].plot(self.fine_tune_learning_rates, label='Fine-tune Learning Rate')\n",
    "        axs[2, 2].set_title('Fine-tune Learning Rate over Time')\n",
    "        axs[2, 2].set_xlabel('Steps')\n",
    "        axs[2, 2].set_ylabel('Learning Rate')\n",
    "        axs[2, 2].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_all_losses_and_lr_1(self, left_log_scale: bool = True, right_log_scale: bool = True) -> None:\n",
    "        \"\"\"绘制训练、验证、微调的损失和学习率曲线，并可选择是否使用对数标尺\"\"\"\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(18, 15))  # 2列布局\n",
    "\n",
    "        # 训练集的单次损失和学习率\n",
    "        ax1 = axs[0, 0]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax1.plot(self.train_losses, label='Train Losses', color='blue')\n",
    "        ax2.plot(self.train_extended_lr, label='Train Learning Rate', color='red', linestyle='--')\n",
    "        ax1.set_title('Train Losses and Learning Rate over Time')\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax2.set_ylabel('Learning Rate')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "        if left_log_scale:\n",
    "            ax1.set_yscale('log')  # 左侧轴设置为对数尺度\n",
    "        if right_log_scale:\n",
    "            ax2.set_yscale('log')  # 右侧轴设置为对数尺度\n",
    "        ax2.set_ylim([min(self.train_extended_lr) * 0.95, max(self.train_extended_lr) * 1.1])\n",
    "\n",
    "        # 训练集的累积损失和学习率\n",
    "        ax3 = axs[0, 1]\n",
    "        ax4 = ax3.twinx()\n",
    "        cumulative_train_losses = np.cumsum(self.train_losses)\n",
    "        ax3.plot(cumulative_train_losses, label='Cumulative Train Losses', color='blue')\n",
    "        ax4.plot(self.train_extended_lr, label='Train Learning Rate', color='red', linestyle='--')\n",
    "        ax3.set_title('Cumulative Train Losses and Learning Rate over Time')\n",
    "        ax3.set_xlabel('Steps')\n",
    "        ax3.set_ylabel('Cumulative Loss')\n",
    "        ax4.set_ylabel('Learning Rate')\n",
    "        ax3.legend(loc='upper left')\n",
    "        ax4.legend(loc='upper right')\n",
    "        if left_log_scale:\n",
    "            ax3.set_yscale('log')  # 左侧轴设置为对数尺度\n",
    "        if right_log_scale:\n",
    "            ax4.set_yscale('log')  # 右侧轴设置为对数尺度\n",
    "        ax4.set_ylim([min(self.train_extended_lr) * 0.95, max(self.train_extended_lr) * 1.1])\n",
    "\n",
    "        # 验证集的单次损失和学习率\n",
    "        ax5 = axs[1, 0]\n",
    "        ax6 = ax5.twinx()\n",
    "        ax5.plot(self.val_losses, label='Val Losses', color='blue')\n",
    "        ax6.plot(self.val_extended_lr, label='Val Learning Rate', color='red', linestyle='--')\n",
    "        ax5.set_title('Val Losses and Learning Rate over Time')\n",
    "        ax5.set_xlabel('Steps')\n",
    "        ax5.set_ylabel('Loss')\n",
    "        ax6.set_ylabel('Learning Rate')\n",
    "        ax5.legend(loc='upper left')\n",
    "        ax6.legend(loc='upper right')\n",
    "        if left_log_scale:\n",
    "            ax5.set_yscale('log')  # 左侧轴设置为对数尺度\n",
    "        if right_log_scale:\n",
    "            ax6.set_yscale('log')  # 右侧轴设置为对数尺度\n",
    "        ax6.set_ylim([min(self.val_extended_lr) * 0.95, max(self.val_extended_lr) * 1.1])\n",
    "\n",
    "        # 验证集的累积损失和学习率\n",
    "        ax7 = axs[1, 1]\n",
    "        ax8 = ax7.twinx()\n",
    "        cumulative_val_losses = np.cumsum(self.val_losses)\n",
    "        ax7.plot(cumulative_val_losses, label='Cumulative Val Losses', color='blue')\n",
    "        ax8.plot(self.val_extended_lr, label='Val Learning Rate', color='red', linestyle='--')\n",
    "        ax7.set_title('Cumulative Val Losses and Learning Rate over Time')\n",
    "        ax7.set_xlabel('Steps')\n",
    "        ax7.set_ylabel('Cumulative Loss')\n",
    "        ax8.set_ylabel('Learning Rate')\n",
    "        ax7.legend(loc='upper left')\n",
    "        ax8.legend(loc='upper right')\n",
    "        if left_log_scale:\n",
    "            ax7.set_yscale('log')  # 左侧轴设置为对数尺度\n",
    "        if right_log_scale:\n",
    "            ax8.set_yscale('log')  # 右侧轴设置为对数尺度\n",
    "        ax8.set_ylim([min(self.val_extended_lr) * 0.95, max(self.val_extended_lr) * 1.1])\n",
    "\n",
    "        # 微调阶段的单次损失和学习率\n",
    "        ax9 = axs[2, 0]\n",
    "        ax10 = ax9.twinx()\n",
    "        ax9.plot(self.fine_tune_losses, label='Fine-tune Losses', color='blue')\n",
    "        ax10.plot(self.fine_tune_learning_rates, label='Fine-tune Learning Rate', color='red', linestyle='--')\n",
    "        ax9.set_title('Fine-tune Losses and Learning Rate over Time')\n",
    "        ax9.set_xlabel('Steps')\n",
    "        ax9.set_ylabel('Loss')\n",
    "        ax10.set_ylabel('Learning Rate')\n",
    "        ax9.legend(loc='upper left')\n",
    "        ax10.legend(loc='upper right')\n",
    "        if left_log_scale:\n",
    "            ax9.set_yscale('log')  # 左侧轴设置为对数尺度\n",
    "        if right_log_scale:\n",
    "            ax10.set_yscale('log')  # 右侧轴设置为对数尺度\n",
    "        ax10.set_ylim([min(self.fine_tune_learning_rates) * 0.95, max(self.fine_tune_learning_rates) * 1.05])\n",
    "\n",
    "        # 微调阶段的累积损失和学习率\n",
    "        ax11 = axs[2, 1]\n",
    "        ax12 = ax11.twinx()\n",
    "        cumulative_fine_tune_losses = np.cumsum(self.fine_tune_losses)\n",
    "        ax11.plot(cumulative_fine_tune_losses, label='Cumulative Fine-tune Losses', color='blue')\n",
    "        ax12.plot(self.fine_tune_learning_rates, label='Fine-tune Learning Rate', color='red', linestyle='--')\n",
    "        ax11.set_title('Cumulative Fine-tune Losses and Learning Rate over Time')\n",
    "        ax11.set_xlabel('Steps')\n",
    "        ax11.set_ylabel('Cumulative Loss')\n",
    "        ax12.set_ylabel('Learning Rate')\n",
    "        ax11.legend(loc='upper left')\n",
    "        ax12.legend(loc='upper right')\n",
    "        if left_log_scale:\n",
    "            ax11.set_yscale('log')  # 左侧轴设置为对数尺度\n",
    "        if right_log_scale:\n",
    "            ax12.set_yscale('log')  # 右侧轴设置为对数尺度\n",
    "        ax12.set_ylim([min(self.fine_tune_learning_rates) * 0.95, max(self.fine_tune_learning_rates) * 1.05])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "34b2ec9c0b5765cb",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 超参数调整",
   "id": "be24744398d90d92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:53.005310Z",
     "start_time": "2024-08-26T07:12:53.001736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config = DataInit.config_manager(if_print=False)\n",
    "# main_config = config"
   ],
   "id": "693ab86f6c73dc61",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:53.016497Z",
     "start_time": "2024-08-26T07:12:53.011459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial: optuna.Trial, config: DictConfig, data_manage: DataManager) -> float:\n",
    "    # 定义超参数搜索空间，使用 suggest_float 替代 suggest_uniform 和 suggest_loguniform\n",
    "    hidden_size = trial.suggest_int('hidden_size', 50, 300)  # 隐藏层神经元数量\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 4)  # LSTM层的数量\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)  # 初始学习率\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)  # L2正则化\n",
    "    factor = trial.suggest_float('factor', 0.1, 0.5)  # 学习率减少因子\n",
    "    patience_lr = trial.suggest_int('patience_lr', 2, 10)  # 学习率减少的耐心值\n",
    "    threshold = trial.suggest_float('threshold', 1e-4, 1e-2)  # 学习率减少阈值\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)  # 训练轮数\n",
    "    patience_epochs = trial.suggest_int('patience_epochs', 2, 10)  # 早停减少耐心值\n",
    "    min_delta = trial.suggest_float('min_delta', 1e-5, 1e-2, log=True)  # 早停减少阈值\n",
    "\n",
    "    # 将非超参数部分从 config 中提取，并与超参数搜索结合\n",
    "    hyperparameter_config = {\n",
    "        'base': {\n",
    "            'N': config.base.N,  # 节点数\n",
    "            'T': config.base.T,  # 总时间步数\n",
    "            'T_train_val': config.base.T_train_val,  # 训练和验证的时间步数\n",
    "            'train_ratio': config.base.train_ratio,  # 训练集占比\n",
    "            'T_train': config.base.T_train,  # 训练的时间步数\n",
    "            'T_val': config.base.T_val,  # 验证的时间步数\n",
    "            'T_test': config.base.T_test  # 测试的时间步数\n",
    "        },\n",
    "        'exp4': {\n",
    "            'seq_length': config.exp4.seq_length,  # Sequence length\n",
    "            'input_size': config.exp4.input_size,  # Input size\n",
    "            'output_size': config.exp4.output_size,  # Output size\n",
    "            'learning_rate': learning_rate,  # 由超参数搜索确定\n",
    "            'num_epochs': num_epochs,  # 由超参数搜索确定\n",
    "            'device': config.exp4.device,  # Device\n",
    "            'mix_precision': config.exp4.mix_precision,  # Mixed precision training\n",
    "            'patience_epochs': patience_epochs,  # 由超参数搜索确定\n",
    "            'min_delta': min_delta,  # 由超参数搜索确定\n",
    "            'mode': config.exp4.mode,  # 'min' 表示监控指标的值越小越好，'max' 表示监控指标的值越大越好\n",
    "            'factor': factor,  # 由超参数搜索确定\n",
    "            'patience_lr': patience_lr,  # 由超参数搜索确定\n",
    "            'min_lr': config.exp4.min_lr,  # 学习率的下限\n",
    "            'threshold': threshold,  # 由超参数搜索确定\n",
    "            'LSTMconfig': {\n",
    "                'hidden_size': hidden_size,  # 由超参数搜索确定\n",
    "                'num_layers': num_layers,  # 由超参数搜索确定\n",
    "                'weight_decay': weight_decay  # 由超参数搜索确定\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 实例化 LSTMTraining 类并进行训练\n",
    "    trainer = LSTMTraining(hyperparameter_config, data_manage, model_type='lstm')\n",
    "\n",
    "    # 执行训练过程\n",
    "    trainer.train()\n",
    "\n",
    "    # 返回验证集上的最小损失作为优化目标\n",
    "    return min(trainer.val_losses)\n"
   ],
   "id": "10e998417e80b235",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T07:12:53.026065Z",
     "start_time": "2024-08-26T07:12:53.022003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hyperparameterisation(config: DictConfig, data_manager: DataManager, data_type: str) -> optuna.study.Study:\n",
    "    db_path = data_path / f'exp4/LSTM_expert/db/{data_type}_study_db.db'\n",
    "\n",
    "    # 检查数据库文件是否存在\n",
    "    if os.path.exists(db_path):\n",
    "        # 如果数据库存在，连接到现有的 study\n",
    "        study = optuna.load_study(\n",
    "            study_name=f'{data_type}_study_db',  # 之前定义的 study 名字\n",
    "            storage=f'sqlite:///{db_path}'  # 数据库文件路径, /// 三个斜杠表示绝对路径\n",
    "        )\n",
    "        print(\"Connected to existing study.\")\n",
    "    else:\n",
    "        # 如果数据库不存在，创建一个新的 study 并开始优化\n",
    "        # 创建一个连接到 SQLite 数据库的 study，数据库文件保存在 '../Data/' 目录中\n",
    "        study = optuna.create_study(\n",
    "            direction='minimize',\n",
    "            storage=f'sqlite:///{db_path}',\n",
    "            study_name=f'{data_type}_study_db',\n",
    "        )\n",
    "        print(\"Created a new study and starting optimization.\")\n",
    "    \n",
    "        # 执行优化\n",
    "        study.optimize(lambda trial: objective(trial, config, data_manager), n_trials=50)\n",
    "    \n",
    "    return study\n",
    "        "
   ],
   "id": "57caa0be665ed614",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-26T07:12:56.169120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "iid_load_study = hyperparameterisation(config, iid_load_data_manager, 'iid_load')\n",
    "print(f'iid_load_study_db: {iid_load_study.best_params}')"
   ],
   "id": "3e0e2aa4da1c2e02",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-26 08:12:56,737] A new study created in RDB with name: iid_load_study_db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new study and starting optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 10/75 [02:02<13:23, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/75], Train_Loss: 1837.2851, Val_Loss: 1811.0750\n",
      "Current Learning Rate: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 20/75 [04:11<11:57, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/75], Train_Loss: 1286.5257, Val_Loss: 1262.9130\n",
      "Current Learning Rate: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 30/75 [06:26<09:57, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/75], Train_Loss: 812.5484, Val_Loss: 793.2705\n",
      "Current Learning Rate: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 40/75 [08:39<07:41, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/75], Train_Loss: 446.6862, Val_Loss: 433.0370\n",
      "Current Learning Rate: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 50/75 [10:55<05:27, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/75], Train_Loss: 206.7408, Val_Loss: 198.8583\n",
      "Current Learning Rate: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 60/75 [13:07<03:18, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/75], Train_Loss: 83.1950, Val_Loss: 80.0161\n",
      "Current Learning Rate: 0.000043\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ar1_load_study = hyperparameterisation(config, ar1_load_data_manager, 'ar1_load')\n",
    "print(f'ar1_load_study_db: {ar1_load_study.best_params}')"
   ],
   "id": "e23db4fb1702ce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iid_latency_study = hyperparameterisation(config, iid_latency_data_manager, 'iid_latency')\n",
    "print(f'iid_latency_study_db: {iid_latency_study.best_params}')"
   ],
   "id": "ccaf074072195ff3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ar1_latency_study = hyperparameterisation(config, ar1_latency_data_manager, 'ar1_latency')\n",
    "print(f'ar1_latency_study_db: {ar1_latency_study.best_params}')"
   ],
   "id": "72ae31a9c2df5b54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 超参数保存",
   "id": "9c8ee1862280b098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用 merge 函数合并两个配置文件\n",
    "iid_load_best_lstm_hyperparameters = OmegaConf.merge(config, iid_load_study.best_params)\n",
    "ar1_load_best_lstm_hyperparameters = OmegaConf.merge(config, ar1_load_study.best_params)\n",
    "iid_latency_best_lstm_hyperparameters = OmegaConf.merge(config, iid_latency_study.best_params)\n",
    "ar1_latency_best_lstm_hyperparameters = OmegaConf.merge(config, ar1_latency_study.best_params)"
   ],
   "id": "a089e072dd9c22c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 将合并后的配置保存为新的 YAML 文件\n",
    "with open(global_path/ \"config/lstm_best_paras/iid_load_best_lstm_hyperparameters.yaml\", \"w\") as file:\n",
    "    OmegaConf.save(config=iid_load_best_lstm_hyperparameters, f=file)\n",
    "\n",
    "with open(global_path/ \"config/lstm_best_paras/ar1_load_best_lstm_hyperparameters.yaml\", \"w\") as file:\n",
    "    OmegaConf.save(config=ar1_load_best_lstm_hyperparameters, f=file)\n",
    "    \n",
    "with open(global_path/ \"config/lstm_best_paras/iid_latency_best_lstm_hyperparameters.yaml\", \"w\") as file:\n",
    "    OmegaConf.save(config=iid_latency_best_lstm_hyperparameters, f=file)\n",
    "    \n",
    "with open(global_path/ \"config/lstm_best_paras/ar1_latency_best_lstm_hyperparameters.yaml\", \"w\") as file:\n",
    "    OmegaConf.save(config=ar1_latency_best_lstm_hyperparameters, f=file)"
   ],
   "id": "9f1f68b94cdbea2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 绘制超参数相关可视化图表",
   "id": "65a5fa3c9080b34e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 绘制优化历史图\n",
    "vis.plot_optimization_history(iid_load_best_lstm_hyperparameters).show()\n"
   ],
   "id": "d0d109e3a06b9698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 绘制超参数重要性图\n",
    "vis.plot_param_importances(iid_load_best_lstm_hyperparameters).show()\n"
   ],
   "id": "1bd656a05981bfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 超参数与目标值的关系图（Hyperparameter vs Objective）\n",
    "vis.plot_parallel_coordinate(iid_load_best_lstm_hyperparameters)"
   ],
   "id": "84913910203e3de2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 多个超参数之间的关系\n",
    "vis.plot_slice(iid_load_best_lstm_hyperparameters)"
   ],
   "id": "c565614d6dccf09d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 绘制平行坐标图\n",
    "vis.plot_parallel_coordinate(iid_load_best_lstm_hyperparameters).show()\n"
   ],
   "id": "5ea870d704ba4d88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 绘制等高线图\n",
    "vis.plot_contour(iid_load_best_lstm_hyperparameters).show()\n"
   ],
   "id": "d81f0085e221a5f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 超参数关系图（Hyperparameter Relationships）\n",
    "vis.plot_slice(iid_load_best_lstm_hyperparameters)"
   ],
   "id": "d9f794f49008e0f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 绘制超参数与目标值的切片图\n",
    "vis.plot_slice(iid_load_best_lstm_hyperparameters).show()\n"
   ],
   "id": "a58a83b969b7e85b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Best hyperparameters:  {'hidden_size': 102, 'num_layers': 3, 'dropout_prob': 0.31507582808498547, 'learning_rate': 0.007784421116708152, 'weight_decay': 0.00030763727804265344, 'factor': 0.10457911623121284, 'patience_lr': 2, 'threshold': 0.007191997097747425, 'batch_size': 36, 'num_epochs': 68, 'patience_epochs': 7, 'min_delta': 0.0004242904281004015}",
   "id": "50c9a058a369551b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型训练",
   "id": "33718a11d7c0206b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def lstm_train_test(config: dict, data_manage: object, data_type:str) -> None:\n",
    "    lstm_trainer = LSTMTraining(config, data_manage)\n",
    "    lstm_trainer.train()\n",
    "    \n",
    "    data_manage.print_dataloader_info(data_manage.train_dataloader, 'train')\n",
    "    data_manage.print_dataloader_info(data_manage.val_dataloader, 'val')\n",
    "    \n",
    "    print(f'len(lstm_trainer.train_losses):{len(lstm_trainer.train_losses)}, len(lstm_trainer.val_losses):{len(lstm_trainer.val_losses)}, len(lstm_trainer.train_with_val_learning_rates):{len(lstm_trainer.train_with_val_learning_rates)}')\n",
    "    print(f'len(lstm_trainer.train_losses) / len(lstm_trainer.train_with_val_learning_rates):{len(lstm_trainer.train_losses) / len(lstm_trainer.train_with_val_learning_rates)}')\n",
    "    print(f'len(lstm_trainer.val_losses) / len(lstm_trainer.train_with_val_learning_rates):{len(lstm_trainer.val_losses) / len(lstm_trainer.train_with_val_learning_rates)}')\n",
    "    \n",
    "    print(f'lstm_trainer.train_predictions: {lstm_trainer.train_predictions}')\n",
    "    print(f'lstm_trainer.val_predictions: {lstm_trainer.val_predictions}')\n",
    "    print(f'lstm_trainer.train_predictions.shape: {lstm_trainer.train_predictions.shape}; lstm_trainer.val_predictions.shape: {lstm_trainer.val_predictions.shape}')\n",
    "    \n",
    "    lstm_trainer.fine_tune()\n",
    "    \n",
    "    print(f'lstm_trainer.train_predictions.shape:{lstm_trainer.train_predictions.shape}, lstm_trainer.val_predictions.shape:{lstm_trainer.val_predictions.shape}, lstm_trainer.test_predictions.shape:{lstm_trainer.test_predictions.shape}')\n",
    "    print(f'len(lstm_trainer.train_losses):{len(lstm_trainer.train_losses)}, len(lstm_trainer.val_losses):{len(lstm_trainer.val_losses)}, len(lstm_trainer.fine_tune_losses):{len(lstm_trainer.fine_tune_losses)}')\n",
    "    print(f'len(lstm_trainer.train_with_val_learning_rates):{len(lstm_trainer.train_with_val_learning_rates)}, len(lstm_trainer.fine_tune_learning_rates):{len(lstm_trainer.fine_tune_learning_rates)}')\n",
    "    \n",
    "    lstm_trainer.plot_loss()  # 绘制训练和验证损失\n",
    "    lstm_trainer.plot_loss_and_lr()  # 同时绘制损失和学习率变化曲线\n",
    "    lstm_trainer.plot_full(0)\n",
    "    lstm_trainer.plot_train(0)\n",
    "    lstm_trainer.plot_val(0)\n",
    "    lstm_trainer.plot_test(0)\n",
    "    lstm_trainer.plot_range(7500, 8000, 0)\n",
    "    lstm_trainer.plot_range(7900, 8100, 1)\n",
    "    lstm_trainer.plot_range(7960, 8100, 1)\n",
    "    lstm_trainer.plot_range(9960, 10100, 1)\n",
    "    lstm_trainer.plot_all_losses_and_lr_0()  # 绘制损失和学习率曲线\n",
    "    lstm_trainer.plot_all_losses_and_lr_1(left_log_scale=False, right_log_scale=False)  # 绘制损失和学习率曲线\n",
    "    lstm_trainer.plot_all_losses_and_lr_1(left_log_scale=False, right_log_scale=True)  # 绘制损失和学习率曲线\n",
    "    lstm_trainer.plot_all_losses_and_lr_1(left_log_scale=True, right_log_scale=True)  # 绘制损失和学习率曲线\n",
    "    \n",
    "    np.savetxt(data_path/ f'/LSTM_expert/predictions/LSTM_{data_type}_predictions.csv', lstm_trainer.test_predictions.detach().cpu().numpy().T[:, -config.base.T_test:], delimiter=',')"
   ],
   "id": "bc5a671a4133312c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lstm_train_test(iid_load_best_lstm_hyperparameters, iid_load_data_manager)",
   "id": "3445a592a997f131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lstm_train_test(ar1_load_best_lstm_hyperparameters, ar1_load_data_manager)",
   "id": "29df50627bbe21fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lstm_train_test(iid_latency_best_lstm_hyperparameters, iid_latency_data_manager)",
   "id": "41666b91c6d23550"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lstm_train_test(ar1_latency_best_lstm_hyperparameters, ar1_latency_data_manager)",
   "id": "779c3aa15c8412f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24abcd2a64102d63",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
