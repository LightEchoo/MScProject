{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9fc61a614ccc80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:49.814295Z",
     "start_time": "2024-08-25T13:18:49.725760Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from importlib_metadata.compat.py39 import ep_matches\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbfe7a5130d5fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:52.741392Z",
     "start_time": "2024-08-25T13:18:49.823303Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 将 MScProject 目录添加到 Python 模块搜索路径\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "# 从 DataGenerate.py 导入 DataGenerate 类 用于pickle导入\n",
    "from DataInit import DataManager, RewardDataManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57241edab107fd99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:52.821026Z",
     "start_time": "2024-08-25T13:18:52.802351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Config Info ----------\n",
      "path:\n",
      "  global_path: E:/Study in the UK/Project/MScProject\n",
      "base:\n",
      "  'N': 10\n",
      "  T: 11000\n",
      "  T_train_val: 10000\n",
      "  train_ratio: 0.8\n",
      "  T_train: 8000\n",
      "  T_val: 2000\n",
      "  T_test: 1000\n",
      "  lambda_load: 0.5\n",
      "  top_k:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "data_generation:\n",
      "  load_data:\n",
      "    node_load_mean_mean: 50.0\n",
      "    node_load_mean_var: 10.0\n",
      "    node_load_iid_var: 5.0\n",
      "    node_load_ar1_theta: 0.9\n",
      "  latency_data:\n",
      "    node_latency_mean_mean: 30.0\n",
      "    node_latency_mean_var: 10.0\n",
      "    node_latency_ar1_theta: 0.9\n",
      "  reward_parameters:\n",
      "    iid:\n",
      "      alpha_load_0: 40.0\n",
      "      alpha_latency_1: 0.041\n",
      "    ar1:\n",
      "      alpha_load_0: 40.0\n",
      "      alpha_latency_1: 0.041\n",
      "  reward_parameters_slider:\n",
      "    alpha_load_0:\n",
      "      value: 1.0\n",
      "      min: 0.001\n",
      "      max: 40.0\n",
      "      step: 0.01\n",
      "      description: alpha_load_0\n",
      "    alpha_latency_0:\n",
      "      value: 1.0\n",
      "      min: 0.001\n",
      "      max: 6.0\n",
      "      step: 0.01\n",
      "      description: alpha_latency_0\n",
      "    alpha_latency_1:\n",
      "      value: 0.5\n",
      "      min: 0.0001\n",
      "      max: 0.5\n",
      "      step: 0.001\n",
      "      description: alpha_latency_1\n",
      "epsilon_greedy:\n",
      "  dynamic_plot:\n",
      "    epsilon_min: 0.001\n",
      "    epsilon_max: 0.5\n",
      "    epsilon_step: 0.01\n",
      "    epsilon_default_value: 0.1\n",
      "  epsilon_vs_cumulative_regret:\n",
      "    epsilon_min: 0.0001\n",
      "    epsilon_max: 0.2001\n",
      "    epsilon_step: 0.0005\n",
      "adaptive_epsilon_greedy:\n",
      "  dynamic_plot:\n",
      "    init_epsilon_min: 0.001\n",
      "    init_epsilon_max: 0.5\n",
      "    init_epsilon_step: 0.01\n",
      "    init_epsilon_default_value: 0.1\n",
      "    min_epsilon_min: 0.001\n",
      "    min_epsilon_max: 0.1\n",
      "    min_epsilon_step: 0.001\n",
      "    min_epsilon_default_value: 0.05\n",
      "  epsilon_vs_cumulative_regret:\n",
      "    epsilon_min: 0.0001\n",
      "    epsilon_max: 0.2001\n",
      "    epsilon_step: 0.0005\n",
      "    min_epsilon: 0.05\n",
      "dynamic_adaptive_epsilon_greedy:\n",
      "  min_epsilon: 0.05\n",
      "  max_epsilon: 0.8\n",
      "  dynamic_plot:\n",
      "    init_epsilon_min: 0.001\n",
      "    init_epsilon_max: 0.5\n",
      "    init_epsilon_step: 0.01\n",
      "    init_epsilon_default_value: 0.1\n",
      "    percentiles_min: 50\n",
      "    percentiles_max: 100\n",
      "    percentiles_step: 1\n",
      "    percentiles_default_value: 80\n",
      "  epsilon_vs_cumulative_regret:\n",
      "    epsilon_min: 0.0001\n",
      "    epsilon_max: 0.2001\n",
      "    epsilon_step: 0.0005\n",
      "    percentiles: 80\n",
      "boltzmann:\n",
      "  dynamic_plot:\n",
      "    temperature_min: 0.01\n",
      "    temperature_max: 1.01\n",
      "    temperature_step: 0.001\n",
      "    temperature_default_value: 0.01\n",
      "  temperature_vs_cumulative_regret:\n",
      "    temperature_min: 0.01\n",
      "    temperature_max: 1.01\n",
      "    temperature_step: 0.001\n",
      "ucb:\n",
      "  dynamic_plot:\n",
      "    c_min: 0.1\n",
      "    c_max: 3.1\n",
      "    c_step: 0.1\n",
      "    c_default_value: 1.5\n",
      "  c_vs_cumulative_regret:\n",
      "    c_min: 0.1\n",
      "    c_max: 3.1\n",
      "    c_step: 0.1\n",
      "exp4:\n",
      "  batch_size: 64\n",
      "  seq_length: 20\n",
      "  input_size: 10\n",
      "  output_size: 10\n",
      "  learning_rate: 0.001\n",
      "  num_workers: 16\n",
      "  num_epochs: 100\n",
      "  device: cuda\n",
      "  mix_precision: true\n",
      "  patience_epochs: 6\n",
      "  min_delta: 0.001\n",
      "  mode: min\n",
      "  factor: 0.1\n",
      "  patience_lr: 2\n",
      "  min_lr: 1.0e-06\n",
      "  threshold: 0.01\n",
      "  ARconfig:\n",
      "    order: 5\n",
      "  LSTMconfig:\n",
      "    hidden_size: 128\n",
      "    num_layers: 4\n",
      "    dropout_prob: 0.2\n",
      "    weight_decay: 0.0001\n",
      "  GNNconfig:\n",
      "    hidden_size: 128\n",
      "    num_layers: 4\n",
      "\n",
      "---------- Path Info ----------\n",
      "Global Path: E:\\Study in the UK\\Project\\MScProject\n",
      "Data Path: E:\\Study in the UK\\Project\\MScProject\\Data\n"
     ]
    }
   ],
   "source": [
    "# 加载 config.yaml 文件\n",
    "config = OmegaConf.load(\"config/config.yaml\")\n",
    "global_path = Path(config.path.global_path)\n",
    "data_path = global_path / 'Data'\n",
    "# 打印完整的配置内容\n",
    "print(f'---------- Config Info ----------')\n",
    "print(OmegaConf.to_yaml(config))\n",
    "# 打印全局路径和数据路径\n",
    "print(f'---------- Path Info ----------')\n",
    "print(f'Global Path: {global_path}')\n",
    "print(f'Data Path: {data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e431905c2f1644c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:53.729701Z",
     "start_time": "2024-08-25T13:18:53.500587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "def import_data_manager(data_type: str) -> DataManager:\n",
    "    \"\"\"\n",
    "    加载之前保存的数据管理对象。\n",
    "\n",
    "    :param data_path: 数据保存的路径\n",
    "    :param data_type: 数据类型，例如 'iid_load', 'ar1_load', 'iid_latency', 'ar1_latency'\n",
    "    :return: 加载的 DataManager 对象\n",
    "    \"\"\"\n",
    "    file_path = data_path / f'{data_type}_data_manage.pkl'\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data_manager = pickle.load(f)\n",
    "\n",
    "    return data_manager\n",
    "\n",
    "# 示例调用\n",
    "load_iid_data_manage = import_data_manager('iid_load')\n",
    "load_ar1_data_manage = import_data_manager('ar1_load')\n",
    "latency_iid_data_manage = import_data_manager('iid_latency')\n",
    "latency_ar1_data_manage = import_data_manager('ar1_latency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e367a16b28d13c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:53.978219Z",
     "start_time": "2024-08-25T13:18:53.975007Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 绘制数据\n",
    "# load_iid_data_manage.plot_range_data(load_iid_data_manage.data_np[:3, :], title='Load IID Data')\n",
    "# load_ar1_data_manage.plot_range_data(load_ar1_data_manage.data_np[:3, :], title='Load AR(1) Data')\n",
    "# latency_iid_data_manage.plot_range_data(latency_iid_data_manage.data_np[:3, :], title='Latency IID Data')\n",
    "# latency_ar1_data_manage.plot_range_data(latency_ar1_data_manage.data_np[:3, :], title='Latency AR(1) Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d68cec9d676d1478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:54.399431Z",
     "start_time": "2024-08-25T13:18:54.391305Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path/'reward_data_manager.pkl', 'rb') as f:\n",
    "    reward_data_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd458115e0a0092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:54.795800Z",
     "start_time": "2024-08-25T13:18:54.792419Z"
    }
   },
   "outputs": [],
   "source": [
    "# reward_data_manager.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52cd1d904fe2e42f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:55.056535Z",
     "start_time": "2024-08-25T13:18:55.053829Z"
    }
   },
   "outputs": [],
   "source": [
    "# reward_data_manager.plot_reward_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:55.415162Z",
     "start_time": "2024-08-25T13:18:55.390285Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import re\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee6c999b964cf89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:55.721996Z",
     "start_time": "2024-08-25T13:18:55.717364Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"简单滑动平均平滑\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "def exponential_moving_average(data, alpha=0.01):\n",
    "    \"\"\"指数加权平均平滑\"\"\"\n",
    "    smoothed_data = np.zeros_like(data)\n",
    "    smoothed_data[0] = data[0]\n",
    "    for t in range(1, len(data)):\n",
    "        smoothed_data[t] = alpha * data[t] + (1 - alpha) * smoothed_data[t-1]\n",
    "    return smoothed_data\n",
    "\n",
    "def gaussian_smooth(data, sigma=15):\n",
    "    \"\"\"高斯平滑\"\"\"\n",
    "    return gaussian_filter1d(data, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fbd929fb7538085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T13:18:56.113446Z",
     "start_time": "2024-08-25T13:18:56.109907Z"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, widgets, HBox, VBox, Output, interactive_output, interactive, GridBox, Layout\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b90287fb853797c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB:\n",
    "    def __init__(self, config, reward_data_manager):\n",
    "        self.config = config\n",
    "        self.reward_data_manager = reward_data_manager\n",
    "\n",
    "        # 初始化通用参数\n",
    "        self.lambda_load = self.config.base.lambda_load\n",
    "        self.top_k = self.config.base.top_k\n",
    "        self.N = self.config.base.N\n",
    "        self.T_test = self.config.base.T_test\n",
    "\n",
    "        # 加载数据\n",
    "        self.iid_load = reward_data_manager.iid_load\n",
    "        self.iid_load_reward_0 = reward_data_manager.iid_load_reward_0\n",
    "        self.iid_load_reward_1 = reward_data_manager.iid_load_reward_1\n",
    "        self.iid_latency = reward_data_manager.iid_latency\n",
    "        self.iid_latency_reward_1 = reward_data_manager.iid_latency_reward_1\n",
    "        self.ar1_load = reward_data_manager.ar1_load\n",
    "        self.ar1_load_reward_0 = reward_data_manager.ar1_load_reward_0\n",
    "        self.ar1_load_reward_1 = reward_data_manager.ar1_load_reward_1\n",
    "        self.ar1_latency = reward_data_manager.ar1_latency\n",
    "        self.ar1_latency_reward_1 = reward_data_manager.ar1_latency_reward_1\n",
    "\n",
    "        # 定义一个映射字典\n",
    "        self.reward_mapping = {\n",
    "            ('iid', 'reward_0'): (self.iid_load_reward_0, self.iid_latency_reward_1),\n",
    "            ('iid', 'reward_1'): (self.iid_load_reward_1, self.iid_latency_reward_1),\n",
    "            ('ar1', 'reward_0'): (self.ar1_load_reward_0, self.ar1_latency_reward_1),\n",
    "            ('ar1', 'reward_1'): (self.ar1_load_reward_1, self.ar1_latency_reward_1),\n",
    "        }\n",
    "        \n",
    "        self.load_reward_method = None\n",
    "        self.data_type = None\n",
    "        self.load_reward, self.latency_reward = None, None\n",
    "        self.combine_reward = None\n",
    "        self.combine_reward_mean = None\n",
    "        self.combine_reward_optimal_node = None\n",
    "        self.combine_reward_optimal_mean = None\n",
    "        self.combine_reward_sorted_mean = None\n",
    "\n",
    "        self.algorithm = None\n",
    "\n",
    "    def set_parameters(self, load_reward_method: str, data_type: str) -> None:\n",
    "        \"\"\"\n",
    "        设置参数并计算组合奖励。\n",
    "        :param load_reward_method: 加载奖励的方法，'reward_0' 或 'reward_1'\n",
    "        :param data_type: 数据类型，'iid' 或 'ar1'\n",
    "        \"\"\"\n",
    "        # 重新设置参数\n",
    "        self.load_reward_method = load_reward_method\n",
    "        self.data_type = data_type\n",
    "        try:\n",
    "            self.load_reward, self.latency_reward = self.reward_mapping[(data_type, load_reward_method)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f'Invalid load_reward_method: {load_reward_method}, data_type: {data_type}')\n",
    "\n",
    "        # 计算组合奖励\n",
    "        self.combine_reward = self.lambda_load * self.load_reward + (1 - self.lambda_load) * self.latency_reward\n",
    "        self.combine_reward_mean = np.mean(self.combine_reward, axis=1)\n",
    "        self.combine_reward_optimal_node = np.argmax(self.combine_reward_mean)\n",
    "        self.combine_reward_optimal_mean = np.max(self.combine_reward_mean)\n",
    "        self.combine_reward_sorted_mean = np.argsort(self.combine_reward_mean)[::-1]\n",
    "\n",
    "    def calculate_top_k_accuracy(self, time_counts: np.ndarray, k_list: list) -> dict:\n",
    "        \"\"\"\n",
    "        计算 MAB 过程的 Top-k Accuracy（定义二）。\n",
    "        \n",
    "        参数:\n",
    "        - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "        - k_list: 一个整数列表，表示需要计算的 k 值。\n",
    "        \n",
    "        返回:\n",
    "        - 一个字典，key 是 k 的值，value 是对应的 Top-k Accuracy。\n",
    "        \n",
    "        说明:\n",
    "        - Top-k Accuracy 的定义二：在 T_test 个时间步中，选择的节点中有多少是最优节点的前 k 个节点。\n",
    "        \"\"\"\n",
    "        \n",
    "        top_k_accuracy = {}\n",
    "        T = len(time_counts)\n",
    "\n",
    "        for k in k_list:\n",
    "            correct_count = 0\n",
    "            optimal_nodes = self.combine_reward_sorted_mean[:k]  # 选择前 k 个最佳节点\n",
    "            for t in range(T):\n",
    "                if time_counts[t] in optimal_nodes:\n",
    "                    correct_count += 1\n",
    "            accuracy = correct_count / T\n",
    "            top_k_accuracy[k] = accuracy\n",
    "\n",
    "        return top_k_accuracy\n",
    "\n",
    "    def plot_combine_rewards(self, selected_nodes=None) -> None:\n",
    "        \"\"\"\n",
    "        绘制组合奖励的图形。\n",
    "        \n",
    "        参数:\n",
    "        - selected_nodes: 一个列表，表示选择的节点的索引。默认为 None，表示选择前 3 个节点。\n",
    "        \n",
    "        说明:\n",
    "        - 组合奖励是加载奖励和延迟奖励的加权和。\n",
    "        \"\"\"\n",
    "        \n",
    "        if selected_nodes is None:\n",
    "            selected_nodes = [0, 1, 2]  # 默认选择前3个节点\n",
    "    \n",
    "        # 数据准备\n",
    "        last_T_data = self.combine_reward[selected_nodes, -self.T_test:]\n",
    "    \n",
    "        # 使用 constrained_layout 进行更智能的布局管理\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    \n",
    "        # 1. combine_reward中选择节点的最后T_test次数据的展示\n",
    "        for i, node in enumerate(selected_nodes):\n",
    "            axs[0].plot(last_T_data[i], label=f'Node {node}')\n",
    "        axs[0].set_title('Last T_test Data - Selected Nodes')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "    \n",
    "        # 2. N个节点的均值的分布图，并标注最值\n",
    "        means = self.combine_reward_mean\n",
    "        axs[1].plot(means, 'bo-', label='Mean Value per Node')\n",
    "        axs[1].axhline(y=np.min(means), color='red', linestyle='--', label='Range')\n",
    "        axs[1].axhline(y=np.max(means), color='red', linestyle='--')\n",
    "        axs[1].set_title('Mean Values of Nodes')\n",
    "        axs[1].set_xlabel('Node')\n",
    "        axs[1].set_ylabel('Mean Value')\n",
    "        axs[1].legend()\n",
    "        \n",
    "        # 调整标注的位置，避免超出图形范围\n",
    "        axs[1].annotate(f'Min: {np.min(means):.3f}', xy=(np.argmin(means), np.min(means)), \n",
    "                        xytext=(np.argmin(means) + 0.5, np.min(means) - 0.05),\n",
    "                        arrowprops=dict(facecolor='black', arrowstyle='->'), fontsize=10, color='black')\n",
    "        axs[1].annotate(f'Max: {np.max(means):.3f}', xy=(np.argmax(means), np.max(means)), \n",
    "                        xytext=(np.argmax(means) + 0.5, np.max(means) + 0.05),\n",
    "                        arrowprops=dict(facecolor='black', arrowstyle='->'), fontsize=10, color='black')\n",
    "    \n",
    "        # 3. 选择节点的最后T_test次数据的直方图\n",
    "        for i, node in enumerate(selected_nodes):\n",
    "            axs[2].hist(last_T_data[i], alpha=0.5, label=f'Node {node}')\n",
    "        axs[2].set_title('Histogram - Selected Nodes')\n",
    "        axs[2].set_xlabel('Value')\n",
    "        axs[2].set_ylabel('Frequency')\n",
    "        axs[2].legend()\n",
    "    \n",
    "        plt.show()\n",
    "        \n",
    "    class EpsilonGreedy:\n",
    "        \"\"\"\n",
    "        Epsilon-Greedy 算法类。\n",
    "        这是最初级的多臂老虎机算法，根据 epsilon 的值以一定概率随机选择节点，以 1-epsilon 的概率选择当前平均奖励最高的节点。\n",
    "        \n",
    "        功能：\n",
    "        - algorithm: 运行 epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "        - plot_results: 绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾以及 Top-k Accuracy，并应用平滑。\n",
    "        - plot_epsilon_vs_cumulative_regret: 绘制 epsilon vs cumulative regret 图。\n",
    "        - dynamic_plot: 动态可视化 epsilon-greedy 算法的结果，epsilon 可以通过滑块调整。\n",
    "        - run_and_plot: 运行 epsilon-greedy 算法并绘制结果，用于动态可视化图中的 epsilon 可以通过滑块调整。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab = mab\n",
    "            self.config_eg = self.mab.config.epsilon_greedy\n",
    "\n",
    "            self.dynamic_epsilon_min = self.config_eg.dynamic_plot.epsilon_min  # 动态 epsilon 的最小值\n",
    "            self.dynamic_epsilon_max = self.config_eg.dynamic_plot.epsilon_max  # 动态 epsilon 的最大值\n",
    "            self.dynamic_epsilon_step = self.config_eg.dynamic_plot.epsilon_step  # 动态 epsilon 的步长\n",
    "            self.dynamic_epsilon_default_value = self.config_eg.dynamic_plot.epsilon_default_value  # 动态 epsilon 的默认值\n",
    "            \n",
    "            self.e_vs_cr_epsilon_min = self.config_eg.epsilon_vs_cumulative_regret.epsilon_min  # epsilon vs cumulative regret 图的 epsilon 最小值\n",
    "            self.e_vs_cr_epsilon_max = self.config_eg.epsilon_vs_cumulative_regret.epsilon_max  # epsilon vs cumulative regret 图的 epsilon 最大值\n",
    "            self.e_vs_cr_epsilon_step = self.config_eg.epsilon_vs_cumulative_regret.epsilon_step  # epsilon vs cumulative regret 图的 epsilon 步长\n",
    "\n",
    "        def algorithm(self, epsilon):\n",
    "            def choose_node(epsilon, estimated_means):\n",
    "                if np.random.rand() < epsilon:\n",
    "                    return np.random.randint(self.mab.N)\n",
    "                else:\n",
    "                    return np.argmax(estimated_means)\n",
    "\n",
    "            estimated_means = np.zeros(self.mab.N)\n",
    "            time_counts = np.zeros(self.mab.T_test)\n",
    "            nodes_counts = np.zeros(self.mab.N)\n",
    "            single_step_regret = np.zeros(self.mab.T_test)\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                chosen_node = choose_node(epsilon, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, epsilon: float) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾以及 Top-k Accuracy，并应用平滑。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy\n",
    "            - epsilon: 当前实验使用的 epsilon 值。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            \n",
    "            fig.suptitle(f\"Epsilon-Greedy Results\\nEpsilon: {epsilon}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：单次遗憾的平滑曲线\n",
    "            axs[0, 1].plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            axs[0, 1].set_title('Single Step Regret (Smoothed)')\n",
    "            axs[0, 1].set_xlabel('Time Step')\n",
    "            axs[0, 1].set_ylabel('Regret')\n",
    "\n",
    "            # 标记最大值和最小值\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            axs[0, 1].annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                               xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                               xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                               arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                               fontsize=10, color='blue')\n",
    "\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            axs[0, 1].annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                               xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                               xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                               arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                               fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：累积遗憾的平滑曲线\n",
    "            axs[1, 0].plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "            axs[1, 0].set_title('Cumulative Regret (Smoothed)')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最大值和最小值\n",
    "            min_idx = np.argmin(smoothed_cumulative_regret)\n",
    "            axs[1, 0].annotate(f'Min (x={min_idx}, y={smoothed_cumulative_regret[min_idx]:.2f})',\n",
    "                               xy=(min_idx, smoothed_cumulative_regret[min_idx]),\n",
    "                               xytext=(min_idx, smoothed_cumulative_regret[min_idx] + 0.05),\n",
    "                               arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                               fontsize=10, color='blue')\n",
    "\n",
    "            max_idx = np.argmax(smoothed_cumulative_regret)\n",
    "            axs[1, 0].annotate(f'Max (x={max_idx}, y={smoothed_cumulative_regret[max_idx]:.2f})',\n",
    "                               xy=(max_idx, smoothed_cumulative_regret[max_idx]),\n",
    "                               xytext=(max_idx, smoothed_cumulative_regret[max_idx] - 0.05),\n",
    "                               arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                               fontsize=10, color='red')\n",
    "\n",
    "            # 子图4：Top-k Accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95)) # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_epsilon_vs_cumulative_regret(self):\n",
    "            \"\"\"\n",
    "            绘制 epsilon vs cumulative regret 图。\n",
    "            \"\"\"\n",
    "            epsilon_values = np.arange(self.e_vs_cr_epsilon_min, self.e_vs_cr_epsilon_max, self.e_vs_cr_epsilon_step)\n",
    "            cumulative_regrets = []\n",
    "        \n",
    "            for epsilon in epsilon_values:\n",
    "                _, _, _, cumulative_regret, _ = self.algorithm(epsilon)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "        \n",
    "            min_idx = np.argmin(cumulative_regrets)\n",
    "            max_idx = np.argmax(cumulative_regrets)\n",
    "        \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(epsilon_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            plt.title('Epsilon vs Cumulative Regret')\n",
    "            plt.xlabel('Epsilon')\n",
    "            plt.ylabel('Cumulative Regret')\n",
    "        \n",
    "            plt.annotate(f'Min (x={epsilon_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                         xy=(epsilon_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                         xytext=(epsilon_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "        \n",
    "            plt.annotate(f'Max (x={epsilon_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                         xy=(epsilon_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                         xytext=(epsilon_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "        \n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    class AdaptiveEpsilonGreedy:\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon-Greedy 算法类。\n",
    "        在 Epsilon-Greedy 算法的基础上，根据时间步调整 epsilon 的值。\n",
    "        越靠后，epsilon 越小，即 epsilon = 1 - t / T_test。\n",
    "        但是为了保证 epsilon 不会小于最小值，因此设置了一个 min_epsilon 参数，表示算法中可以使用的最小 epsilon 值。\n",
    "        原先的 epsilon greedy 算法中的 epsilon 在此算法中相当于初始值。\n",
    "        \n",
    "        func:\n",
    "        - algorithm: 运行 adaptive epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "        - plot_results: 绘制 adaptive epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy 以及 epsilon 随时间的变化。\n",
    "        - plot_epsilon_vs_cumulative_regret: 绘制 epsilon vs cumulative regret 图。\n",
    "        - dynamic_plot: 动态可视化 adaptive epsilon-greedy 算法的结果，init_epsilon 和 min_epsilon 可以通过滑块调整。\n",
    "        - run_and_plot: 运行 adaptive epsilon-greedy 算法并绘制结果，用于动态可视化图中的 init_epsilon 和 min_epsilon 可以通过滑块调整。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab = mab\n",
    "            self.config_aeg = self.mab.config.adaptive_epsilon_greedy\n",
    "\n",
    "            self.dynamic_init_epsilon_min = self.config_aeg.dynamic_plot.init_epsilon_min  # 动态 epsilon 的最小值\n",
    "            self.dynamic_init_epsilon_max = self.config_aeg.dynamic_plot.init_epsilon_max  # 动态 epsilon 的最大值\n",
    "            self.dynamic_init_epsilon_step = self.config_aeg.dynamic_plot.init_epsilon_step  # 动态 epsilon 的步长\n",
    "            self.dynamic_init_epsilon_default_value = self.config_aeg.dynamic_plot.init_epsilon_default_value  # 动态 epsilon 的默认值\n",
    "\n",
    "            self.dynamic_min_epsilon_min = self.config_aeg.dynamic_plot.min_epsilon_min  # 动态 min_epsilon 的最小值\n",
    "            self.dynamic_min_epsilon_max = self.config_aeg.dynamic_plot.min_epsilon_max  # 动态 min_epsilon 的最大值\n",
    "            self.dynamic_min_epsilon_step = self.config_aeg.dynamic_plot.min_epsilon_step  # 动态 min_epsilon 的步长\n",
    "            self.dynamic_min_epsilon_default_value = self.config_aeg.dynamic_plot.min_epsilon_default_value  # 动态 min_epsilon 的默认值\n",
    "\n",
    "            self.e_vs_cr_epsilon_min = self.config_aeg.epsilon_vs_cumulative_regret.epsilon_min  # epsilon vs cumulative regret 图的 epsilon 最小值\n",
    "            self.e_vs_cr_epsilon_max = self.config_aeg.epsilon_vs_cumulative_regret.epsilon_max  # epsilon vs cumulative regret 图的 epsilon 最大值\n",
    "            self.e_vs_cr_epsilon_step = self.config_aeg.epsilon_vs_cumulative_regret.epsilon_step  # epsilon vs cumulative regret 图的 epsilon 步长\n",
    "            self.e_vs_cr_min_epsilon = self.config_aeg.epsilon_vs_cumulative_regret.min_epsilon  # epsilon vs cumulative regret 图的算法可取到的 epsilon 最小值\n",
    "\n",
    "        def algorithm(self, init_epsilon: float, min_epsilon: float):\n",
    "            epsilon = init_epsilon\n",
    "            def choose_node(epsilon, estimated_means):\n",
    "                if np.random.rand() < epsilon:\n",
    "                    return np.random.randint(self.mab.N)\n",
    "                else:\n",
    "                    return np.argmax(estimated_means)\n",
    "\n",
    "            estimated_means = np.zeros(self.mab.N)\n",
    "            time_counts = np.zeros(self.mab.T_test)\n",
    "            nodes_counts = np.zeros(self.mab.N)\n",
    "            single_step_regret = np.zeros(self.mab.T_test)\n",
    "            epsilon_time = np.zeros(self.mab.T_test)\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                epsilon = max(min_epsilon, epsilon * (1 - t / self.mab.T_test))\n",
    "                epsilon_time[t] = epsilon\n",
    "                chosen_node = choose_node(epsilon, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, epsilon: float, epsilon_time: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy，以及 epsilon 随时间的变化。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - epsilon: 当前实验使用的 epsilon 值。\n",
    "            - epsilon_time: 一个数组，表示每个时间步的 epsilon 值（适用于 adaptive_epsilon_greedy）。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "            fig.suptitle(f\"Adaptive Epsilon-Greedy Results\\nInitial Epsilon: {epsilon}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 epsilon_time 随时间的变化\n",
    "            axs[1, 0].plot(epsilon_time, marker='o', linestyle='-', color='purple')\n",
    "            axs[1, 0].set_title('Epsilon over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Epsilon')\n",
    "\n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_epsilon_vs_cumulative_regret(self):\n",
    "            epsilon_values = np.arange(self.e_vs_cr_epsilon_min, self.e_vs_cr_epsilon_max, self.e_vs_cr_epsilon_step)\n",
    "            cumulative_regrets = []\n",
    "\n",
    "            for epsilon in epsilon_values:\n",
    "                _, _, _, cumulative_regret, _, _ = self.algorithm(epsilon, self.e_vs_cr_min_epsilon)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "\n",
    "            min_idx = np.argmin(cumulative_regrets)\n",
    "            max_idx = np.argmax(cumulative_regrets)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(epsilon_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            plt.title('Epsilon vs Cumulative Regret')\n",
    "            plt.xlabel('Epsilon')\n",
    "            plt.ylabel('Cumulative Regret')\n",
    "\n",
    "            plt.annotate(f'Min (x={epsilon_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                         xy=(epsilon_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                         xytext=(epsilon_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            plt.annotate(f'Max (x={epsilon_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                         xy=(epsilon_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                         xytext=(epsilon_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    class DynamicAdaptiveEpsilonGreedy:\n",
    "        \"\"\"\n",
    "        Dynamic Adaptive Epsilon-Greedy 算法类。\n",
    "        在Adaptive Epsilon-Greedy算法的基础上，根据单步遗憾调整epsilon。\n",
    "        最初是手动设置阈值，当单步遗憾超过阈值时，增加epsilon，否则减小epsilon。\n",
    "        （最初始的变化幅度是init_alpha * 上一次的单步遗憾。）\n",
    "        当前使用的变化幅度是遗憾值与阈值之间的差距，归一化处理后的相对位置。\n",
    "        但是这种方法不够灵活，因此改为根据单步遗憾的百分位数调整epsilon。\n",
    "        \n",
    "        func:\n",
    "        - algorithm: 运行 dynamic adaptive epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "        - plot_results: 绘制 dynamic adaptive epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy 以及 epsilon 随时间的变化。\n",
    "        - plot_epsilon_vs_cumulative_regret: 绘制 epsilon vs cumulative regret 图。\n",
    "        - dynamic_plot: 动态可视化 dynamic adaptive epsilon-greedy 算法的结果\n",
    "        - run_and_plot: 运行 dynamic adaptive epsilon-greedy 算法并绘制结果，init_epsilon, percentiles 可以通过滑块调整。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab = mab\n",
    "            self.config_daeg = self.mab.config.dynamic_adaptive_epsilon_greedy\n",
    "\n",
    "            self.min_epsilon = self.config_daeg.min_epsilon  # 最小 epsilon 值\n",
    "            self.max_epsilon = self.config_daeg.max_epsilon  # 最大 epsilon 值\n",
    "            # self.threshold = self.mab.config.dynamic_adaptive_epsilon_greedy.threshold  # 阈值\n",
    "\n",
    "            self.dynamic_init_epsilon_min = self.config_daeg.dynamic_plot.init_epsilon_min  # 动态 epsilon 的最小值\n",
    "            self.dynamic_init_epsilon_max = self.config_daeg.dynamic_plot.init_epsilon_max  # 动态 epsilon 的最大值\n",
    "            self.dynamic_init_epsilon_step = self.config_daeg.dynamic_plot.init_epsilon_step  # 动态 epsilon 的步长\n",
    "            self.dynamic_init_epsilon_default_value = self.config_daeg.dynamic_plot.init_epsilon_default_value  # 动态 epsilon 的默认值\n",
    "\n",
    "            self.dynamic_percentiles_min = self.config_daeg.dynamic_plot.percentiles_min  # 动态 percentiles 的最小值\n",
    "            self.dynamic_percentiles_max = self.config_daeg.dynamic_plot.percentiles_max  # 动态 percentiles 的最大值\n",
    "            self.dynamic_percentiles_step = self.config_daeg.dynamic_plot.percentiles_step  # 动态 percentiles 的步长\n",
    "            self.dynamic_percentiles_default_value = self.config_daeg.dynamic_plot.percentiles_default_value  # 动态 percentiles 的默认值\n",
    "\n",
    "            self.e_vs_cr_epsilon_min = self.config_daeg.epsilon_vs_cumulative_regret.epsilon_min  # epsilon vs cumulative regret 图的 epsilon 最小值\n",
    "            self.e_vs_cr_epsilon_max = self.config_daeg.epsilon_vs_cumulative_regret.epsilon_max  # epsilon vs cumulative regret 图的 epsilon 最大值\n",
    "            self.e_vs_cr_epsilon_step = self.config_daeg.epsilon_vs_cumulative_regret.epsilon_step  # epsilon vs cumulative regret 图的 epsilon 步长\n",
    "            self.e_vs_cr_percentiles = self.config_daeg.epsilon_vs_cumulative_regret.percentiles  # epsilon vs cumulative regret 图的百分位数\n",
    "\n",
    "        def algorithm(self, init_epsilon: float, percentiles: float):\n",
    "            epsilon = init_epsilon\n",
    "            def choose_node(epsilon, estimated_means):\n",
    "                if np.random.rand() < epsilon:\n",
    "                    return np.random.randint(self.mab.N)\n",
    "                else:\n",
    "                    return np.argmax(estimated_means)\n",
    "\n",
    "            estimated_means = np.zeros(self.mab.N)\n",
    "            time_counts = np.zeros(self.mab.T_test)\n",
    "            nodes_counts = np.zeros(self.mab.N)\n",
    "            single_step_regret = np.zeros(self.mab.T_test)\n",
    "            # cumulative_regret = np.zeros(self.mab.T_test)\n",
    "            epsilon_time = np.zeros(self.mab.T_test)\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                if t > 0: # 如果 t > 0，则根据单步遗憾调整epsilon，计算百分位数\n",
    "                    threshold = np.percentile(single_step_regret[:t], percentiles)\n",
    "                    if single_step_regret[t - 1] > threshold:\n",
    "                        # 如果单步遗憾值超过了阈值，计算遗憾值与阈值之间的差距，并进行归一化处理。归一化的计算方法是将差值除以阈值（加上一个非常小的值 1e−6，以防止分母为零）。\n",
    "                        # 归一化的结果 adjustment 会是一个相对值，用来调整 epsilon。这个值越大，说明当前的遗憾值相对于阈值越大，调整的幅度也就越大。\n",
    "                        adjustment = (single_step_regret[t - 1] - threshold) / (threshold + 1e-6)  # 归一化的相对位置\n",
    "                        # 当前遗憾值超过阈值的情况下，通过增加 epsilon 的值来增强探索的力度\n",
    "                        epsilon = min(self.max_epsilon, epsilon + adjustment)\n",
    "                    else:\n",
    "                        # 如果当前的单步遗憾没有超过阈值，说明当前的动作表现相对较好，因此倾向于减少探索，更多地利用当前的策略。\n",
    "                        adjustment = (threshold - single_step_regret[t - 1]) / (threshold + 1e-6)\n",
    "                        epsilon = max(self.min_epsilon, epsilon - adjustment)\n",
    "                else:\n",
    "                    # 如果 t == 0，则不计算百分位数，直接使用初始 epsilon\n",
    "                    epsilon_time[t] = epsilon\n",
    "\n",
    "                epsilon_time[t] = epsilon\n",
    "                chosen_node = choose_node(epsilon, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, epsilon: float, epsilon_time: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy，以及 epsilon 随时间的变化。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - epsilon: 当前实验使用的 epsilon 值。\n",
    "            - epsilon_time: 一个数组，表示每个时间步的 epsilon 值（适用于 adaptive_epsilon_greedy）。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "            fig.suptitle(f\"Dynamically Adaptive Epsilon-Greedy Results\\nInitial Epsilon: {epsilon}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 epsilon_time 随时间的变化\n",
    "            axs[1, 0].plot(epsilon_time, marker='o', linestyle='-', color='purple')\n",
    "            axs[1, 0].set_title('Epsilon over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Epsilon')\n",
    "\n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_epsilon_vs_cumulative_regret(self):\n",
    "            \"\"\"\n",
    "            绘制 epsilon vs cumulative regret 图。\n",
    "            \"\"\"\n",
    "            # epsilon_values = np.arange(0.0001, 0.2001, 0.0005)\n",
    "            epsilon_values = np.arange(self.e_vs_cr_epsilon_min, self.e_vs_cr_epsilon_max, self.e_vs_cr_epsilon_step)\n",
    "\n",
    "            cumulative_regrets = []\n",
    "\n",
    "            for epsilon in epsilon_values:\n",
    "                _, _, _, cumulative_regret, _, _ = self.algorithm(epsilon, self.e_vs_cr_percentiles)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "\n",
    "            min_idx = np.argmin(cumulative_regrets)\n",
    "            max_idx = np.argmax(cumulative_regrets)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(epsilon_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            plt.title('Epsilon vs Cumulative Regret')\n",
    "            plt.xlabel('Epsilon')\n",
    "            plt.ylabel('Cumulative Regret')\n",
    "\n",
    "            plt.annotate(f'Min (x={epsilon_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                         xy=(epsilon_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                         xytext=(epsilon_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            plt.annotate(f'Max (x={epsilon_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                         xy=(epsilon_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                         xytext=(epsilon_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    class Boltzmann:\n",
    "        \"\"\"\n",
    "        Boltzmann 算法类。\n",
    "        Boltzmann策略基于每个动作的预期奖励（estimated mean）来计算一个概率分布，然后根据这个概率分布选择动作。这个概率分布由每个动作的预期奖励通过温度参数（temperature）调节后得到的软最大值（Softmax函数）确定。\n",
    "        控制参数为温度。当温度较高时，算法的选择更随机，倾向于探索；当温度较低时，算法更倾向于选择当前已知的最优动作（利用）。\n",
    "        当温度 τ 很低时，Softmax 函数倾向于将选择概率集中在奖励最高的动作上，类似于贪心策略。\n",
    "        随着温度 τ 的升高，选择概率变得更加均匀，Softmax 函数逐渐接近于随机选择。\n",
    "        适用场景：Boltzmann策略在奖励分布相对稳定、噪声较小的场景中表现较好，因为它可以根据已知信息较为精确地调整选择概率。\n",
    "        \n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab = mab\n",
    "            self.config_boltzmann = self.mab.config.boltzmann\n",
    "\n",
    "            self.dynamic_temperature_min = self.config_boltzmann.dynamic_plot.temperature_min  # 动态 temperature 的最小值\n",
    "            self.dynamic_temperature_max = self.config_boltzmann.dynamic_plot.temperature_max  # 动态 temperature 的最大值\n",
    "            self.dynamic_temperature_step = self.config_boltzmann.dynamic_plot.temperature_step  # 动态 temperature 的步长\n",
    "            self.dynamic_temperature_default_value = self.config_boltzmann.dynamic_plot.temperature_default_value  # 动态 temperature 的默认值\n",
    "\n",
    "            self.t_vs_cr_temperature_min = self.config_boltzmann.temperature_vs_cumulative_regret.temperature_min  # temperature vs cumulative regret 图的 temperature 最小值\n",
    "            self.t_vs_cr_temperature_max = self.config_boltzmann.temperature_vs_cumulative_regret.temperature_max  # temperature vs cumulative regret 图的 temperature 最大值\n",
    "            self.t_vs_cr_temperature_step = self.config_boltzmann.temperature_vs_cumulative_regret.temperature_step  # temperature vs cumulative regret 图的 temperature 步长\n",
    "\n",
    "        def algorithm(self, temperature: float):\n",
    "            def choose_node(temperature: float, estimated_means: np.ndarray):\n",
    "                exp_values = np.exp(estimated_means / temperature)\n",
    "                probabilities = exp_values / np.sum(exp_values)\n",
    "                return np.random.choice(self.mab.N, p=probabilities), probabilities\n",
    "\n",
    "            estimated_means = np.zeros(self.mab.N)\n",
    "            time_counts = np.zeros(self.mab.T_test)\n",
    "            nodes_counts = np.zeros(self.mab.N)\n",
    "            single_step_regret = np.zeros(self.mab.T_test)\n",
    "            time_probabilities = np.zeros((self.mab.T_test, self.mab.N))\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                chosen_node, probabilities = choose_node(temperature, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                time_probabilities[t] = probabilities\n",
    "                reward = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, temperature: float, temperature_time_probabilities: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy，以及 epsilon 随时间的变化。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - temperature: 当前实验使用的温度值。\n",
    "            - temperature_time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            \n",
    "            fig.suptitle(f\"Boltzmann Results\\nTemperature: {temperature}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 temperature_time_probabilities 随时间的变化\n",
    "            for i in range(self.mab.N):\n",
    "                axs[1, 0].plot(temperature_time_probabilities[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            axs[1, 0].set_title('Node Selection Probabilities Over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Probability')\n",
    "            axs[1, 0].legend()\n",
    "            \n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_temperature_vs_cumulative_regret(self):\n",
    "            \"\"\"\n",
    "            绘制 temperature vs cumulative regret 图。\n",
    "            \"\"\"\n",
    "            temperature_values = np.arange(self.t_vs_cr_temperature_min, self.t_vs_cr_temperature_max, self.t_vs_cr_temperature_step)\n",
    "\n",
    "            cumulative_regrets = []\n",
    "\n",
    "            for temperature in temperature_values:\n",
    "                _, _, _, cumulative_regret, _, _ = self.algorithm(temperature)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "\n",
    "            min_idx = np.argmin(cumulative_regrets)\n",
    "            max_idx = np.argmax(cumulative_regrets)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(temperature_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            plt.title('Temperature vs Cumulative Regret')\n",
    "            plt.xlabel('Temperature')\n",
    "            plt.ylabel('Cumulative Regret')\n",
    "\n",
    "            plt.annotate(f'Min (x={temperature_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                         xy=(temperature_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                         xytext=(temperature_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            plt.annotate(f'Max (x={temperature_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                         xy=(temperature_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                         xytext=(temperature_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "    class UCB:\n",
    "        \"\"\"\n",
    "        UCB 算法类。\n",
    "        UCB 算法是一种基于置信上界的多臂赌博机算法，通过计算每个动作的置信上界来选择动作。\n",
    "        UCB 算法的核心思想是：在每个时间步 t，选择使得置信上界最大的动作。\n",
    "        置信上界的计算方法是：动作的平均奖励 + 置信区间（置信区间的计算方法是置信上界的上限减去置信上界的下限）。\n",
    "        控制参数为置信区间的参数 c。\n",
    "        适用场景：UCB 算法在奖励分布相对稳定、噪声较小的场景中表现较好，因为它可以根据已知信息较为精确地调整选择概率。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab = mab\n",
    "            self.config_ucb = self.mab.config.ucb\n",
    "\n",
    "            # self.c = self.config_ucb.c\n",
    "            self.dynamic_c_min = self.config_ucb.dynamic_plot.c_min  # 动态 c 的最小值\n",
    "            self.dynamic_c_max = self.config_ucb.dynamic_plot.c_max  # 动态 c 的最大值\n",
    "            self.dynamic_c_step = self.config_ucb.dynamic_plot.c_step  # 动态 c 的步长\n",
    "            self.dynamic_c_default_value = self.config_ucb.dynamic_plot.c_default_value  # 动态 c 的默认值\n",
    "            \n",
    "            self.c_vs_cr_c_min = self.config_ucb.c_vs_cumulative_regret.c_min  # c vs cumulative regret 图的 c 最小值\n",
    "            self.c_vs_cr_c_max = self.config_ucb.c_vs_cumulative_regret.c_max  # c vs cumulative regret 图的 c 最大值\n",
    "            self.c_vs_cr_c_step = self.config_ucb.c_vs_cumulative_regret.c_step  # c vs cumulative regret 图的 c 步长\n",
    "            \n",
    "        def algorithm(self, c: float):\n",
    "            estimated_means = np.zeros(self.mab.N)\n",
    "            time_counts = np.zeros(self.mab.T_test)\n",
    "            nodes_counts = np.zeros(self.mab.N)\n",
    "            single_step_regret = np.zeros(self.mab.T_test)\n",
    "            time_ucb_values = np.zeros((self.mab.T_test, self.mab.N))\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                ucb_values = estimated_means + c * np.sqrt(np.log(t + 1) / (nodes_counts + 1e-6))\n",
    "                time_ucb_values[t] = ucb_values\n",
    "                chosen_node = np.argmax(ucb_values)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_ucb_values\n",
    "        \n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, c: float, time_ucb_values: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 UCB 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - c: 当前实验使用的置信区间参数。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "            fig.suptitle(f\"UCB Results\\nConfidence Interval Parameter: {c}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 time_ucb_values 随时间的变化\n",
    "            for i in range(self.mab.N):\n",
    "                axs[1, 0].plot(time_ucb_values[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            axs[1, 0].set_title('UCB Values Over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('UCB Value')\n",
    "            axs[1, 0].legend()\n",
    "            \n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_confidence_interval_parameter_vs_cumulative_regret(self):\n",
    "            \"\"\"\n",
    "            绘制 confidence interval parameter vs cumulative regret 图。\n",
    "            \"\"\"\n",
    "            c_values = np.arange(self.c_vs_cr_c_min, self.c_vs_cr_c_max, self.c_vs_cr_c_step)\n",
    "\n",
    "            cumulative_regrets = []\n",
    "\n",
    "            for c in c_values:\n",
    "                _, _, _, cumulative_regret, _, _ = self.algorithm(c)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "\n",
    "            min_idx = np.argmin(cumulative_regrets)\n",
    "            max_idx = np.argmax(cumulative_regrets)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(c_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            plt.title('Confidence Interval Parameter vs Cumulative Regret')\n",
    "            plt.xlabel('Confidence Interval Parameter')\n",
    "            plt.ylabel('Cumulative Regret')\n",
    "\n",
    "            plt.annotate(f'Min (x={c_values[min_idx]:.1f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                         xy=(c_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                         xytext=(c_values[min_idx] + 0.1, cumulative_regrets[min_idx] + 0.1),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            plt.annotate(f'Max (x={c_values[max_idx]:.1f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                         xy=(c_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                         xytext=(c_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.1),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "    def epsilon_greedy_plot_interactive(self, epsilon: float):\n",
    "        \"\"\"\n",
    "        交互式绘制 epsilon-greedy 算法的结果。\n",
    "        :param epsilon: 固定的全局 epsilon 值\n",
    "        \"\"\"\n",
    "        time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy = self.algorithm.algorithm(epsilon)\n",
    "        self.algorithm.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon)\n",
    "    \n",
    "    def adaptive_epsilon_greedy_plot_interactive(self, init_epsilon: float, min_epsilon: float):\n",
    "        \"\"\"\n",
    "        交互式绘制 adaptive epsilon-greedy 算法的结果。\n",
    "        :param init_epsilon: 初始 epsilon 值\n",
    "        :param min_epsilon: 最小 epsilon 值\n",
    "        \"\"\"\n",
    "        time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time = self.algorithm.algorithm(init_epsilon, min_epsilon)\n",
    "        self.algorithm.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, init_epsilon, epsilon_time)\n",
    "        \n",
    "    def dynamic_adaptive_epsilon_greedy_plot_interactive(self, init_epsilon: float, percentiles: float):\n",
    "        \"\"\"\n",
    "        交互式绘制 dynamic adaptive epsilon-greedy 算法的结果。\n",
    "        :param init_epsilon: 初始 epsilon 值\n",
    "        :param percentiles: 百分位数\n",
    "        \"\"\"\n",
    "        time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time = self.algorithm.algorithm(init_epsilon, percentiles)\n",
    "        self.algorithm.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, init_epsilon, epsilon_time)\n",
    "        \n",
    "    def boltzmann_plot_interactive(self, temperature: float):\n",
    "        \"\"\"\n",
    "        交互式绘制 Boltzmann 算法的结果。\n",
    "        :param temperature: 固定的全局 temperature 值\n",
    "        \"\"\"\n",
    "        time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities = self.algorithm.algorithm(temperature)\n",
    "        self.algorithm.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, temperature, time_probabilities)\n",
    "        \n",
    "    def ucb_plot_interactive(self, c: float):\n",
    "        \"\"\"\n",
    "        交互式绘制 UCB 算法的结果。\n",
    "        :param c: 固定的全局 c 值\n",
    "        \"\"\"\n",
    "        time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_ucb_values = self.algorithm.algorithm(c)\n",
    "        self.algorithm.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, c, time_ucb_values)\n",
    "\n",
    "    def dynamic_plot(self):\n",
    "        \"\"\"\n",
    "        动态绘制算法结果。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 调整 ToggleButtons 部分的代码\n",
    "        load_reward_method_widget = widgets.ToggleButtons(\n",
    "            options=['reward_0', 'reward_1'],\n",
    "            value='reward_0',\n",
    "            description='Reward Method:',\n",
    "            button_style='info',  # 可以设置为 'success', 'info', 'warning', 'danger' 来改变按钮颜色\n",
    "            layout=widgets.Layout(width='150px', height='auto', margin='10px')  # 调整宽度、高度并增加间距\n",
    "        )\n",
    "        \n",
    "        data_type_widget = widgets.ToggleButtons(\n",
    "            options=['iid', 'ar1'],\n",
    "            value='iid',\n",
    "            description='Data Type:',\n",
    "            button_style='warning',  # 同样可以设置按钮的样式\n",
    "            layout=widgets.Layout(width='150px', height='auto', margin='10px')  # 调整宽度、高度并增加间距\n",
    "        )\n",
    "        \n",
    "        algorithm_type_widget = widgets.ToggleButtons(\n",
    "            options=['epsilon_greedy', 'adaptive_epsilon_greedy', 'dynamic_adaptive_epsilon_greedy', 'boltzmann', 'ucb'],\n",
    "            value='epsilon_greedy',\n",
    "            description='Algorithm:',\n",
    "            button_style='success',  # 设置按钮样式\n",
    "            style={'button_width': '300px'},  # 使用 style 调整按钮宽度\n",
    "            layout=widgets.Layout(width='300px', height='auto', margin='10px')  # 设置按钮的宽度\n",
    "        )\n",
    "        output = Output()\n",
    "\n",
    "        def on_button_clicked(b):\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                self.set_parameters(load_reward_method_widget.value, data_type_widget.value)\n",
    "                self.plot_combine_rewards()\n",
    "\n",
    "                # 根据不同的算法类型显示对应的滑块\n",
    "                if algorithm_type_widget.value == 'epsilon_greedy':\n",
    "                    self.algorithm = self.EpsilonGreedy(self)\n",
    "                    epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_epsilon_default_value,\n",
    "                        description='Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "\n",
    "                    )\n",
    "                    interact(self.epsilon_greedy_plot_interactive, epsilon=epsilon_widget)\n",
    "                    self.algorithm.plot_epsilon_vs_cumulative_regret()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'adaptive_epsilon_greedy':\n",
    "                    self.algorithm = self.AdaptiveEpsilonGreedy(self)\n",
    "                    init_epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_init_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_init_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_init_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_init_epsilon_default_value,\n",
    "                        description='Initial Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        \n",
    "                    )\n",
    "                    min_epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_min_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_min_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_min_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_min_epsilon_default_value,\n",
    "                        description='Min Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                    )\n",
    "                    interact(self.adaptive_epsilon_greedy_plot_interactive, init_epsilon=init_epsilon_widget, min_epsilon=min_epsilon_widget)\n",
    "                    self.algorithm.plot_epsilon_vs_cumulative_regret()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'dynamic_adaptive_epsilon_greedy':\n",
    "                    self.algorithm = self.DynamicAdaptiveEpsilonGreedy(self)\n",
    "                    init_epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_init_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_init_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_init_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_init_epsilon_default_value,\n",
    "                        description='Initial Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                    )\n",
    "                    percentiles_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_percentiles_min,\n",
    "                        max=self.algorithm.dynamic_percentiles_max,\n",
    "                        step=self.algorithm.dynamic_percentiles_step,\n",
    "                        value=self.algorithm.dynamic_percentiles_default_value,\n",
    "                        description='Percentiles:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                    )\n",
    "                    interact(self.dynamic_adaptive_epsilon_greedy_plot_interactive, init_epsilon=init_epsilon_widget, percentiles=percentiles_widget)\n",
    "                    self.algorithm.plot_epsilon_vs_cumulative_regret()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'boltzmann':\n",
    "                    self.algorithm = self.Boltzmann(self)\n",
    "                    temperature_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_temperature_min,\n",
    "                        max=self.algorithm.dynamic_temperature_max,\n",
    "                        step=self.algorithm.dynamic_temperature_step,\n",
    "                        value=self.algorithm.dynamic_temperature_default_value,\n",
    "                        description='Temperature:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                    )\n",
    "                    interact(self.boltzmann_plot_interactive, temperature=temperature_widget)\n",
    "                    self.algorithm.plot_temperature_vs_cumulative_regret()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'ucb':\n",
    "                    self.algorithm = self.UCB(self)\n",
    "                    c_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_c_min,\n",
    "                        max=self.algorithm.dynamic_c_max,\n",
    "                        step=self.algorithm.dynamic_c_step,\n",
    "                        value=self.algorithm.dynamic_c_default_value,\n",
    "                        description='Confidence Interval Parameter:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                    )\n",
    "                    interact(self.ucb_plot_interactive, c=c_widget)\n",
    "                    self.algorithm.plot_confidence_interval_parameter_vs_cumulative_regret()\n",
    "\n",
    "        run_button = widgets.Button(\n",
    "            description='Set Parameters',\n",
    "            button_style='primary',  # 设置为 'primary' 风格\n",
    "            layout=widgets.Layout(width='150px', margin='10px')  # 增加按钮的间距\n",
    "        )\n",
    "        run_button.on_click(on_button_clicked)\n",
    "        \n",
    "        # 确保输出区域显示\n",
    "        controls = HBox([load_reward_method_widget, data_type_widget, algorithm_type_widget, run_button])\n",
    "        display(controls, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e7934ce687f3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5a1013532e4c6dbaf1f9937a8cd1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(ToggleButtons(button_style='info', description='Reward Method:', layout=Layout(height='auto', m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7a9a981f094f8fb31955d4c7706aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mab = MAB(config, reward_data_manager)\n",
    "mab.dynamic_plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
