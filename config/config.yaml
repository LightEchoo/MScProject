path:
  global_path: "E:/Study in the UK/Project/MScProject"  # 项目的根目录

base:
  N: 10  # 节点数
  T: 11000  # 总时间步数
  T_train_val: 10000  # 训练和验证的时间步数
  train_ratio: 0.8  # 训练集占比
  T_train: 8000 # 训练的时间步数
  T_val: 2000  # 验证的时间步数
  T_test: 1000  # 测试的时间步数
  lambda: 0.5  # load和latency的权重, 0.5表示load的权重

data_generation:
  load_data:
    node_load_mean_mean: 50.0  # 节点的load的初始值/均值的均值
    node_load_mean_var: 10.0  # 节点的load的初始值/均值的方差
    node_load_iid_var: 5.0  # 节点的load值的iid方法的方差
    node_load_ar1_theta: 0.9  # 节点的load值的ar1方法的自相关系数

  latency_data:
    node_latency_mean_mean: 30.0  # 节点的latency的初始值/均值的均值
    node_latency_mean_var: 10.0  # 节点的latency的初始值/均值的方差
    node_latency_ar1_theta: 0.9  # 节点的latency值的ar1方法的自相关系数

  reward_parameters:
    iid:
      alpha_load_0: 40.0
      alpha_latency_1: 0.041

    ar1:
      alpha_load_0: 40.0
      alpha_latency_1: 0.041

  reward_parameters_slider:
    alpha_load_0:
        value: 1.0
        min: 0.001
        max: 40.0
        step: 0.01
        description: 'alpha_load_0'
    alpha_latency_0:
        value: 1.0
        min: 0.001
        max: 6.0
        step: 0.01
        description: 'alpha_latency_0'
    alpha_latency_1:
        value: 0.5
        min: 0.0001
        max: 0.5
        step: 0.001
        description: 'alpha_latency_1'

exp4:
  batch_size: 64  # Batch size
  seq_length: 20  # Sequence length
  input_size: 10  # Input size
  output_size: 10  # Output size
  learning_rate: 0.001  # Learning rate
  num_workers: 16  # Number of workers for DataLoader
  num_epochs: 100  # Number of epochs
  device: 'cuda'  # Device
  mix_precision: True  # Mixed precision training

  # 早停的参数
  patience_epochs: 6  # 'patience_epochs' 个 epoch 没有提升，就停止训练
  min_delta: 1e-3  # 当监控指标的变化小于 min_delta 时，就视为没有提升

  # 调度器的参数
  mode: 'min'  # 'min' 表示监控指标的值越小越好，'max' 表示监控指标的值越大越好
  factor: 0.1  # 学习率调度器的缩放因子
  patience_lr: 2  # 'patience_lr' 个 epoch 没有提升，就缩放学习率
  min_lr: 1e-6  # 学习率的下限
  threshold: 1e-2  # 监控指标的变化小于 threshold 时，就视为没有提升

  ARconfig:
    order: 5  # AR 模型的阶数

  LSTMconfig:
    hidden_size: 128  # LSTM 模型的隐藏层大小
    num_layers: 4  # LSTM 模型的层数
    dropout_prob: 0.2  # LSTM 模型的 dropout 率
    weight_decay: 1e-4  # LSTM 模型的权重衰减
  #    batch_first: True  # LSTM 模型的输入是否 batch_first

  GNNconfig:
    hidden_size: 128  # GNN 模型的隐藏层大小
    num_layers: 4  # GNN 模型的层数
#    dropout: 0.2  # GNN 模型的 dropout 率
#    batch_first: True  # GNN 模型的输入是否 batch_first
#    num_heads: 8  # GNN 模型的头数
#    concat: True  # GNN 模型的是否拼接多头的输出