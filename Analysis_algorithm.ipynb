{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bbbfe7a5130d5fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:32:16.030704Z",
     "start_time": "2024-08-28T08:32:14.865298Z"
    }
   },
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "from DataInit import DataManager, RewardDataManager\n",
    "import DataInit\n",
    "from Exp4DataManager import Exp4DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a519a9c45ce92d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T01:53:32.789251Z",
     "start_time": "2024-08-26T01:53:32.765474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Config Info ----------\n",
      "path:\n",
      "  global_path: /home/alex4060/PythonProject/MScProject/MScProject\n",
      "base:\n",
      "  'N': 10\n",
      "  T: 11000\n",
      "  T_train_val: 10000\n",
      "  train_ratio: 0.8\n",
      "  T_train: 8000\n",
      "  T_val: 2000\n",
      "  T_test: 1000\n",
      "  lambda_load: 0.5\n",
      "  top_k:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "data_generation:\n",
      "  load_data:\n",
      "    node_load_mean_mean: 50.0\n",
      "    node_load_mean_std: 10.0\n",
      "    node_load_iid_std: 5.0\n",
      "    node_load_ar1_theta: 0.9\n",
      "    node_load_ar1_std: 80.0\n",
      "  latency_data:\n",
      "    node_latency_mean_mean: 30.0\n",
      "    node_latency_mean_std: 10.0\n",
      "    node_latency_ar1_theta: 0.9\n",
      "  reward_parameters:\n",
      "    iid:\n",
      "      alpha_load_0: 30.0\n",
      "      alpha_latency_1: 0.035\n",
      "    ar1:\n",
      "      alpha_load_0: 30.0\n",
      "      alpha_latency_1: 0.086\n",
      "  reward_parameters_slider:\n",
      "    alpha_load_0:\n",
      "      value: 1.0\n",
      "      min: 0.001\n",
      "      max: 40.0\n",
      "      step: 0.01\n",
      "      description: alpha_load_0\n",
      "    alpha_latency_0:\n",
      "      value: 1.0\n",
      "      min: 0.001\n",
      "      max: 6.0\n",
      "      step: 0.01\n",
      "      description: alpha_latency_0\n",
      "    alpha_latency_1:\n",
      "      value: 0.5\n",
      "      min: 0.0001\n",
      "      max: 0.5\n",
      "      step: 0.001\n",
      "      description: alpha_latency_1\n",
      "epsilon_greedy:\n",
      "  dynamic_plot:\n",
      "    epsilon_min: 0.001\n",
      "    epsilon_max: 0.5\n",
      "    epsilon_step: 0.01\n",
      "    epsilon_default_value: 0.1\n",
      "  evaluation:\n",
      "    epsilon_min: 0.0001\n",
      "    epsilon_max: 0.2001\n",
      "    epsilon_step: 0.0005\n",
      "adaptive_epsilon_greedy:\n",
      "  dynamic_plot:\n",
      "    init_epsilon_min: 0.001\n",
      "    init_epsilon_max: 0.5\n",
      "    init_epsilon_step: 0.01\n",
      "    init_epsilon_default_value: 0.1\n",
      "    min_epsilon_min: 0.001\n",
      "    min_epsilon_max: 0.1\n",
      "    min_epsilon_step: 0.001\n",
      "    min_epsilon_default_value: 0.05\n",
      "  evaluation:\n",
      "    init_epsilon_min: 0.0001\n",
      "    init_epsilon_max: 0.1001\n",
      "    init_epsilon_step: 0.0005\n",
      "    min_epsilon: 0.05\n",
      "dynamic_adaptive_epsilon_greedy:\n",
      "  min_epsilon: 0.05\n",
      "  max_epsilon: 0.8\n",
      "  dynamic_plot:\n",
      "    init_epsilon_min: 0.001\n",
      "    init_epsilon_max: 0.5\n",
      "    init_epsilon_step: 0.01\n",
      "    init_epsilon_default_value: 0.1\n",
      "    percentiles_min: 50\n",
      "    percentiles_max: 100\n",
      "    percentiles_step: 1\n",
      "    percentiles_default_value: 80\n",
      "  evaluation:\n",
      "    init_epsilon_min: 0.0001\n",
      "    init_epsilon_max: 0.1001\n",
      "    init_epsilon_step: 0.0005\n",
      "    default_percentiles: 80\n",
      "    percentiles_min: 50\n",
      "    percentiles_max: 100\n",
      "    percentiles_step: 1\n",
      "    default_init_epsilon: 0.04\n",
      "boltzmann:\n",
      "  dynamic_plot:\n",
      "    temperature_min: 0.01\n",
      "    temperature_max: 1.01\n",
      "    temperature_step: 0.001\n",
      "    temperature_default_value: 0.5\n",
      "  evaluation:\n",
      "    temperature_min: 0.01\n",
      "    temperature_max: 0.25\n",
      "    temperature_step: 0.001\n",
      "thompson_sampling: None\n",
      "ucb:\n",
      "  dynamic_plot:\n",
      "    c_min: 0.001\n",
      "    c_max: 1\n",
      "    c_step: 0.001\n",
      "    c_default_value: 0.5\n",
      "  evaluation:\n",
      "    c_min: 0.001\n",
      "    c_max: 1.0\n",
      "    c_step: 0.001\n",
      "exp3:\n",
      "  dynamic_plot:\n",
      "    gamma_min: 0.001\n",
      "    gamma_max: 1.0\n",
      "    gamma_step: 0.001\n",
      "    gamma_default_value: 0.01\n",
      "  evaluation:\n",
      "    gamma_min: 0.001\n",
      "    gamma_max: 1.0\n",
      "    gamma_step: 0.005\n",
      "exp_ix:\n",
      "  dynamic_plot:\n",
      "    eta_min: 0.01\n",
      "    eta_max: 1.01\n",
      "    eta_step: 0.01\n",
      "    eta_default_value: 0.5\n",
      "  evaluation:\n",
      "    eta_min: 0.01\n",
      "    eta_max: 1.01\n",
      "    eta_step: 0.05\n",
      "exp4:\n",
      "  batch_size: 64\n",
      "  seq_length: 20\n",
      "  input_size: 10\n",
      "  output_size: 10\n",
      "  learning_rate: 0.001\n",
      "  num_workers: 16\n",
      "  num_epochs: 100\n",
      "  device: cuda\n",
      "  mix_precision: true\n",
      "  patience_epochs: 6\n",
      "  min_delta: 0.001\n",
      "  mode: min\n",
      "  factor: 0.1\n",
      "  patience_lr: 2\n",
      "  min_lr: 1.0e-06\n",
      "  threshold: 0.01\n",
      "  ARconfig:\n",
      "    order: 5\n",
      "  LSTMconfig:\n",
      "    hidden_size: 128\n",
      "    num_layers: 4\n",
      "    dropout_prob: 0.2\n",
      "  GNNconfig:\n",
      "    hidden_size: 128\n",
      "    num_layers: 4\n",
      "\n",
      "----------- config End -----------\n",
      "\n",
      "---------- Path Info ----------\n",
      "Global Path: /home/alex4060/PythonProject/MScProject/MScProject\n",
      "Data Path: /home/alex4060/PythonProject/MScProject/MScProject/Data\n",
      "Load Latency Original CSV Path: /home/alex4060/PythonProject/MScProject/MScProject/Data/load_latency_original_csv\n",
      "Rewards NPY Path: /home/alex4060/PythonProject/MScProject/MScProject/Data/rewards_npy\n",
      "Models PKL Path: /home/alex4060/PythonProject/MScProject/MScProject/Data/models_pkl\n",
      "----------- Path End -----------\n"
     ]
    }
   ],
   "source": [
    "# 配置管理\n",
    "config = DataInit.config_manager()\n",
    "\n",
    "# 路径管理\n",
    "global_path, data_path, load_latency_original_csv_path, rewards_npy_path, models_pkl_path = DataInit.path_manager(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27d3a9c1ff3ed714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T01:53:33.883863Z",
     "start_time": "2024-08-26T01:53:33.877358Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "reward_data_manager = DataInit.import_data_manager(models_pkl_path, 'reward', if_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import re\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import norm\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cee6c999b964cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"简单滑动平均平滑\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "def exponential_moving_average(data, alpha=0.01):\n",
    "    \"\"\"指数加权平均平滑\"\"\"\n",
    "    smoothed_data = np.zeros_like(data)\n",
    "    smoothed_data[0] = data[0]\n",
    "    for t in range(1, len(data)):\n",
    "        smoothed_data[t] = alpha * data[t] + (1 - alpha) * smoothed_data[t-1]\n",
    "    return smoothed_data\n",
    "\n",
    "def gaussian_smooth(data, sigma=15):\n",
    "    \"\"\"高斯平滑\"\"\"\n",
    "    return gaussian_filter1d(data, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fbd929fb7538085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, widgets, HBox, VBox, Output, interactive_output, interactive, GridBox, Layout\n",
    "from IPython.display import display\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c774e799b600aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB:\n",
    "    def __init__(self, config, reward_data_manager, exp4_data_manager):\n",
    "        self.config: DictConfig = config\n",
    "        self.reward_data_manager: RewardDataManager = reward_data_manager\n",
    "\n",
    "        # 初始化通用参数\n",
    "        self.lambda_load: float = self.config.base.lambda_load\n",
    "        self.top_k: list[int] = self.config.base.top_k\n",
    "        self.N: int = self.config.base.N\n",
    "        self.T_test: int = self.config.base.T_test\n",
    "\n",
    "        # 加载数据\n",
    "        self.iid_load: np.ndarray = reward_data_manager.iid_load\n",
    "        self.iid_load_reward_0: np.ndarray = reward_data_manager.iid_load_reward_0\n",
    "        self.iid_load_reward_1: np.ndarray = reward_data_manager.iid_load_reward_1\n",
    "        self.iid_latency: np.ndarray = reward_data_manager.iid_latency\n",
    "        self.iid_latency_reward_1: np.ndarray = reward_data_manager.iid_latency_reward_1\n",
    "        self.ar1_load: np.ndarray = reward_data_manager.ar1_load\n",
    "        self.ar1_load_reward_0: np.ndarray = reward_data_manager.ar1_load_reward_0\n",
    "        self.ar1_load_reward_1: np.ndarray = reward_data_manager.ar1_load_reward_1\n",
    "        self.ar1_latency: np.ndarray = reward_data_manager.ar1_latency\n",
    "        self.ar1_latency_reward_1: np.ndarray = reward_data_manager.ar1_latency_reward_1\n",
    "        \n",
    "        # Exp4 算法 专家系统数据\n",
    "        self.exp4_data_manager = exp4_data_manager\n",
    "\n",
    "        # 定义一个映射字典\n",
    "        self.reward_mapping: dict[tuple[str, str], tuple[np.ndarray, np.ndarray]] = {\n",
    "            ('iid', 'reward_0'): (self.iid_load_reward_0, self.iid_latency_reward_1),\n",
    "            ('iid', 'reward_1'): (self.iid_load_reward_1, self.iid_latency_reward_1),\n",
    "            ('ar1', 'reward_0'): (self.ar1_load_reward_0, self.ar1_latency_reward_1),\n",
    "            ('ar1', 'reward_1'): (self.ar1_load_reward_1, self.ar1_latency_reward_1),\n",
    "        }\n",
    "        \n",
    "        self.load_reward_method: str | None = None\n",
    "        self.data_type: str | None = None\n",
    "        self.load_reward: np.ndarray | None = None\n",
    "        self.latency_reward: np.ndarray | None = None\n",
    "        self.combine_reward: np.ndarray | None = None\n",
    "        self.combine_reward_mean: np.ndarray | None = None\n",
    "        self.combine_reward_optimal_node: int | None = None\n",
    "        self.combine_reward_optimal_mean: float | None = None\n",
    "        self.combine_reward_sorted_mean: np.ndarray | None = None\n",
    "        \n",
    "        self.algorithm: MAB.EpsilonGreedy | MAB.AdaptiveEpsilonGreedy | MAB.DynamicAdaptiveEpsilonGreedy | MAB.Boltzmann | MAB.UCB | MAB.ThompsonSampling | MAB.Exp3 | MAB.ExpIx | MAB.EXP4 | None = None\n",
    "\n",
    "        # 生成器对象，用于断点执行\n",
    "        # self.exp4_gen: Generator[None, None, None] | None = None\n",
    "        # self.exp4_executed = False  # 用于控制 exp4_generator 只执行一次\n",
    "        \n",
    "        # self.select_exp4_counts: int = 0  # 用于记录调取EXP4算法的次数，用于判断重新运算combine rewards\n",
    "\n",
    "    def set_parameters(self, load_reward_method: str, data_type: str) -> None:\n",
    "        \"\"\"\n",
    "        设置参数并计算组合奖励。\n",
    "        :param load_reward_method: 加载奖励的方法，'reward_0' 或 'reward_1'\n",
    "        :param data_type: 数据类型，'iid' 或 'ar1'\n",
    "        \"\"\"\n",
    "        # 重新设置参数\n",
    "        self.load_reward_method = load_reward_method\n",
    "        self.data_type = data_type\n",
    "        try:\n",
    "            self.load_reward, self.latency_reward = self.reward_mapping[(data_type, load_reward_method)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f'Invalid load_reward_method: {load_reward_method}, data_type: {data_type}')\n",
    "\n",
    "        # 计算组合奖励\n",
    "        self.combine_reward = self.lambda_load * self.load_reward + (1 - self.lambda_load) * self.latency_reward\n",
    "        self.combine_reward_mean = np.mean(self.combine_reward, axis=1)\n",
    "        self.combine_reward_optimal_node = np.argmax(self.combine_reward_mean)\n",
    "        self.combine_reward_optimal_mean = np.max(self.combine_reward_mean)\n",
    "        self.combine_reward_sorted_mean = np.argsort(self.combine_reward_mean)[::-1]\n",
    "\n",
    "        \n",
    "        self.exp4_data_manager.ar_data.set_parameters(load_reward_method, data_type)\n",
    "        self.exp4_data_manager.lstm_data.set_parameters(load_reward_method, data_type)\n",
    "        self.exp4_data_manager.gnn_data.set_parameters(load_reward_method, data_type)\n",
    "\n",
    "        # def exp4_generator() -> Generator[None, None, None]:\n",
    "        #     # 在Exp4专家模型数据管理器中，也选择相应方法计算好相应的reward_combine值，以待备用\n",
    "        # \n",
    "        #     # 生成器的第一个部分\n",
    "        #     yield  # 暂停执行，等待选择到EXP4算法时，调用next()继续\n",
    "        # \n",
    "        #     # 生成器的第二部分\n",
    "        #     # exp4相关的代码在这里    \n",
    "        #     self.exp4_data_manager.ar_data.set_parameters(data_type, load_reward_method)\n",
    "        #     self.exp4_data_manager.lstm_data.set_parameters(data_type, load_reward_method)\n",
    "        #     self.exp4_data_manager.gnn_data.set_parameters(data_type, load_reward_method)\n",
    "        #     # 生成器的结束\n",
    "        # \n",
    "        # self.exp4_gen: Generator[None, None, None] = self.exp4_generator()\n",
    "        # # 当 exp4_generator() 被调用时，生成器并不会立即执行，而是返回一个生成器对象。此时，生成器还未开始执行。\n",
    "        # # 第一个 None 表示生成器 yield 语句产生的值的类型。在你的例子中，没有 yield 产生的值，所以是 None。\n",
    "        # # 第二个 None 表示生成器接收 send() 方法发送的值的类型。你没有使用 send()，所以是 None。\n",
    "        # # 第三个 None 表示生成器函数最终返回的值的类型。通常，生成器不会返回值，或者返回 None。\n",
    "        \n",
    "\n",
    "    def calculate_top_k_accuracy(self, time_counts: np.ndarray, k_list: list[int]) -> dict[int, float]:\n",
    "        \"\"\"\n",
    "        计算 MAB 过程的 Top-k Accuracy（定义二）。\n",
    "        \n",
    "        参数:\n",
    "        - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "        - k_list: 一个整数列表，表示需要计算的 k 值。\n",
    "        \n",
    "        返回:\n",
    "        - 一个字典，key 是 k 的值，value 是对应的 Top-k Accuracy。\n",
    "        \n",
    "        说明:\n",
    "        - Top-k Accuracy 的定义二：在 T_test 个时间步中，选择的节点中有多少是最优节点的前 k 个节点。\n",
    "        \"\"\"\n",
    "        \n",
    "        top_k_accuracy: dict[int, float] = {}\n",
    "\n",
    "        for k in k_list:\n",
    "            correct_count: int = 0\n",
    "            optimal_nodes: np.ndarray = self.combine_reward_sorted_mean[:k]\n",
    "            \n",
    "            for t in range(self.T_test):\n",
    "                if time_counts[t] in optimal_nodes:\n",
    "                    correct_count += 1\n",
    "            accuracy: float = correct_count / self.T_test\n",
    "            top_k_accuracy[k] = accuracy\n",
    "\n",
    "        return top_k_accuracy\n",
    "\n",
    "    def plot_combine_rewards(self, selected_nodes: list[int] | None = None) -> None:\n",
    "        \"\"\"\n",
    "        绘制组合奖励的图形。\n",
    "        \n",
    "        参数:\n",
    "        - selected_nodes: 一个列表，表示选择的节点的索引。默认为 None，表示选择前 3 个节点。\n",
    "        \n",
    "        说明:\n",
    "        - 组合奖励是加载奖励和延迟奖励的加权和。\n",
    "        \"\"\"\n",
    "        \n",
    "        if selected_nodes is None:\n",
    "            selected_nodes: list[int] = [0, 1, 2]  # 默认选择前3个节点\n",
    "    \n",
    "        # 数据准备\n",
    "        last_T_data: np.ndarray = self.combine_reward[selected_nodes, -self.T_test:]\n",
    "    \n",
    "        # 使用 constrained_layout 进行更智能的布局管理\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 3), constrained_layout=True)\n",
    "    \n",
    "        # 1. combine_reward中选择节点的最后T_test次数据的展示\n",
    "        for i, node in enumerate(selected_nodes):\n",
    "            axs[0].plot(last_T_data[i], label=f'Node {node}')\n",
    "        axs[0].set_title('Last T_test Data - Selected Nodes')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "    \n",
    "        # 2. N个节点的均值的分布图，并标注最值\n",
    "        means: np.ndarray = self.combine_reward_mean\n",
    "        axs[1].plot(means, 'bo-', label='Mean Value per Node')\n",
    "        axs[1].axhline(y=np.min(means), color='red', linestyle='--', label='Range')\n",
    "        axs[1].axhline(y=np.max(means), color='red', linestyle='--')\n",
    "        axs[1].set_title('Mean Values of Nodes')\n",
    "        axs[1].set_xlabel('Node')\n",
    "        axs[1].set_ylabel('Mean Value')\n",
    "        axs[1].legend()\n",
    "        \n",
    "        # 调整标注的位置，避免超出图形范围\n",
    "        axs[1].annotate(f'Min: {np.min(means):.3f}', xy=(np.argmin(means), np.min(means)), \n",
    "                        xytext=(np.argmin(means) + 0.5, np.min(means) - 0.05),\n",
    "                        arrowprops=dict(facecolor='black', arrowstyle='->'), fontsize=10, color='black')\n",
    "        axs[1].annotate(f'Max: {np.max(means):.3f}', xy=(np.argmax(means), np.max(means)), \n",
    "                        xytext=(np.argmax(means) + 0.5, np.max(means) + 0.05),\n",
    "                        arrowprops=dict(facecolor='black', arrowstyle='->'), fontsize=10, color='black')\n",
    "    \n",
    "        # 3. 选择节点的最后T_test次数据的直方图\n",
    "        for i, node in enumerate(selected_nodes):\n",
    "            axs[2].hist(last_T_data[i], alpha=0.5, label=f'Node {node}')\n",
    "        axs[2].set_title('Histogram - Selected Nodes')\n",
    "        axs[2].set_xlabel('Value')\n",
    "        axs[2].set_ylabel('Frequency')\n",
    "        axs[2].legend()\n",
    "    \n",
    "        plt.show()\n",
    "        \n",
    "    class EpsilonGreedy:\n",
    "        \"\"\"\n",
    "        Epsilon-Greedy 算法类。\n",
    "        这是最初级的多臂老虎机算法，根据 epsilon 的值以一定概率随机选择节点，以 1-epsilon 的概率选择当前平均奖励最高的节点。\n",
    "        \n",
    "        func:\n",
    "        - algorithm: 运行 epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "        - plot_results: 绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾以及 Top-k Accuracy，并应用平滑。\n",
    "        - plot_evaluation: 绘制累积遗憾、Top-k Accuracy 和 epsilon 的关系图。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_eg: DictConfig = self.mab.config.epsilon_greedy\n",
    "\n",
    "            self.dynamic_epsilon_min: float = self.config_eg.dynamic_plot.epsilon_min  # 动态 epsilon 的最小值\n",
    "            self.dynamic_epsilon_max: float = self.config_eg.dynamic_plot.epsilon_max  # 动态 epsilon 的最大值\n",
    "            self.dynamic_epsilon_step: float = self.config_eg.dynamic_plot.epsilon_step  # 动态 epsilon 的步长\n",
    "            self.dynamic_epsilon_default_value: float = self.config_eg.dynamic_plot.epsilon_default_value\n",
    "            \n",
    "            self.evaluation_epsilon_min: float = self.config_eg.evaluation.epsilon_min  # 评估 epsilon 的最小值\n",
    "            self.evaluation_epsilon_max: float = self.config_eg.evaluation.epsilon_max  # 评估 epsilon 的最大值\n",
    "            self.evaluation_epsilon_step: float = self.config_eg.evaluation.epsilon_step  # 评估 epsilon 的步长\n",
    "            \n",
    "        def algorithm(self, epsilon: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float]]:\n",
    "            \"\"\"\n",
    "            运行 epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - epsilon: epsilon 的值，表示随机选择节点的概率。\n",
    "            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            \"\"\"\n",
    "            def choose_node(epsilon: float, estimated_means: np.ndarray) -> np.signedinteger | int:\n",
    "                if np.random.rand() < epsilon:\n",
    "                    return np.random.randint(self.mab.N)\n",
    "                else:\n",
    "                    return np.argmax(estimated_means)\n",
    "\n",
    "            estimated_means: np.ndarray = np.zeros(self.mab.N)\n",
    "            time_counts: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            nodes_counts: np.ndarray = np.zeros(self.mab.N)\n",
    "            single_step_regret: np.ndarray = np.zeros(self.mab.T_test)\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                chosen_node: np.signedinteger | int = choose_node(epsilon, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward: float = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict[int, float], epsilon: float) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾以及 Top-k Accuracy，并应用平滑。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy\n",
    "            - epsilon: 当前实验使用的 epsilon 值。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "            \n",
    "            fig.suptitle(f\"Epsilon-Greedy Results\\nEpsilon: {epsilon}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node: int = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：单次遗憾的平滑曲线\n",
    "            axs[0, 1].plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            axs[0, 1].set_title('Single Step Regret (Smoothed)')\n",
    "            axs[0, 1].set_xlabel('Time Step')\n",
    "            axs[0, 1].set_ylabel('Regret')\n",
    "\n",
    "            # 标记最大值和最小值\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            axs[0, 1].annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                               xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                               xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                               arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                               fontsize=10, color='blue')\n",
    "\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            axs[0, 1].annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                               xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                               xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                               arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                               fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：累积遗憾的平滑曲线\n",
    "            axs[1, 0].plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "            axs[1, 0].set_title('Cumulative Regret (Smoothed)')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最大值和最小值\n",
    "            min_idx = np.argmin(smoothed_cumulative_regret)\n",
    "            axs[1, 0].annotate(f'Min (x={min_idx}, y={smoothed_cumulative_regret[min_idx]:.2f})',\n",
    "                               xy=(min_idx, smoothed_cumulative_regret[min_idx]),\n",
    "                               xytext=(min_idx, smoothed_cumulative_regret[min_idx] + 0.05),\n",
    "                               arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                               fontsize=10, color='blue')\n",
    "\n",
    "            max_idx = np.argmax(smoothed_cumulative_regret)\n",
    "            axs[1, 0].annotate(f'Max (x={max_idx}, y={smoothed_cumulative_regret[max_idx]:.2f})',\n",
    "                               xy=(max_idx, smoothed_cumulative_regret[max_idx]),\n",
    "                               xytext=(max_idx, smoothed_cumulative_regret[max_idx] - 0.05),\n",
    "                               arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                               fontsize=10, color='red')\n",
    "\n",
    "            # 子图4：Top-k Accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95)) # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_evaluation(self) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon vs cumulative regret 图和 epsilon vs top-k accuracy 图。\n",
    "            \"\"\"\n",
    "            epsilon_values: np.ndarray = np.arange(self.evaluation_epsilon_min, self.evaluation_epsilon_max, self.evaluation_epsilon_step)\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "        \n",
    "            for epsilon in epsilon_values:\n",
    "                _, _, _, cumulative_regret, top_k_accuracy = self.algorithm(epsilon)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "                time_top_k_accuracy.append(top_k_accuracy)\n",
    "        \n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "            \n",
    "            fig.suptitle('Epsilon vs Cumulative Regret and Top-k Accuracy')\n",
    "            \n",
    "            # 绘制 epsilon vs cumulative regret 图\n",
    "            axs[0].plot(epsilon_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Epsilon vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Epsilon')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "            \n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={epsilon_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(epsilon_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(epsilon_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "            \n",
    "            axs[0].annotate(f'Max (x={epsilon_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(epsilon_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(epsilon_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(epsilon_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                axs[i+1].set_xlabel('Epsilon')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_interactive(self, epsilon: float):\n",
    "            \"\"\"\n",
    "            交互式绘制 epsilon-greedy 算法的结果。\n",
    "            :param epsilon: 固定的全局 epsilon 值\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy = self.algorithm(epsilon)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon)\n",
    "\n",
    "    class AdaptiveEpsilonGreedy:\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon-Greedy 算法类。\n",
    "        在 Epsilon-Greedy 算法的基础上，根据时间步调整 epsilon 的值。\n",
    "        越靠后，epsilon 越小，即 epsilon = 1 - t / T_test。\n",
    "        但是为了保证 epsilon 不会小于最小值，因此设置了一个 min_epsilon 参数，表示算法中可以使用的最小 epsilon 值。\n",
    "        原先的 epsilon greedy 算法中的 epsilon 在此算法中相当于初始值。\n",
    "        \n",
    "        func:\n",
    "        - algorithm: 运行 adaptive epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "        - plot_results: 绘制 adaptive epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy 以及 epsilon 随时间的变化。\n",
    "        - plot_evaluation: 绘制 epsilon vs cumulative regret 图和 epsilon vs top-k accuracy 图。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_aeg: DictConfig = self.mab.config.adaptive_epsilon_greedy\n",
    "        \n",
    "            self.dynamic_init_epsilon_min: float = self.config_aeg.dynamic_plot.init_epsilon_min  # 动态图 epsilon 的最小值\n",
    "            self.dynamic_init_epsilon_max: float = self.config_aeg.dynamic_plot.init_epsilon_max  # 动态图 epsilon 的最大值\n",
    "            self.dynamic_init_epsilon_step: float = self.config_aeg.dynamic_plot.init_epsilon_step  # 动态图 epsilon 的步长\n",
    "            self.dynamic_init_epsilon_default_value: float = self.config_aeg.dynamic_plot.init_epsilon_default_value  # 动态 epsilon 的默认值\n",
    "        \n",
    "            self.dynamic_min_epsilon_min: float = self.config_aeg.dynamic_plot.min_epsilon_min  # 动态图 min_epsilon 的最小值\n",
    "            self.dynamic_min_epsilon_max: float = self.config_aeg.dynamic_plot.min_epsilon_max  # 动态图 min_epsilon 的最大值\n",
    "            self.dynamic_min_epsilon_step: float = self.config_aeg.dynamic_plot.min_epsilon_step  # 动态图 min_epsilon 的步长\n",
    "            self.dynamic_min_epsilon_default_value: float = self.config_aeg.dynamic_plot.min_epsilon_default_value  # 动态图 min_epsilon 的默认值\n",
    "        \n",
    "            self.eval_init_epsilon_min: float = self.config_aeg.evaluation.init_epsilon_min  # 评估图 init_epsilon 的最小值\n",
    "            self.eval_init_epsilon_max: float = self.config_aeg.evaluation.init_epsilon_max  # 评估图 init_epsilon 的最大值\n",
    "            self.eval_init_epsilon_step: float = self.config_aeg.evaluation.init_epsilon_step  # 评估图 init_epsilon 的步长\n",
    "            self.eval_min_epsilon: float = self.config_aeg.evaluation.min_epsilon  # 评估图算法可取到的 epsilon 的最小值\n",
    "\n",
    "\n",
    "        def algorithm(self, init_epsilon: float, min_epsilon: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float], np.ndarray]:\n",
    "            \"\"\"\n",
    "            运行 adaptive epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - init_epsilon: epsilon 的初始值，表示随机选择节点的概率。\n",
    "            - min_epsilon: epsilon 的最小值，表示算法中可以使用的最小 epsilon 值。\n",
    "            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - epsilon_time: 一个数组，表示每个时间步的 epsilon 值。\n",
    "            \"\"\"\n",
    "            epsilon: float = init_epsilon\n",
    "            def choose_node(epsilon: float, estimated_means: np.ndarray) -> int | np.signedinteger:\n",
    "                if np.random.rand() < epsilon:\n",
    "                    return np.random.randint(self.mab.N)\n",
    "                else:\n",
    "                    return np.argmax(estimated_means)\n",
    "\n",
    "            estimated_means: np.ndarray = np.zeros(self.mab.N)\n",
    "            time_counts: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            nodes_counts: np.ndarray = np.zeros(self.mab.N)\n",
    "            single_step_regret: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            epsilon_time: np.ndarray = np.zeros(self.mab.T_test)\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                epsilon = max(min_epsilon, epsilon * (1 - t / self.mab.T_test))\n",
    "                epsilon_time[t] = epsilon\n",
    "                chosen_node: int = choose_node(epsilon, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward: float = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, epsilon: float, epsilon_time: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy，以及 epsilon 随时间的变化。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - epsilon: 当前实验使用的 epsilon 值。\n",
    "            - epsilon_time: 一个数组，表示每个时间步的 epsilon 值（适用于 adaptive_epsilon_greedy）。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "            fig.suptitle(f\"Adaptive Epsilon-Greedy Results\\nInitial Epsilon: {epsilon}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 epsilon_time 随时间的变化\n",
    "            axs[1, 0].plot(epsilon_time, marker='o', linestyle='-', color='purple')\n",
    "            axs[1, 0].set_title('Epsilon over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Epsilon')\n",
    "\n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_evaluation(self) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon vs cumulative regret 图和 epsilon vs top-k accuracy 图。\n",
    "            \"\"\"\n",
    "            epsilon_values: np.ndarray = np.arange(self.eval_init_epsilon_min, self.eval_init_epsilon_max, self.eval_init_epsilon_step)\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "\n",
    "            for epsilon in epsilon_values:\n",
    "                _, _, _, cumulative_regret, top_k_accuracy, _ = self.algorithm(epsilon, self.eval_min_epsilon)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "                time_top_k_accuracy.append(top_k_accuracy)\n",
    "\n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "            fig.suptitle(f'Init Epsilon vs Cumulative Regret with fixed Min Epsilon: {self.eval_min_epsilon} and Top-k Accuracy')\n",
    "\n",
    "            # 绘制 epsilon vs cumulative regret 图\n",
    "            axs[0].plot(epsilon_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Epsilon vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Epsilon')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={epsilon_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(epsilon_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(epsilon_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "\n",
    "            axs[0].annotate(f'Max (x={epsilon_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(epsilon_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(epsilon_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(epsilon_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                axs[i+1].set_xlabel('Epsilon')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_interactive(self, init_epsilon: float, min_epsilon: float):\n",
    "            \"\"\"\n",
    "            交互式绘制 adaptive epsilon-greedy 算法的结果。\n",
    "            :param init_epsilon: 初始 epsilon 值\n",
    "            :param min_epsilon: 最小 epsilon 值\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time = self.algorithm(init_epsilon, min_epsilon)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, init_epsilon, epsilon_time)\n",
    "\n",
    "    class DynamicAdaptiveEpsilonGreedy:\n",
    "        \"\"\"\n",
    "        Dynamic Adaptive Epsilon-Greedy 算法类。\n",
    "        在Adaptive Epsilon-Greedy算法的基础上，根据单步遗憾调整epsilon。\n",
    "        \n",
    "        最初是手动设置阈值，当单步遗憾超过阈值时，增加epsilon，否则减小epsilon。\n",
    "        但是这种方法不够灵活，因此改为根据单步遗憾的百分位数调整epsilon。\n",
    "        \n",
    "        最初始的变化幅度是init_alpha * 上一次的单步遗憾）\n",
    "        当前使用的变化幅度是遗憾值与阈值之间的差距，归一化处理后的相对位置。\n",
    "        \n",
    "        func:\n",
    "        - algorithm: 运行 dynamic adaptive epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "        - plot_results: 绘制 dynamic adaptive epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy 以及 epsilon 随时间的变化。\n",
    "        - plot_evaluation: 绘制 epsilon vs cumulative regret 图。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_daeg: DictConfig = self.mab.config.dynamic_adaptive_epsilon_greedy\n",
    "    \n",
    "            self.min_epsilon: float = self.config_daeg.min_epsilon  # 最小 epsilon 值\n",
    "            self.max_epsilon: float = self.config_daeg.max_epsilon  # 最大 epsilon 值\n",
    "    \n",
    "            self.dynamic_init_epsilon_min: float = self.config_daeg.dynamic_plot.init_epsilon_min  # 动态 epsilon 的最小值\n",
    "            self.dynamic_init_epsilon_max: float = self.config_daeg.dynamic_plot.init_epsilon_max  # 动态 epsilon 的最大值\n",
    "            self.dynamic_init_epsilon_step: float = self.config_daeg.dynamic_plot.init_epsilon_step  # 动态 epsilon 的步长\n",
    "            self.dynamic_init_epsilon_default_value: float = self.config_daeg.dynamic_plot.init_epsilon_default_value  # 动态 epsilon 的默认值\n",
    "    \n",
    "            self.dynamic_percentiles_min: float = self.config_daeg.dynamic_plot.percentiles_min  # 动态 percentiles 的最小值\n",
    "            self.dynamic_percentiles_max: float = self.config_daeg.dynamic_plot.percentiles_max  # 动态 percentiles 的最大值\n",
    "            self.dynamic_percentiles_step: float = self.config_daeg.dynamic_plot.percentiles_step  # 动态 percentiles 的步长\n",
    "            self.dynamic_percentiles_default_value: float = self.config_daeg.dynamic_plot.percentiles_default_value  # 动态 percentiles 的默认值\n",
    "    \n",
    "            self.evaluation_init_epsilon_min: float = self.config_daeg.evaluation.init_epsilon_min  # 评估图 init_epsilon 的最小值\n",
    "            self.evaluation_init_epsilon_max: float = self.config_daeg.evaluation.init_epsilon_max  # 评估图 init_epsilon 的最大值\n",
    "            self.evaluation_init_epsilon_step: float = self.config_daeg.evaluation.init_epsilon_step # 评估图 init_epsilon 的步长\n",
    "            self.evaluation_default_percentiles: float = self.config_daeg.evaluation.default_percentiles  # 评估图的百分位数\n",
    "            \n",
    "            self.evaluation_percentiles_min: float = self.config_daeg.evaluation.percentiles_min  # 评估图 percentiles 的最小值\n",
    "            self.evaluation_percentiles_max: float = self.config_daeg.evaluation.percentiles_max  # 评估图 percentiles 的最大值\n",
    "            self.evaluation_percentiles_step: float = self.config_daeg.evaluation.percentiles_step  # 评估图 percentiles 的步长\n",
    "            self.evaluation_default_init_epsilon: float = self.config_daeg.evaluation.default_init_epsilon\n",
    "            \n",
    "        def algorithm(self, init_epsilon: float, percentiles: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float], np.ndarray]:\n",
    "            \"\"\"\n",
    "            运行 dynamic adaptive epsilon-greedy 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - init_epsilon: epsilon 的初始值，表示随机选择节点的概率。\n",
    "            - percentiles: 百分位数，用于计算遗憾值的阈值。\n",
    "            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - epsilon_time: 一个数组，表示每个时间步的 epsilon 值。\n",
    "            \"\"\"\n",
    "            \n",
    "            epsilon: float = init_epsilon\n",
    "            def choose_node(epsilon, estimated_means):\n",
    "                if np.random.rand() < epsilon:\n",
    "                    return np.random.randint(self.mab.N)\n",
    "                else:\n",
    "                    return np.argmax(estimated_means)\n",
    "\n",
    "            estimated_means: np.ndarray = np.zeros(self.mab.N)\n",
    "            time_counts: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            nodes_counts: np.ndarray = np.zeros(self.mab.N)\n",
    "            single_step_regret: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            epsilon_time: np.ndarray = np.zeros(self.mab.T_test)\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                if t > 0: # 如果 t > 0，则根据单步遗憾调整epsilon，计算百分位数\n",
    "                    threshold: float = np.percentile(single_step_regret[:t], percentiles)\n",
    "                    if single_step_regret[t - 1] > threshold:\n",
    "                        # 如果单步遗憾值超过了阈值，计算遗憾值与阈值之间的差距，并进行归一化处理。归一化的计算方法是将差值除以阈值（加上一个非常小的值 1e−6，以防止分母为零）。\n",
    "                        # 归一化的结果 adjustment 会是一个相对值，用来调整 epsilon。这个值越大，说明当前的遗憾值相对于阈值越大，调整的幅度也就越大。\n",
    "                        adjustment: float = float(single_step_regret[t - 1] - threshold) / (threshold + 1e-6)  # 归一化的相对位置\n",
    "                        # 当前遗憾值超过阈值的情况下，通过增加 epsilon 的值来增强探索的力度\n",
    "                        epsilon = min(self.max_epsilon, epsilon + adjustment)\n",
    "                    else:\n",
    "                        # 如果当前的单步遗憾没有超过阈值，说明当前的动作表现相对较好，因此倾向于减少探索，更多地利用当前的策略。\n",
    "                        adjustment: float = float(threshold - single_step_regret[t - 1]) / (threshold + 1e-6)\n",
    "                        epsilon = max(self.min_epsilon, epsilon - adjustment)\n",
    "                else:\n",
    "                    # 如果 t == 0，则不计算百分位数，直接使用初始 epsilon\n",
    "                    epsilon_time[t] = epsilon\n",
    "\n",
    "                epsilon_time[t] = epsilon\n",
    "                chosen_node: int = choose_node(epsilon, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward: float = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, epsilon: float, epsilon_time: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy，以及 epsilon 随时间的变化。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - epsilon: 当前实验使用的 epsilon 值。\n",
    "            - epsilon_time: 一个数组，表示每个时间步的 epsilon 值（适用于 adaptive_epsilon_greedy）。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "            fig.suptitle(f\"Dynamically Adaptive Epsilon-Greedy Results\\nInitial Epsilon: {epsilon}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            axs[0, 0].axhline(y=self.mab.combine_reward_optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 epsilon_time 随时间的变化\n",
    "            axs[1, 0].plot(epsilon_time, marker='o', linestyle='-', color='purple')\n",
    "            axs[1, 0].set_title('Epsilon over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Epsilon')\n",
    "\n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_evaluation(self, hyperparameters: str) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon vs cumulative regret 图 和 epsilon vs top-k accuracy 图。\n",
    "            \"\"\"\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "            \n",
    "            if hyperparameters == 'init_epsilon':\n",
    "                epsilon_values: np.ndarray = np.arange(self.evaluation_init_epsilon_min, self.evaluation_init_epsilon_max, self.evaluation_init_epsilon_step)\n",
    "                fixed_percentiles: float = self.evaluation_default_percentiles\n",
    "    \n",
    "                for epsilon in epsilon_values:\n",
    "                    _, _, _, cumulative_regret, top_k_accuracy, _ = self.algorithm(epsilon, fixed_percentiles)\n",
    "                    cumulative_regrets.append(cumulative_regret[-1])\n",
    "                    time_top_k_accuracy.append(top_k_accuracy)\n",
    "\n",
    "            elif hyperparameters == 'percentiles':\n",
    "                fixed_epsion = self.evaluation_default_init_epsilon\n",
    "                percentiles_values: np.ndarray = np.arange(self.evaluation_percentiles_min, self.evaluation_percentiles_max, self.evaluation_percentiles_step)\n",
    "                for percentiles in percentiles_values:\n",
    "                    _, _, _, cumulative_regret, top_k_accuracy, _ = self.algorithm(fixed_epsion, percentiles)\n",
    "                    cumulative_regrets.append(cumulative_regret[-1])\n",
    "                    time_top_k_accuracy.append(top_k_accuracy)\n",
    "                    \n",
    "\n",
    "            \n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "            \n",
    "            if hyperparameters == 'init_epsilon':\n",
    "                fig.suptitle(f'Init Epsilon vs Cumulative Regret with fixed Percentiles: {self.evaluation_default_percentiles} and Top-k Accuracy')\n",
    "                plot_values = epsilon_values\n",
    "                \n",
    "            elif hyperparameters == 'percentiles':\n",
    "                fig.suptitle(f'Percentiles vs Cumulative Regret with fixed Init Epsilon: {self.evaluation_default_init_epsilon} and Top-k Accuracy')\n",
    "                plot_values = percentiles_values\n",
    "\n",
    "            \n",
    "            # 绘制 percentiles vs cumulative regret 图\n",
    "            axs[0].plot(plot_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Epsilon vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Epsilon')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={plot_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(plot_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(plot_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "\n",
    "            axs[0].annotate(f'Max (x={plot_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(plot_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(plot_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(plot_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                if hyperparameters == 'init_epsilon':\n",
    "                    axs[i+1].set_xlabel('Epsilon')\n",
    "                elif hyperparameters == 'percentiles':\n",
    "                    axs[i+1].set_xlabel('Percentiles')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_interactive(self, init_epsilon: float, percentiles: float):\n",
    "            \"\"\"\n",
    "            交互式绘制 dynamic adaptive epsilon-greedy 算法的结果。\n",
    "            :param init_epsilon: 初始 epsilon 值\n",
    "            :param percentiles: 百分位数\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, epsilon_time = self.algorithm(init_epsilon, percentiles)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, init_epsilon, epsilon_time)\n",
    "\n",
    "    class Boltzmann:\n",
    "        \"\"\"\n",
    "        Boltzmann 算法类。\n",
    "        Boltzmann策略基于每个动作的预期奖励（estimated mean）来计算一个概率分布，然后根据这个概率分布选择动作。这个概率分布由每个动作的预期奖励通过温度参数（temperature）调节后得到的软最大值（Softmax函数）确定。\n",
    "        控制参数为温度。当温度较高时，算法的选择更随机，倾向于探索；当温度较低时，算法更倾向于选择当前已知的最优动作（利用）。\n",
    "        当温度 τ 很低时，Softmax 函数倾向于将选择概率集中在奖励最高的动作上，类似于贪心策略。\n",
    "        随着温度 τ 的升高，选择概率变得更加均匀，Softmax 函数逐渐接近于随机选择。\n",
    "        适用场景：Boltzmann策略在奖励分布相对稳定、噪声较小的场景中表现较好，因为它可以根据已知信息较为精确地调整选择概率。\n",
    "        \n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_boltzmann: DictConfig = self.mab.config.boltzmann\n",
    "\n",
    "            self.dynamic_temperature_min: float = self.config_boltzmann.dynamic_plot.temperature_min  # 动态 temperature 的最小值\n",
    "            self.dynamic_temperature_max: float = self.config_boltzmann.dynamic_plot.temperature_max  # 动态 temperature 的最大值\n",
    "            self.dynamic_temperature_step: float = self.config_boltzmann.dynamic_plot.temperature_step  # 动态 temperature 的步长\n",
    "            self.dynamic_temperature_default_value: float = self.config_boltzmann.dynamic_plot.temperature_default_value  # 动态 temperature 的默认值\n",
    "    \n",
    "            self.evaluation_temperature_min: float = self.config_boltzmann.evaluation.temperature_min  # 评估图 temperature 的最小值\n",
    "            self.evaluation_temperature_max: float = self.config_boltzmann.evaluation.temperature_max  # 评估图 temperature 的最大值\n",
    "            self.evaluation_temperature_step: float = self.config_boltzmann.evaluation.temperature_step  # 评估图 temperature 的步长\n",
    "\n",
    "        def algorithm(self, temperature: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float], np.ndarray]:\n",
    "            \"\"\"\n",
    "            运行 Boltzmann 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - temperature: 温度值，用于调节选择概率。它控制了选择动作的随机性，温度越高，选择越随机，温度越低，选择越倾向于最优动作。\n",
    "            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            \"\"\"\n",
    "            def choose_node(temperature: float, estimated_means: np.ndarray) -> tuple[int, np.ndarray]:\n",
    "                exp_values: np.ndarray = np.exp(estimated_means / temperature)\n",
    "                probabilities: np.ndarray = exp_values / np.sum(exp_values)\n",
    "                return np.random.choice(self.mab.N, p=probabilities), probabilities\n",
    "\n",
    "            estimated_means: np.ndarray = np.zeros(self.mab.N)\n",
    "            time_counts: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            nodes_counts: np.ndarray = np.zeros(self.mab.N)\n",
    "            single_step_regret: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            time_probabilities: np.ndarray = np.zeros((self.mab.T_test, self.mab.N))\n",
    "        \n",
    "            for t in range(self.mab.T_test):\n",
    "                chosen_node, probabilities = choose_node(temperature, estimated_means)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                time_probabilities[t] = probabilities\n",
    "                reward: float = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, temperature: float, temperature_time_probabilities: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 epsilon-greedy 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy，以及 epsilon 随时间的变化。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - temperature: 当前实验使用的温度值。\n",
    "            - temperature_time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "            \n",
    "            fig.suptitle(f\"Boltzmann Results\\nTemperature: {temperature}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            axs[0, 0].axhline(y=self.mab.combine_reward_optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 temperature_time_probabilities 随时间的变化\n",
    "            for i in range(self.mab.N):\n",
    "                axs[1, 0].plot(temperature_time_probabilities[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            axs[1, 0].set_title('Node Selection Probabilities Over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Probability')\n",
    "            axs[1, 0].legend()\n",
    "            \n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_evaluation(self) -> None:\n",
    "            \"\"\"\n",
    "            绘制 temperature vs cumulative regret 图 和 temperature vs top-k accuracy 图。\n",
    "            \"\"\"\n",
    "            temperature_values: np.ndarray = np.arange(self.evaluation_temperature_min, self.evaluation_temperature_max, self.evaluation_temperature_step)\n",
    "\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "\n",
    "            for temperature in temperature_values:\n",
    "                _, _, _, cumulative_regret, top_k_accuracy, _ = self.algorithm(temperature)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "                time_top_k_accuracy.append(top_k_accuracy)\n",
    "\n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "            fig.suptitle('Temperature vs Cumulative Regret and Top-k Accuracy')\n",
    "\n",
    "            # 绘制 epsilon vs cumulative regret 图\n",
    "            axs[0].plot(temperature_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Temperature vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Temperature')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={temperature_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(temperature_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(temperature_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "\n",
    "            axs[0].annotate(f'Max (x={temperature_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(temperature_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(temperature_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(temperature_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                axs[i+1].set_xlabel('Temperature')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_interactive(self, temperature: float) -> None:\n",
    "            \"\"\"\n",
    "            交互式绘制 Boltzmann 算法的结果。\n",
    "            :param temperature: 固定的全局 temperature 值\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities = self.algorithm(temperature)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, temperature, time_probabilities)\n",
    "            \n",
    "    class UCB:\n",
    "        \"\"\"\n",
    "        UCB 算法类。\n",
    "        UCB 算法是一种基于置信上界的多臂赌博机算法，通过计算每个动作的置信上界来选择动作。\n",
    "        UCB 算法的核心思想是：在每个时间步 t，选择使得置信上界最大的动作。\n",
    "        置信上界的计算方法是：动作的平均奖励 + 置信区间（置信区间的计算方法是置信上界的上限减去置信上界的下限）。\n",
    "        控制参数为置信区间的参数 c。\n",
    "        适用场景：UCB 算法在奖励分布相对稳定、噪声较小的场景中表现较好，因为它可以根据已知信息较为精确地调整选择概率。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_ucb: DictConfig = self.mab.config.ucb\n",
    "\n",
    "            self.dynamic_c_min: float = self.config_ucb.dynamic_plot.c_min  # 动态 c 的最小值\n",
    "            self.dynamic_c_max: float = self.config_ucb.dynamic_plot.c_max  # 动态 c 的最大值\n",
    "            self.dynamic_c_step: float = self.config_ucb.dynamic_plot.c_step  # 动态 c 的步长\n",
    "            self.dynamic_c_default_value: float = self.config_ucb.dynamic_plot.c_default_value  # 动态 c 的默认值\n",
    "    \n",
    "            self.evaluation_c_min: float = self.config_ucb.evaluation.c_min  # 评估图 c 的最小值\n",
    "            self.evaluation_c_max: float = self.config_ucb.evaluation.c_max  # 评估图 c 的最大值\n",
    "            self.evaluation_c_step: float = self.config_ucb.evaluation.c_step  # 评估图 c 的步长\n",
    "\n",
    "        def algorithm(self, c: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float], np.ndarray]:\n",
    "            \"\"\"\n",
    "            运行 UCB 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - c: 置信区间的参数。它控制置信区间的宽度，进而影响探索和利用之间的平衡。较大的 c 值会增加置信区间的宽度，使得算法更加倾向于探索；较小的 c 值会缩小置信区间，使得算法更加倾向于利用当前已知的最优选择。简单来说，c 值决定了算法在选择过程中对不确定性（即未被充分探索的选项）的容忍度。\n",
    "            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - time_ucb_values: 一个二维数组，表示每个时间步的节点的 UCB 值, 即上置信界值。\n",
    "            \"\"\"\n",
    "            estimated_means: np.ndarray = np.zeros(self.mab.N)\n",
    "            time_counts: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            nodes_counts: np.ndarray = np.zeros(self.mab.N)\n",
    "            single_step_regret: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            time_ucb_values: np.ndarray = np.zeros((self.mab.T_test, self.mab.N))\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                ucb_values = estimated_means + c * np.sqrt(np.log(t + 1) / (nodes_counts + 1e-6))\n",
    "                time_ucb_values[t] = ucb_values\n",
    "                chosen_node: int = np.argmax(ucb_values)\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                reward: float = self.mab.combine_reward[chosen_node, t]\n",
    "                estimated_means[chosen_node] += (reward - estimated_means[chosen_node]) / nodes_counts[chosen_node]\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_ucb_values\n",
    "        \n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, c: float, time_ucb_values: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 UCB 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - c: 当前实验使用的 c 值。\n",
    "            - time_ucb_values: 一个二维数组，表示每个时间步的节点的 UCB 值\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "            fig.suptitle(f\"UCB Results\\nConfidence Interval Parameter: {c}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            axs[0, 0].axhline(y=self.mab.combine_reward_optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx: np.signedinteger = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx: np.signedinteger = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3：绘制 time_ucb_values 随时间的变化\n",
    "            for i in range(self.mab.N):\n",
    "                axs[1, 0].plot(time_ucb_values[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            axs[1, 0].set_title('UCB Values Over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('UCB Value')\n",
    "            axs[1, 0].legend()\n",
    "            \n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_evaluation(self) -> None:\n",
    "            \"\"\"\n",
    "            绘制 confidence interval parameter vs cumulative regret 图 和 confidence interval parameter vs top-k accuracy 图。\n",
    "            \"\"\"\n",
    "            c_values: np.ndarray = np.arange(self.evaluation_c_min, self.evaluation_c_max, self.evaluation_c_step)\n",
    "\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "\n",
    "            for c in c_values:\n",
    "                _, _, _, cumulative_regret, top_k_accuracy, _ = self.algorithm(c)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "                time_top_k_accuracy.append(top_k_accuracy)\n",
    "\n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "            fig.suptitle('UCB Confidence Interval Parameter vs Cumulative Regret and Top-k Accuracy')\n",
    "\n",
    "            # 绘制 confidence interval parameter vs cumulative regret 图\n",
    "            axs[0].plot(c_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Confidence Interval Parameter vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Confidence Interval Parameter')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={c_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(c_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(c_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "\n",
    "            axs[0].annotate(f'Max (x={c_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(c_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(c_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(c_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                axs[i+1].set_xlabel('Confidence Interval Parameter')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_interactive(self, c: float):\n",
    "            \"\"\"\n",
    "            交互式绘制 UCB 算法的结果。\n",
    "            :param c: 固定的全局 c 值\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_ucb_values = self.algorithm(c)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, c, time_ucb_values)\n",
    "            \n",
    "    class ThompsonSampling:\n",
    "        \"\"\"\n",
    "        Thompson Sampling 算法类。\n",
    "        Thompson Sampling 算法是一种基于贝叶斯推断的多臂赌博机算法，通过采样每个动作的奖励分布来选择动作。\n",
    "        Thompson Sampling 算法的核心思想是：在每个时间步 t，对每个动作采样一个奖励，然后选择使得采样奖励最大的动作。\n",
    "        采样奖励的计算方法是：对每个动作的奖励分布进行采样，然后选择采样值最大的动作。\n",
    "        控制参数为采样次数 T。\n",
    "        适用场景：Thompson Sampling 算法在奖励分布相对不稳定、噪声较大的场景中表现较好，因为它可以通过采样奖励分布来调整选择概率。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "\n",
    "        def algorithm(self, distribution_method: str) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float], np.ndarray]:\n",
    "            \"\"\"\n",
    "            运行 Thompson Sampling 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - distribution_method: 选择奖励分布的方法。可以选择：\n",
    "                - 'bernoulli': 伯努利分布, 适用于二元奖励, 例如点击率。\n",
    "                - 'normal': 正态分布, 适用于连续奖励, 例如收入。\n",
    "                - 'gamma': Gamma 分布, 适用于非负连续奖励, 例如转化率。\n",
    "            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            \"\"\"\n",
    "            if distribution_method == 'bernoulli':\n",
    "                alpha = np.ones(self.mab.N)  # 成功次数初始化为 1\n",
    "                beta = np.ones(self.mab.N)  # 失败次数初始化为 1\n",
    "                \n",
    "            elif distribution_method == 'normal':\n",
    "                mu = np.zeros(self.mab.N)  # 每个节点的均值估计初始化为 0\n",
    "                sigma2 = np.ones(self.mab.N)  # 每个节点的方差初始化为 1\n",
    "                \n",
    "            elif distribution_method == 'gamma':\n",
    "                shape = np.ones(self.mab.N)  # Gamma 分布的形状参数\n",
    "                rate = np.ones(self.mab.N)  # Gamma 分布的速率参数\n",
    "                \n",
    "            time_counts = np.zeros(self.mab.T_test)\n",
    "            nodes_counts = np.zeros(self.mab.N)\n",
    "            single_step_regret = np.zeros(self.mab.T_test)\n",
    "            time_theta = np.zeros((self.mab.T_test, self.mab.N))\n",
    "        \n",
    "            for t in range(self.mab.T_test):\n",
    "                if distribution_method == 'bernoulli':\n",
    "                    theta = np.random.beta(alpha, beta)  # 从 Beta 分布中采样\n",
    "                    chosen_node = np.argmax(theta)  # 选择使得采样值最大的节点\n",
    "                    nodes_counts[chosen_node] += 1  # 更新选择次数\n",
    "                    # 获取奖励\n",
    "                    reward = self.mab.combine_reward[chosen_node, t]\n",
    "                    # 更新 alpha 和 beta\n",
    "                    if reward == 1:\n",
    "                        alpha[chosen_node] += 1\n",
    "                    else:\n",
    "                        beta[chosen_node] += 1\n",
    "        \n",
    "                elif distribution_method == 'normal':\n",
    "                    theta = np.random.normal(mu, np.sqrt(sigma2))  # 从正态分布中采样\n",
    "                    chosen_node = np.argmax(theta)  # 选择使得采样值最大的节点\n",
    "                    # 获取奖励\n",
    "                    reward = self.mab.combine_reward[chosen_node, t]\n",
    "                    # 更新均值和方差\n",
    "                    nodes_counts[chosen_node] += 1\n",
    "                    mu[chosen_node] = (mu[chosen_node] * (nodes_counts[chosen_node] - 1) + reward) / nodes_counts[chosen_node]\n",
    "                    sigma2[chosen_node] = 1 / nodes_counts[chosen_node]  # 方差随选择次数的增加而减小\n",
    "                    \n",
    "                    \n",
    "                elif distribution_method == 'gamma':\n",
    "                    theta = np.random.gamma(shape, 1/rate)  # 从 Gamma 分布中采样\n",
    "                    chosen_node = np.argmax(theta)  # 选择使得采样值最大的节点\n",
    "                    # 获取奖励\n",
    "                    reward = self.mab.combine_reward[chosen_node, t]\n",
    "                    # 更新 shape 和 rate\n",
    "                    nodes_counts[chosen_node] += 1\n",
    "                    shape[chosen_node] += reward\n",
    "                    rate[chosen_node] += 1\n",
    "                \n",
    "                time_theta[t] = theta\n",
    "                time_counts[t] = chosen_node\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "        \n",
    "            cumulative_regret = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "        \n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_theta\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, time_theta: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 Thompson Sampling 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - time_theta: 一个二维数组，表示每个时间步的节点的 theta 值。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "            fig.suptitle(f\"Thompson Sampling Results\\nLoad Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1: plt.Axes = axs[0, 1]\n",
    "            ax2: plt.Axes = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3: 绘制 time_theta 随时间的变化\n",
    "            for i in range(self.mab.N):\n",
    "                axs[1, 0].plot(time_theta[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            axs[1, 0].set_title('Theta Values Over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Theta Value')\n",
    "            axs[1, 0].legend()\n",
    "\n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "        \n",
    "        def plot_interactive(self, distribution_method: str):\n",
    "            \"\"\"\n",
    "            交互式绘制 Thompson Sampling 算法的结果。\n",
    "            :param distribution_method: 选择奖励分布的方法\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_theta = self.algorithm(distribution_method)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_theta)\n",
    "            \n",
    "    class Exp3:\n",
    "        \"\"\"\n",
    "        Exp3 算法类。\n",
    "        Exp3 算法是一种基于概率的多臂赌博机算法，通过计算每个动作的概率分布来选择动作。\n",
    "        Exp3 算法的核心思想是：在每个时间步 t，根据每个动作的权重计算概率分布，然后按照这个概率分布选择动作。\n",
    "        算法的核心参数是学习率 eta。\n",
    "        适用场景：Exp3 算法在奖励分布相对稳定、噪声较小的场景中表现较好，因为它可以根据已知信息较为精确地调整选择概率。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_exp3: DictConfig = self.mab.config.exp3\n",
    "\n",
    "            self.dynamic_gamma_min: float = self.config_exp3.dynamic_plot.gamma_min  # 动态 eta 的最小值\n",
    "            self.dynamic_gamma_max: float = self.config_exp3.dynamic_plot.gamma_max  # 动态 eta 的最大值\n",
    "            self.dynamic_gamma_step: float = self.config_exp3.dynamic_plot.gamma_step  # 动态 eta 的步长\n",
    "            self.dynamic_gamma_default_value: float = self.config_exp3.dynamic_plot.gamma_default_value  # 动态 eta 的默认值\n",
    "    \n",
    "            self.evaluation_gamma_min: float = self.config_exp3.evaluation.gamma_min  # 评估图 eta 的最小值\n",
    "            self.evaluation_gamma_max: float = self.config_exp3.evaluation.gamma_max  # 评估图 eta 的最大值\n",
    "            self.evaluation_gamma_step: float = self.config_exp3.evaluation.gamma_step  # 评估图 eta 的步长\n",
    "\n",
    "\n",
    "        def algorithm(self, gamma: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float], np.ndarray]:\n",
    "            \"\"\"\n",
    "            运行 Exp3 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - eta: 学习率。它控制了算法在选择过程中对不确定性（即未被充分探索的选项）的容忍度。较大的 eta 值会增加对不确定性的容忍度，使得算法更加倾向于探索；较小的 eta 值会减小对不确定性的容忍度，使得算法更加倾向于利用当前已知的最优选择。\n",
    "            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            \"\"\"\n",
    "            weights: np.ndarray = np.ones(self.mab.N)\n",
    "            time_counts: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            nodes_counts: np.ndarray = np.zeros(self.mab.N)\n",
    "            single_step_regret: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            time_probabilities: np.ndarray = np.zeros((self.mab.T_test, self.mab.N))\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                probabilities = (1 - gamma) * (weights / np.sum(weights)) + gamma / self.mab.N  # 计算概率分布\n",
    "                chosen_node: int = np.random.choice(self.mab.N, p=probabilities)  # 根据概率分布选择节点\n",
    "                reward = self.mab.combine_reward[chosen_node, t]  # 获取奖励\n",
    "                estimates_reward: float = reward / probabilities[chosen_node]  # 计算奖励估计值\n",
    "                weights[chosen_node] *= np.exp(gamma * estimates_reward / self.mab.N)  # 更新权重\n",
    "\n",
    "                time_probabilities[t] = probabilities\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1                \n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "\n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, gamma: float, time_probabilities: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 Exp3 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - eta: 当前实验使用的 eta 值。\n",
    "            - time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "    \n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "            fig.suptitle(f\"Exp3 Results\\nGamma: {gamma}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            axs[0, 0].axhline(y=self.mab.combine_reward_optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = axs[0, 1]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx: np.signedinteger = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx: np.signedinteger = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3: 绘制 time_probabilities 随时间的变化\n",
    "            for i in range(self.mab.N):\n",
    "                axs[1, 0].plot(time_probabilities[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            axs[1, 0].set_title('Node Selection Probabilities Over Time')\n",
    "            axs[1, 0].set_xlabel('Time Step')\n",
    "            axs[1, 0].set_ylabel('Probability')\n",
    "            axs[1, 0].legend()\n",
    "\n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_evaluation(self) -> None:\n",
    "            \"\"\"\n",
    "            绘制 eta vs cumulative regret 图 和 eta vs top-k accuracy 图。\n",
    "            \"\"\"\n",
    "            gamma_values: np.ndarray = np.arange(self.evaluation_gamma_min, self.evaluation_gamma_max, self.evaluation_gamma_step)\n",
    "\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "\n",
    "            for gamma in gamma_values:\n",
    "                _, _, _, cumulative_regret, top_k_accuracy, _ = self.algorithm(gamma)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "                time_top_k_accuracy.append(top_k_accuracy)\n",
    "\n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "            fig.suptitle('Exp3 Gamma vs Cumulative Regret and Top-k Accuracy')\n",
    "\n",
    "            # 绘制 epsilon vs cumulative regret 图\n",
    "            axs[0].plot(gamma_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Gamma vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Gamma')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={gamma_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(gamma_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(gamma_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "\n",
    "            axs[0].annotate(f'Max (x={gamma_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(gamma_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(gamma_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(gamma_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                axs[i+1].set_xlabel('Gamma')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_interactive(self, gamma: float):\n",
    "            \"\"\"\n",
    "            交互式绘制 Exp3 算法的结果。\n",
    "            :param gamma: 固定的全局 eta 值\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities = self.algorithm(gamma)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, gamma, time_probabilities)\n",
    "            \n",
    "    class ExpIx:\n",
    "        \"\"\"\n",
    "        Exp3-IX 算法类。\n",
    "        Exp3-IX 算法是 Exp3 算法的一种变种，通过引入探索指数来增加对未知节点的探索。\n",
    "        Exp3-IX 算法的核心思想是：在每个时间步 t，根据每个动作的权重计算概率分布，然后按照这个概率分布选择动作。\n",
    "        算法的核心参数是学习率 eta 和探索指数 xi。\n",
    "        适用场景：Exp3-IX 算法在奖励分布相对稳定、噪声较小的场景中表现较好，因为它可以根据已知信息较为精确地调整选择概率。\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_exp_ix: DictConfig = self.mab.config.exp_ix\n",
    "            \n",
    "            self.dynamic_eta_min: float = self.config_exp_ix.dynamic_plot.eta_min  # 动态 eta 的最小值\n",
    "            self.dynamic_eta_max: float = self.config_exp_ix.dynamic_plot.eta_max  # 动态 eta 的最大值\n",
    "            self.dynamic_eta_step: float = self.config_exp_ix.dynamic_plot.eta_step  # 动态 eta 的步长\n",
    "            self.dynamic_eta_default_value: float = self.config_exp_ix.dynamic_plot.eta_default_value  # 动态 eta 的默认值\n",
    "            \n",
    "            self.evaluation_eta_min: float = self.config_exp_ix.evaluation.eta_min  # 评估图 eta 的最小值\n",
    "            self.evaluation_eta_max: float = self.config_exp_ix.evaluation.eta_max  # 评估图 eta 的最大值\n",
    "            self.evaluation_eta_step: float = self.config_exp_ix.evaluation.eta_step  # 评估图 eta 的步长\n",
    "            \n",
    "            self.eta_method: str | None = None  # eta 的计算方法\n",
    "\n",
    "\n",
    "        def algorithm(self, eta: float, eta_method: str) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict, np.ndarray, np.ndarray]:\n",
    "            \"\"\"\n",
    "            运行 Exp3-IX 算法，返回选择的节点、每个节点的选择次数、单次遗憾、累积遗憾以及 Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - eta: 学习率。它控制了算法在选择过程中对不确定性（即未被充分探索的选项）的容忍度。较大的 eta 值会增加对不确定性的容忍度，使得算法更加倾向于探索；较小的 eta 值会减小对不确定性的容忍度，使得算法更加倾向于利用当前已知的最优选择。\n",
    "            - eta_method: eta 的计算方法。可选值为：\n",
    "                - 'auto_Nt': eta = sqrt(ln(N) / t)\n",
    "                - 'auto_t': eta = 1 / sqrt(t)\n",
    "                - 'auto_log_t': eta = ln(T) / sqrt(t + ln(T))\n",
    "                - 'auto_gradient_norm': eta = eta * exp(-t / T)\n",
    "                - 'manual': 手动设置固定的 eta\n",
    "                            \n",
    "            返回:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - nodes_counts: 一个数组，表示每个节点的选择次数。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            - time_eta: 一个数组，表示每个时间步的 eta 值。\n",
    "            \"\"\"\n",
    "            self.eta_method = eta_method\n",
    "            weights: np.ndarray = np.ones(self.mab.N)  # 初始化权重\n",
    "            time_counts: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            nodes_counts: np.ndarray = np.zeros(self.mab.N)\n",
    "            single_step_regret: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            time_probabilities: np.ndarray = np.zeros((self.mab.T_test, self.mab.N))\n",
    "            time_eta: np.ndarray = np.zeros(self.mab.T_test)\n",
    "            max_eta: float = 1.0\n",
    "            max_weight_value = 1e8  # 设置权重的最大值\n",
    "            before_exp_max = 600 # 经验值，exp(700)接近浮点数安全上限\n",
    "\n",
    "            for t in range(self.mab.T_test):\n",
    "                probabilities: np.ndarray = weights / np.sum(weights)   # 计算概率分布(不再使用显式的 eta)\n",
    "                \n",
    "                # # 检查并修正 NaN 和 Inf 值\n",
    "                # if np.any(np.isnan(probabilities)) or np.any(np.isinf(probabilities)):\n",
    "                #     probabilities = np.nan_to_num(probabilities, nan=1.0/self.mab.N, posinf=1.0/self.mab.N, neginf=1.0/self.mab.N)\n",
    "                \n",
    "                # # 归一化，使概率总和为 1\n",
    "                # probabilities /= np.sum(probabilities)\n",
    "                \n",
    "                chosen_node: int = np.random.choice(self.mab.N, p=probabilities)  # 根据概率分布选择节点\n",
    "                reward: float = self.mab.combine_reward[chosen_node, t]  # 获取奖励\n",
    "                estimates_reward = reward / probabilities[chosen_node]  # 计算奖励估计值\n",
    "                \n",
    "                if self.eta_method == 'auto_NT':\n",
    "                    eta: float = min(max_eta, np.sqrt(np.log(self.mab.N) / (t + 1)))\n",
    "                elif self.eta_method == 'auto_T':\n",
    "                    eta: float = min(max_eta, 1 / np.sqrt(t + 1))\n",
    "                elif self.eta_method == 'auto_log_T':\n",
    "                    eta: float = min(max_eta, np.log(self.mab.T_test) / np.sqrt(t + np.log(self.mab.T_test)))\n",
    "                elif self.eta_method == 'auto_gradient_norm':\n",
    "                    # 计算梯度\n",
    "                    gradient = -reward * (1 / probabilities[chosen_node]) * np.log(weights[chosen_node])\n",
    "                    # 计算梯度的L2范数\n",
    "                    gradient_norm = np.linalg.norm(gradient)\n",
    "                    eta: float = min(max_eta, 1 / (1 + gradient_norm)) # 使用梯度范数动态调整eta，梯度范数越大，eta越小\n",
    "                elif self.eta_method == 'manual':\n",
    "                    eta: float = min(max_eta, eta)\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid eta_method: {self.eta_method}\")\n",
    "\n",
    "                before_exp = np.clip((eta * estimates_reward / self.mab.N), None, before_exp_max)  # 确保不超过上限\n",
    "                    \n",
    "                weights[chosen_node] *= np.exp(before_exp)  # 更新权重（使用隐式探索，即通过 eta 控制的权重更新）\n",
    "                # 限制权重的最大值\n",
    "                weights = np.clip(weights, None, max_weight_value)  # 将所有权重限制在 [0, max_weight_value] 之间\n",
    "\n",
    "                time_probabilities[t] = probabilities\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                time_eta[t] = eta\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "                \n",
    "                # self.plot_evaluation()\n",
    "\n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities, time_eta\n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, eta: float, time_probabilities: np.ndarray, time_eta: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 Exp3-IX 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy。\n",
    "            \n",
    "            参数:\n",
    "            - time_counts: 一个数组，表示在每个时间步中选择的节点。\n",
    "            - single_step_regret: 一个数组，表示每次选择的单次遗憾。\n",
    "            - cumulative_regret: 一个数组，表示累积遗憾。\n",
    "            - top_k_accuracy: 一个字典，表示不同 k 值对应的 Top-k Accuracy。\n",
    "            - eta: 当前实验使用的 eta 值。\n",
    "            - time_probabilities: 一个二维数组，表示每个时间步的节点选择概率。\n",
    "            - time_eta: 一个数组，表示每个时间步的 eta 值。\n",
    "            \"\"\"\n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "            fig.suptitle(f\"Exp3-IX Results\\nEta Method: {self.eta_method}, Eta: {eta}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "\n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            axs[0, 0].plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            axs[0, 0].set_title('Node Selection Over Time')\n",
    "            axs[0, 0].set_xlabel('Time Step')\n",
    "            axs[0, 0].set_ylabel('Chosen Node')\n",
    "\n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            optimal_node = self.mab.combine_reward_optimal_node\n",
    "            axs[0, 0].axhline(y=optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            axs[0, 0].legend()\n",
    "\n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1: plt.Axes = axs[0, 1]\n",
    "            ax2: plt.Axes = ax1.twinx()\n",
    "\n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "\n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "\n",
    "            # 标记最小和最大点\n",
    "            min_idx = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "\n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "\n",
    "            # 子图3: 绘制 time_probabilities 随时间的变化和 time_eta 随时间的变化\n",
    "            ax1: plt.Axes = axs[1, 0]\n",
    "            ax2: plt.Axes = ax1.twinx()\n",
    "            \n",
    "            for i in range(self.mab.N):\n",
    "                ax1.plot(time_probabilities[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            ax1.set_title('Node Selection Probabilities Over Time')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Probability')\n",
    "            ax1.legend()\n",
    "            \n",
    "            ax2.plot(time_eta, color='red', marker='x', linestyle='--', label='Eta')\n",
    "            ax2.set_ylabel('Eta', color='red')\n",
    "            ax2.legend(loc='upper right')\n",
    "\n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            axs[1, 1].bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            axs[1, 1].set_title('Top-k Accuracy')\n",
    "            axs[1, 1].set_xlabel('k')\n",
    "            axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            # self.plot_evaluation()\n",
    "\n",
    "        def plot_evaluation(self):\n",
    "            \"\"\"\n",
    "            绘制 confidence interval parameter vs cumulative regret 图。\n",
    "            \"\"\"\n",
    "            eta_values: np.ndarray = np.arange(self.evaluation_eta_min, self.evaluation_eta_max, self.evaluation_eta_step)\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "\n",
    "            for gamma in eta_values:\n",
    "                _, _, _, cumulative_regret, top_k_accuracy, _, _ = self.algorithm(gamma, self.eta_method)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "                time_top_k_accuracy.append(top_k_accuracy)\n",
    "\n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "            fig.suptitle(f'Exp3-IX Eta vs Cumulative Regret and Top-k Accuracy\\nEta Method: {self.eta_method}')\n",
    "\n",
    "            # 绘制 epsilon vs cumulative regret 图\n",
    "            axs[0].plot(eta_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Eta vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Eta')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={eta_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(eta_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(eta_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "\n",
    "            axs[0].annotate(f'Max (x={eta_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(eta_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(eta_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(eta_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                axs[i+1].set_xlabel('Eta')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_interactive(self, eta: float, eta_method: str):\n",
    "            \"\"\"\n",
    "            交互式绘制 Exp3-IX 算法的结果。\n",
    "            :param eta: 固定的全局 eta 值\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities, time_eta = self.algorithm(eta, eta_method)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, eta, time_probabilities, time_eta)\n",
    "\n",
    "    class Exp4:\n",
    "        \"\"\"\n",
    "        Exp4 算法类\n",
    "        \"\"\"\n",
    "        def __init__(self, mab):\n",
    "            self.mab: MAB = mab\n",
    "            self.config_exp3: DictConfig = self.mab.config.exp3\n",
    "\n",
    "            self.dynamic_gamma_min: float = self.config_exp3.dynamic_plot.gamma_min  # 动态 eta 的最小值\n",
    "            self.dynamic_gamma_max: float = self.config_exp3.dynamic_plot.gamma_max  # 动态 eta 的最大值\n",
    "            self.dynamic_gamma_step: float = self.config_exp3.dynamic_plot.gamma_step  # 动态 eta 的步长\n",
    "            self.dynamic_gamma_default_value: float = self.config_exp3.dynamic_plot.gamma_default_value  # 动态 eta 的默认值\n",
    "\n",
    "            self.evaluation_gamma_min: float = self.config_exp3.evaluation.gamma_min  # 评估图 eta 的最小值\n",
    "            self.evaluation_gamma_max: float = self.config_exp3.evaluation.gamma_max  # 评估图 eta 的最大值\n",
    "            self.evaluation_gamma_step: float = self.config_exp3.evaluation.gamma_step  # 评估图 eta 的步长\n",
    "\n",
    "            # 初始化专家的combine_reward\n",
    "            self.expert_rewards = [\n",
    "                self.mab.exp4_data_manager.ar_data.combine_reward,\n",
    "                self.mab.exp4_data_manager.lstm_data.combine_reward,\n",
    "                self.mab.exp4_data_manager.gnn_data.combine_reward\n",
    "            ]\n",
    "\n",
    "        def algorithm(self, gamma: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, float], np.ndarray, np.ndarray]:\n",
    "            \n",
    "            K = len(self.expert_rewards)  # 专家数量（固定为3）\n",
    "            N = self.mab.N  # 节点数量\n",
    "            T = self.mab.T_test  # 轮次数\n",
    "    \n",
    "            weights: np.ndarray = np.ones(K)\n",
    "            time_counts: np.ndarray = np.zeros(T)\n",
    "            nodes_counts: np.ndarray = np.zeros(N)\n",
    "            single_step_regret: np.ndarray = np.zeros(T)\n",
    "            time_probabilities: np.ndarray = np.zeros((T, N))\n",
    "            time_weights: np.ndarray = np.zeros((T, K))\n",
    "    \n",
    "            for t in range(T):\n",
    "                # 1. 从每个专家获取当前时间步的建议（即每个专家在时间步t对所有节点的combine_reward值）\n",
    "                experts_advice = np.array([reward[:, t] for reward in self.expert_rewards])  # shape (K, N)\n",
    "    \n",
    "                # 2. 计算每个节点的最终选择概率\n",
    "                weighted_probabilities = np.dot(weights, experts_advice) / np.sum(weights)\n",
    "                probabilities = (1 - gamma) * weighted_probabilities + gamma / N\n",
    "                probabilities /= np.sum(probabilities)  # 确保总和为 1\n",
    "    \n",
    "                # 3. 根据概率选择节点\n",
    "                chosen_node = np.random.choice(N, p=probabilities)\n",
    "                reward = self.mab.combine_reward[chosen_node, t]  # 获取奖励\n",
    "    \n",
    "                # 4. 更新选择的专家的权重\n",
    "                for i in range(K):\n",
    "                    estimated_reward = reward / experts_advice[i, chosen_node]\n",
    "                    weights[i] *= np.exp(gamma * estimated_reward / N)\n",
    "    \n",
    "                # 5. 记录相关信息\n",
    "                time_probabilities[t] = probabilities\n",
    "                time_counts[t] = chosen_node\n",
    "                nodes_counts[chosen_node] += 1\n",
    "                single_step_regret[t] = self.mab.combine_reward_optimal_mean - reward\n",
    "                time_weights[t] = weights\n",
    "    \n",
    "            cumulative_regret: np.ndarray = np.cumsum(single_step_regret)\n",
    "            top_k_accuracy: dict[int, float] = self.mab.calculate_top_k_accuracy(time_counts, self.mab.top_k)\n",
    "            return time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities, time_weights\n",
    "            \n",
    "\n",
    "        def plot_results(self, time_counts: np.ndarray, single_step_regret: np.ndarray, cumulative_regret: np.ndarray, top_k_accuracy: dict, gamma: float, time_probabilities: np.ndarray, time_weights: np.ndarray) -> None:\n",
    "            \"\"\"\n",
    "            绘制 Exp4 算法的结果，包括选择的节点、单次遗憾、累积遗憾、Top-k Accuracy。\n",
    "            \"\"\"\n",
    "            \n",
    "            # 应用平滑方法\n",
    "            smoothed_single_step_regret: np.ndarray = exponential_moving_average(single_step_regret)\n",
    "            smoothed_cumulative_regret: np.ndarray = exponential_moving_average(cumulative_regret)\n",
    "\n",
    "            fig = plt.figure(figsize=(14, 12))\n",
    "            gs = gridspec.GridSpec(3, 2, height_ratios=[1, 1, 0.6])  # 调整最后一行的高度比例\n",
    "        \n",
    "            fig.suptitle(f\"Exp4 Results\\nGamma: {gamma}, Load Reward Method: {self.mab.load_reward_method}, Data Type: {self.mab.data_type}\", fontsize=16)\n",
    "        \n",
    "            # 子图1：time_counts 绘制 T_test 次里每次选择的节点\n",
    "            ax0 = fig.add_subplot(gs[0, 0])\n",
    "            ax0.plot(time_counts, marker='o', linestyle='-', color='blue')\n",
    "            ax0.set_title('Node Selection Over Time')\n",
    "            ax0.set_xlabel('Time Step')\n",
    "            ax0.set_ylabel('Chosen Node')\n",
    "        \n",
    "            # 在子图1上绘制 combine_reward_optimal_node 的横向虚线\n",
    "            ax0.axhline(y=self.mab.combine_reward_optimal_node, color='red', linestyle='--', label='Optimal Node')\n",
    "            ax0.legend()\n",
    "        \n",
    "            # 子图2：smoothed_single_step_regret 和 smoothed_cumulative_regret 使用双 y 轴\n",
    "            ax1 = fig.add_subplot(gs[0, 1])\n",
    "            ax2 = ax1.twinx()\n",
    "        \n",
    "            ax1.plot(smoothed_single_step_regret, marker='o', linestyle='-', color='green', label='Smoothed Single Step Regret')\n",
    "            ax2.plot(smoothed_cumulative_regret, marker='o', linestyle='-', color='red', label='Smoothed Cumulative Regret')\n",
    "        \n",
    "            ax1.set_title('Single Step and Cumulative Regret (Smoothed)')\n",
    "            ax1.set_xlabel('Time Step')\n",
    "            ax1.set_ylabel('Single Step Regret', color='green')\n",
    "            ax2.set_ylabel('Cumulative Regret', color='red')\n",
    "        \n",
    "            # 标记最小和最大点\n",
    "            min_idx: np.signedinteger = np.argmin(smoothed_single_step_regret)\n",
    "            max_idx: np.signedinteger = np.argmax(smoothed_single_step_regret)\n",
    "            ax1.annotate(f'Min (x={min_idx}, y={smoothed_single_step_regret[min_idx]:.2f})',\n",
    "                         xy=(min_idx, smoothed_single_step_regret[min_idx]),\n",
    "                         xytext=(min_idx, smoothed_single_step_regret[min_idx] + 0.05),\n",
    "                         arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                         fontsize=10, color='blue')\n",
    "        \n",
    "            ax1.annotate(f'Max (x={max_idx}, y={smoothed_single_step_regret[max_idx]:.2f})',\n",
    "                         xy=(max_idx, smoothed_single_step_regret[max_idx]),\n",
    "                         xytext=(max_idx, smoothed_single_step_regret[max_idx] - 0.05),\n",
    "                         arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                         fontsize=10, color='red')\n",
    "        \n",
    "            # 子图3: 绘制 time_probabilities 随时间的变化\n",
    "            ax3 = fig.add_subplot(gs[1, 0])\n",
    "            for i in range(self.mab.N):\n",
    "                ax3.plot(time_probabilities[:, i], marker='o', linestyle='-', label=f'Node {i}')\n",
    "            ax3.set_title('Node Selection Probabilities Over Time')\n",
    "            ax3.set_xlabel('Time Step')\n",
    "            ax3.set_ylabel('Probability')\n",
    "            ax3.legend()\n",
    "        \n",
    "            # 子图4：柱状图绘制 top_k_accuracy\n",
    "            ax4 = fig.add_subplot(gs[1, 1])\n",
    "            ax4.bar(top_k_accuracy.keys(), top_k_accuracy.values(), color='purple')\n",
    "            ax4.set_title('Top-k Accuracy')\n",
    "            ax4.set_xlabel('k')\n",
    "            ax4.set_ylabel('Accuracy')\n",
    "        \n",
    "            # 子图5：绘制 time_weights 随时间的变化，占据最后一整行\n",
    "            ax5 = fig.add_subplot(gs[2, :])\n",
    "            ax5.plot(time_weights[:, 0], marker='o', linestyle='-', label='Expert AR')\n",
    "            ax5.plot(time_weights[:, 1], marker='o', linestyle='-', label='Expert LSTM')\n",
    "            ax5.plot(time_weights[:, 2], marker='o', linestyle='-', label='Expert GNN')\n",
    "            ax5.set_title('Expert Weights Over Time')\n",
    "            ax5.set_xlabel('Time Step')\n",
    "            ax5.set_ylabel('Weight')\n",
    "            ax5.legend()\n",
    "        \n",
    "            # 调整布局\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_evaluation(self) -> None:\n",
    "            \"\"\"\n",
    "            绘制 eta vs cumulative regret 图 和 eta vs top-k accuracy 图。\n",
    "            \"\"\"\n",
    "            gamma_values: np.ndarray = np.arange(self.evaluation_gamma_min, self.evaluation_gamma_max, self.evaluation_gamma_step)\n",
    "\n",
    "            cumulative_regrets: list[float] = []\n",
    "            time_top_k_accuracy: list[dict[int, float]] = []\n",
    "\n",
    "            for gamma in gamma_values:\n",
    "                _, _, _, cumulative_regret, top_k_accuracy, _, _ = self.algorithm(gamma)\n",
    "                cumulative_regrets.append(cumulative_regret[-1])\n",
    "                time_top_k_accuracy.append(top_k_accuracy)\n",
    "\n",
    "            min_idx: np.signedinteger = np.argmin(cumulative_regrets)\n",
    "            max_idx: np.signedinteger = np.argmax(cumulative_regrets)\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "            fig.suptitle('Exp4 Gamma vs Cumulative Regret and Top-k Accuracy')\n",
    "\n",
    "            # 绘制 epsilon vs cumulative regret 图\n",
    "            axs[0].plot(gamma_values, cumulative_regrets, marker='o', linestyle='-', color='blue')\n",
    "            axs[0].set_title('Gamma vs Cumulative Regret')\n",
    "            axs[0].set_xlabel('Gamma')\n",
    "            axs[0].set_ylabel('Cumulative Regret')\n",
    "\n",
    "            # 标记最小值和最大值\n",
    "            axs[0].annotate(f'Min (x={gamma_values[min_idx]:.3f}, y={cumulative_regrets[min_idx]:.2f})',\n",
    "                            xy=(gamma_values[min_idx], cumulative_regrets[min_idx]),\n",
    "                            xytext=(gamma_values[min_idx] + 0.02, cumulative_regrets[min_idx] + 0.02),\n",
    "                            arrowprops=dict(facecolor='blue', arrowstyle='->'),\n",
    "                            fontsize=10, color='blue')\n",
    "\n",
    "            axs[0].annotate(f'Max (x={gamma_values[max_idx]:.3f}, y={cumulative_regrets[max_idx]:.2f})',\n",
    "                            xy=(gamma_values[max_idx], cumulative_regrets[max_idx]),\n",
    "                            xytext=(gamma_values[max_idx] - 0.1, cumulative_regrets[max_idx] - 0.02),\n",
    "                            arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "                            fontsize=10, color='red')\n",
    "\n",
    "            # 绘制 top-k accuracy，单独显示每个 k\n",
    "            for i, k in enumerate([1, 2, 5]):\n",
    "                axs[i+1].plot(gamma_values, [top_k_accuracy[k] for top_k_accuracy in time_top_k_accuracy], marker='o', linestyle='-', color='green')\n",
    "                axs[i+1].set_title(f'Top-{k} Accuracy')\n",
    "                axs[i+1].set_xlabel('Gamma')\n",
    "                axs[i+1].set_ylabel('Accuracy')\n",
    "                axs[i+1].set_ylim(0, 1)\n",
    "\n",
    "            plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.95))  # 调整 tight_layout，使得 suptitle 不会与子图重叠\n",
    "            plt.show()\n",
    "\n",
    "        def plot_interactive(self, gamma: float):\n",
    "            \"\"\"\n",
    "            交互式绘制 Exp4 算法的结果。\n",
    "            :param gamma: 固定的全局 eta 值\n",
    "            \"\"\"\n",
    "            time_counts, nodes_counts, single_step_regret, cumulative_regret, top_k_accuracy, time_probabilities, time_weights = self.algorithm(gamma)\n",
    "            self.plot_results(time_counts, single_step_regret, cumulative_regret, top_k_accuracy, gamma, time_probabilities, time_weights)\n",
    "\n",
    "    def dynamic_plot(self):\n",
    "        \"\"\"\n",
    "        动态绘制算法结果。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 调整 ToggleButtons 部分的代码\n",
    "        load_reward_method_widget = widgets.ToggleButtons(\n",
    "            options=['reward_0', 'reward_1'],\n",
    "            value='reward_0',\n",
    "            description='Reward Method:',\n",
    "            button_style='info',  # 可以设置为 'success', 'info', 'warning', 'danger' 来改变按钮颜色\n",
    "            layout=widgets.Layout(width='150px', height='auto', margin='10px')  # 调整宽度、高度并增加间距\n",
    "        )\n",
    "        \n",
    "        data_type_widget = widgets.ToggleButtons(\n",
    "            options=['iid', 'ar1'],\n",
    "            value='iid',\n",
    "            description='Data Type:',\n",
    "            button_style='warning',  # 同样可以设置按钮的样式\n",
    "            layout=widgets.Layout(width='150px', height='auto', margin='10px')  # 调整宽度、高度并增加间距\n",
    "        )\n",
    "        \n",
    "        algorithm_type_widget = widgets.ToggleButtons(\n",
    "            options=['epsilon_greedy', 'adaptive_epsilon_greedy', 'dynamic_adaptive_epsilon_greedy', 'boltzmann', 'thompson_sampling', 'ucb', 'exp3', 'exp3_ix', 'exp4'],\n",
    "            value='epsilon_greedy',\n",
    "            description='Algorithm:',\n",
    "            button_style='success',  # 设置按钮样式\n",
    "            style={'button_width': '300px'},  # 使用 style 调整按钮宽度\n",
    "            layout=widgets.Layout(width='300px', height='auto', margin='10px')  # 设置按钮的宽度\n",
    "        )\n",
    "        output = Output()\n",
    "\n",
    "        def on_button_clicked(b):\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                self.set_parameters(load_reward_method_widget.value, data_type_widget.value)\n",
    "                self.plot_combine_rewards()\n",
    "\n",
    "                # 根据不同的算法类型显示对应的滑块\n",
    "                if algorithm_type_widget.value == 'epsilon_greedy':\n",
    "                    self.algorithm = self.EpsilonGreedy(self)\n",
    "                    epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_epsilon_default_value,\n",
    "                        description='Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    interact(self.algorithm.plot_interactive, epsilon=epsilon_widget)\n",
    "                    self.algorithm.plot_evaluation()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'adaptive_epsilon_greedy':\n",
    "                    self.algorithm = self.AdaptiveEpsilonGreedy(self)\n",
    "                    init_epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_init_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_init_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_init_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_init_epsilon_default_value,\n",
    "                        description='Initial Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    min_epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_min_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_min_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_min_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_min_epsilon_default_value,\n",
    "                        description='Min Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                    )\n",
    "                    interact(self.algorithm.plot_interactive, init_epsilon=init_epsilon_widget, min_epsilon=min_epsilon_widget)\n",
    "                    self.algorithm.plot_evaluation()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'dynamic_adaptive_epsilon_greedy':\n",
    "                    self.algorithm = self.DynamicAdaptiveEpsilonGreedy(self)\n",
    "                    init_epsilon_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_init_epsilon_min,\n",
    "                        max=self.algorithm.dynamic_init_epsilon_max,\n",
    "                        step=self.algorithm.dynamic_init_epsilon_step,\n",
    "                        value=self.algorithm.dynamic_init_epsilon_default_value,\n",
    "                        description='Initial Epsilon:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    percentiles_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_percentiles_min,\n",
    "                        max=self.algorithm.dynamic_percentiles_max,\n",
    "                        step=self.algorithm.dynamic_percentiles_step,\n",
    "                        value=self.algorithm.dynamic_percentiles_default_value,\n",
    "                        description='Percentiles:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    interact(self.algorithm.plot_interactive, init_epsilon=init_epsilon_widget, percentiles=percentiles_widget)\n",
    "                    self.algorithm.plot_evaluation('init_epsilon')\n",
    "                    self.algorithm.plot_evaluation('percentiles')\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'boltzmann':\n",
    "                    self.algorithm = self.Boltzmann(self)\n",
    "                    temperature_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_temperature_min,\n",
    "                        max=self.algorithm.dynamic_temperature_max,\n",
    "                        step=self.algorithm.dynamic_temperature_step,\n",
    "                        value=self.algorithm.dynamic_temperature_default_value,\n",
    "                        description='Temperature:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    interact(self.algorithm.plot_interactive, temperature=temperature_widget)\n",
    "                    self.algorithm.plot_evaluation()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'thompson_sampling':\n",
    "                    self.algorithm = self.ThompsonSampling(self)\n",
    "                    distribution_method_widget = widgets.ToggleButtons(\n",
    "                        options=['bernoulli', 'normal', 'gamma'],\n",
    "                        value='bernoulli',\n",
    "                        description='Distribution Method:',\n",
    "                        button_style='info',  # 可以设置为 'success', 'info', 'warning', 'danger' 来改变按钮颜色\n",
    "                        layout=widgets.Layout(width='600px', height='auto', margin='10px')  # 调整宽度、高度并增加间距\n",
    "                    )                    \n",
    "                    interact(self.algorithm.plot_interactive, distribution_method=distribution_method_widget)\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'ucb':\n",
    "                    self.algorithm = self.UCB(self)\n",
    "                    c_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_c_min,\n",
    "                        max=self.algorithm.dynamic_c_max,\n",
    "                        step=self.algorithm.dynamic_c_step,\n",
    "                        value=self.algorithm.dynamic_c_default_value,\n",
    "                        description='Confidence Interval Parameter:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    interact(self.algorithm.plot_interactive, c=c_widget)\n",
    "                    self.algorithm.plot_evaluation()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'exp3':\n",
    "                    self.algorithm = self.Exp3(self)\n",
    "                    gamma_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_gamma_min,\n",
    "                        max=self.algorithm.dynamic_gamma_max,\n",
    "                        step=self.algorithm.dynamic_gamma_step,\n",
    "                        value=self.algorithm.dynamic_gamma_default_value,\n",
    "                        description='Gamma:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    interact(self.algorithm.plot_interactive, gamma=gamma_widget)\n",
    "                    self.algorithm.plot_evaluation()\n",
    "                    \n",
    "                elif algorithm_type_widget.value == 'exp3_ix':\n",
    "                    self.algorithm = self.ExpIx(self)\n",
    "                    eta_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_eta_min,\n",
    "                        max=self.algorithm.dynamic_eta_max,\n",
    "                        step=self.algorithm.dynamic_eta_step,\n",
    "                        value=self.algorithm.dynamic_eta_default_value,\n",
    "                        description='Eta:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    eta_method_widget = widgets.ToggleButtons(\n",
    "                        options=['auto_NT', 'auto_T', 'auto_log_T', 'auto_gradient_norm', 'manual'],\n",
    "                        value='auto_NT',\n",
    "                        description='Eta Method:',\n",
    "                        button_style='info',  # 可以设置为 'success', 'info', 'warning', 'danger' 来改变按钮颜色\n",
    "                        layout=widgets.Layout(width='600px', height='auto', margin='10px')  # 调整宽度、高度并增加间距\n",
    "                    )\n",
    "                    # interact(self.algorithm.plot_interactive, eta=eta_widget, eta_method=eta_method_widget)\n",
    "                    # self.algorithm.plot_evaluation()\n",
    "                    \n",
    "                    # 重新绑定交互控件的事件处理函数\n",
    "                    def update_plot(eta, eta_method):\n",
    "                        self.algorithm.plot_interactive(eta, eta_method)\n",
    "                        self.algorithm.plot_evaluation()  # 确保每次都调用 plot_evaluation\n",
    "\n",
    "                    # 绑定 interact 方法\n",
    "                    interact(update_plot, eta=eta_widget, eta_method=eta_method_widget)\n",
    "\n",
    "                    # # 手动调用以初始化显示\n",
    "                    # update_plot(eta_widget.value, eta_method_widget.value)\n",
    "\n",
    "                # 每次点击都会重新设置参数，但只执行一次 exp4_generator\n",
    "                # elif algorithm_type_widget.value == 'exp4' and not self.exp4_executed:\n",
    "                elif algorithm_type_widget.value == 'exp4':\n",
    "                    # next(self.exp4_gen)  # 执行延迟的 exp4 计算\n",
    "                    self.algorithm = self.Exp4(self)\n",
    "                    gamma_widget = widgets.FloatSlider(\n",
    "                        min=self.algorithm.dynamic_gamma_min,\n",
    "                        max=self.algorithm.dynamic_gamma_max,\n",
    "                        step=self.algorithm.dynamic_gamma_step,\n",
    "                        value=self.algorithm.dynamic_gamma_default_value,\n",
    "                        description='Gamma:',\n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        readout_format='.3f',\n",
    "                        continuous_update=False,  # 松手后才更新\n",
    "                        style={'description_width': '150px'}  # 调整 description 的宽度\n",
    "                    )\n",
    "                    interact(self.algorithm.plot_interactive, gamma=gamma_widget)\n",
    "                    self.algorithm.plot_evaluation()\n",
    "\n",
    "        run_button = widgets.Button(\n",
    "            description='Set Parameters',\n",
    "            button_style='primary',  # 设置为 'primary' 风格\n",
    "            layout=widgets.Layout(width='150px', margin='10px')  # 增加按钮的间距\n",
    "        )\n",
    "        run_button.on_click(on_button_clicked)\n",
    "        \n",
    "        # 确保输出区域显示\n",
    "        controls = HBox([load_reward_method_widget, data_type_widget, algorithm_type_widget, run_button])\n",
    "        display(controls, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df403c5d-e127-43f8-96f4-465e2764653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DataInit.config_manager(if_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc83235bd4d5c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4_data_manager = DataInit.import_data_manager(models_pkl_path, 'exp4', if_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e7934ce687f3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317215f13d5b4c68af422769ebe5fc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(ToggleButtons(button_style='info', description='Reward Method:', layout=Layout(height='auto', m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21009e03d44e4e8e9dddba28376b0b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mab = MAB(config, reward_data_manager, exp4_data_manager)\n",
    "mab.dynamic_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14836862-1db4-4943-b2c5-c9ffbd495045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f42a2-1a22-40c6-8456-23c68c972965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
